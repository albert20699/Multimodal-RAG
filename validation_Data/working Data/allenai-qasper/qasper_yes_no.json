[
  {
    "paper_id": "2003.07723",
    "question": "Does the paper report macro F1?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
      ]
    }
  },
  {
    "paper_id": "2003.07723",
    "question": "Does the paper report macro F1?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We find that the multilingual model cannot handle infrequent categories, i.e., Awe/Sublime, Suspense and Humor. However, increasing the dataset with English data improves the results, suggesting that the classification would largely benefit from more annotated data. The best model overall is DBMDZ (.520), showing a balanced response on both validation and test set. See Table TABREF37 for a breakdown of all emotions as predicted by the this model. Precision is mostly higher than recall. The labels Awe/Sublime, Suspense and Humor are harder to predict than the other labels.",
        "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
      ],
      "highlighted_evidence": [
        "See Table TABREF37 for a breakdown of all emotions as predicted by the this model.",
        "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
      ]
    }
  },
  {
    "paper_id": "1705.09665",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 )."
      ],
      "highlighted_evidence": [
        "We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. "
      ]
    }
  },
  {
    "paper_id": "1805.02400",
    "question": "Do they use a pretrained NMT model to help generating reviews?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1805.02400",
    "question": "Do they use a pretrained NMT model to help generating reviews?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1805.02400",
    "question": "Does their detection tool work better than human detection?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We noticed some variation in the detection of different fake review categories. The respondents in our MTurk survey had most difficulties recognizing reviews of category $(b=0.3, \\lambda=-5)$, where true positive rate was $40.4\\%$, while the true negative rate of the real class was $62.7\\%$. The precision were $16\\%$ and $86\\%$, respectively. The class-averaged F-score is $47.6\\%$, which is close to random. Detailed classification reports are shown in Table~\\ref{table:MTurk_sub} in Appendix. Our MTurk-study shows that \\emph{our NMT-Fake reviews pose a significant threat to review systems}, since \\emph{ordinary native English-speakers have very big difficulties in separating real reviews from fake reviews}. We use the review category $(b=0.3, \\lambda=-5)$ for future user tests in this paper, since MTurk participants had most difficulties detecting these reviews. We refer to this category as NMT-Fake* in this paper.",
        "Figure~\\ref{fig:adaboost_matrix_b_lambda} shows our AdaBoost classifier's class-averaged F-score at detecting different kind of fake reviews. The classifier is very effective in detecting reviews that humans have difficulties detecting. For example, the fake reviews MTurk users had most difficulty detecting ($b=0.3, \\lambda=-5$) are detected with an excellent 97\\% F-score."
      ],
      "highlighted_evidence": [
        "The respondents in our MTurk survey had most difficulties recognizing reviews of category $(b=0.3, \\lambda=-5)$, where true positive rate was $40.4\\%$, while the true negative rate of the real class was $62.7\\%$. The precision were $16\\%$ and $86\\%$, respectively. The class-averaged F-score is $47.6\\%$, which is close to random. Detailed classification reports are shown in Table~\\ref{table:MTurk_sub} in Appendix. Our MTurk-study shows that \\emph{our NMT-Fake reviews pose a significant threat to review systems}, since \\emph{ordinary native English-speakers have very big difficulties in separating real reviews from fake reviews}. We use the review category $(b=0.3, \\lambda=-5)$ for future user tests in this paper, since MTurk participants had most difficulties detecting these reviews. We refer to this category as NMT-Fake* in this paper.",
        "The classifier is very effective in detecting reviews that humans have difficulties detecting. For example, the fake reviews MTurk users had most difficulty detecting ($b=0.3, \\lambda=-5$) are detected with an excellent 97\\% F-score."
      ]
    }
  },
  {
    "paper_id": "1907.05664",
    "question": "Is the explanation from saliency map correct?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We showed that in some cases the saliency maps are truthful to the network's computation, meaning that they do highlight the input features that the network focused on. But we also showed that in some cases the saliency maps seem to not capture the important input features. This brought us to discuss the fact that these attributions are not sufficient by themselves, and that we need to define the counter-factual case and test it to measure how truthful the saliency maps are.",
        "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates. The saliency maps on Figure 3 correspond to the summary from Figure 1 , and we don't see the word “video\" highlighted in the input text, which seems to be important for the output."
      ],
      "highlighted_evidence": [
        "But we also showed that in some cases the saliency maps seem to not capture the important input features. ",
        "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates"
      ]
    }
  },
  {
    "paper_id": "2002.02224",
    "question": "Did they experiment on this dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2002.02224",
    "question": "Did they experiment on this dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
        "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
      ],
      "highlighted_evidence": [
        "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
        "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
      ]
    }
  },
  {
    "paper_id": "2003.07433",
    "question": "Do the authors mention any possible confounds in this study?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2003.12218",
    "question": "Did they experiment with the dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In Figure FIGREF28, we show some examples of the annotation results in CORD-19-NER. We can see that our distantly- or weakly supervised methods achieve high quality recognizing the new entity types, requiring only several seed examples as the input. For example, we recognized “SARS-CoV-2\" as the “CORONAVIRUS\" type, “bat\" and “pangolins\" as the “WILDLIFE\" type and “Van der Waals forces\" as the “PHYSICAL_SCIENCE\" type. This NER annotation results help downstream text mining tasks in discovering the origin and the physical nature of the virus. Our NER methods are domain-independent that can be applied to corpus in different domains. In addition, we show another example of NER annotation on New York Times with our system in Figure FIGREF29.",
        "In Figure FIGREF30, we show the comparison of our annotation results with existing NER/BioNER systems. In Figure FIGREF30, we can see that only our method can identify “SARS-CoV-2\" as a coronavirus. In Figure FIGREF30, we can see that our method can identify many more entities such as “pylogenetic\" as a evolution term and “bat\" as a wildlife. In Figure FIGREF30, we can also see that our method can identify many more entities such as “racism\" as a social behavior. In summary, our distantly- and weakly-supervised NER methods are reliable for high-quality entity recognition without requiring human effort for training data annotation."
      ],
      "highlighted_evidence": [
        "In Figure FIGREF28, we show some examples of the annotation results in CORD-19-NER. We can see that our distantly- or weakly supervised methods achieve high quality recognizing the new entity types, requiring only several seed examples as the input. For example, we recognized “SARS-CoV-2\" as the “CORONAVIRUS\" type, “bat\" and “pangolins\" as the “WILDLIFE\" type and “Van der Waals forces\" as the “PHYSICAL_SCIENCE\" type. This NER annotation results help downstream text mining tasks in discovering the origin and the physical nature of the virus. Our NER methods are domain-independent that can be applied to corpus in different domains. In addition, we show another example of NER annotation on New York Times with our system in Figure FIGREF29.\n\nIn Figure FIGREF30, we show the comparison of our annotation results with existing NER/BioNER systems. In Figure FIGREF30, we can see that only our method can identify “SARS-CoV-2\" as a coronavirus. In Figure FIGREF30, we can see that our method can identify many more entities such as “pylogenetic\" as a evolution term and “bat\" as a wildlife. In Figure FIGREF30, we can also see that our method can identify many more entities such as “racism\" as a social behavior. In summary, our distantly- and weakly-supervised NER methods are reliable for high-quality entity recognition without requiring human effort for training data annotation."
      ]
    }
  },
  {
    "paper_id": "2003.12218",
    "question": "Do they list all the named entity types present?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER."
      ]
    }
  },
  {
    "paper_id": "2003.06651",
    "question": "Is the method described in this work a clustering-based method?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The task of Word Sense Induction (WSI) can be seen as an unsupervised version of WSD. WSI aims at clustering word senses and does not require to map each cluster to a predefined sense. Instead of that, word sense inventories are induced automatically from the clusters, treating each cluster as a single sense of a word. WSI approaches fall into three main groups: context clustering, word ego-network clustering and synonyms (or substitute) clustering.",
        "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”. Thus, our algorithm is based on graph-based word sense induction, but it also relies on vector-based operations between word embeddings to perform filtering of graph nodes. Analogously to the work of Pelevina:16, we construct a semantic relatedness graph from a list of nearest neighbours, but we filter this list using the following procedure:"
      ],
      "highlighted_evidence": [
        "The task of Word Sense Induction (WSI) can be seen as an unsupervised version of WSD. WSI aims at clustering word senses and does not require to map each cluster to a predefined sense. Instead of that, word sense inventories are induced automatically from the clusters, treating each cluster as a single sense of a word.",
        "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”."
      ]
    }
  },
  {
    "paper_id": "2003.06651",
    "question": "Is the method described in this work a clustering-based method?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”. Thus, our algorithm is based on graph-based word sense induction, but it also relies on vector-based operations between word embeddings to perform filtering of graph nodes. Analogously to the work of Pelevina:16, we construct a semantic relatedness graph from a list of nearest neighbours, but we filter this list using the following procedure:"
      ],
      "highlighted_evidence": [
        "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space."
      ]
    }
  },
  {
    "paper_id": "2003.06651",
    "question": "Was any extrinsic evaluation carried out?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task."
      ],
      "highlighted_evidence": [
        "We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task."
      ]
    }
  },
  {
    "paper_id": "1910.04269",
    "question": "Does the model use both spectrogram images and raw waveforms as features?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 4: Results of the two models and all its variations"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 4: Results of the two models and all its variations"
      ]
    }
  },
  {
    "paper_id": "1910.04269",
    "question": "Is the performance compared against a baseline model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel)."
      ],
      "highlighted_evidence": [
        "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel)."
      ]
    }
  },
  {
    "paper_id": "1910.04269",
    "question": "Is the performance compared against a baseline model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.13072",
    "question": "Did they experiment on all the tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Implementation & Models Parameters. For all our tasks, we use the BERT-Base Multilingual Cased model released by the authors . The model is trained on 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 15 epochs and choose the best model based on performance on a development set. We use the same hyper-parameters in all of our BERT models. We fine-tune BERT on each respective labeled dataset for each task. For BERT input, we apply WordPiece tokenization, setting the maximal sequence length to 50 words/WordPieces. For all tasks, we use a TensorFlow implementation. An exception is the sentiment analysis task, where we used a PyTorch implementation with the same hyper-parameters but with a learning rate $2e-6$.",
        "We presented AraNet, a deep learning toolkit for a host of Arabic social media processing. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. AraNet has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions. Our toolkit can be used to make important discoveries on the wide region of the Arab world, and can enhance our understating of Arab online communication. AraNet will be publicly available upon acceptance."
      ],
      "highlighted_evidence": [
        "For all tasks, we use a TensorFlow implementation.",
        "AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. "
      ]
    }
  },
  {
    "paper_id": "1712.09127",
    "question": "Do they evaluate grammaticality of generated text?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We hypothesize that because weGAN takes into account document labels in a semi-supervised way, the embeddings trained from weGAN can better incorporate the labeling information and therefore, produce document embeddings which are better separated. The results are shown in Table 1 and averaged over 5 randomized runs. Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. Because the Rand index captures matching accuracy, we observe from the Table 1 that weGAN tends to improve both metrics."
      ],
      "highlighted_evidence": [
        "Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. "
      ]
    }
  },
  {
    "paper_id": "2001.00137",
    "question": "Do they report results only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.",
        "The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names."
      ],
      "highlighted_evidence": [
        "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.",
        "The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names."
      ]
    }
  },
  {
    "paper_id": "2001.00137",
    "question": "Do they test their approach on a dataset without incomplete data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF22. The first step is to apply a TTS module to the available complete sentence. Here, we apply gtts , a Google Text-to-Speech python library, and macsay , a terminal command available in Mac OS as say. The second step consists of applying an STT module to the obtained audio files in order to obtain text containing STT errors. The STT module used here was witai , freely available and maintained by Wit.ai. The mentioned TTS and STT modules were chosen according to code availability and whether it's freely available or has high daily usage limitations."
      ],
      "highlighted_evidence": [
        "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora."
      ]
    }
  },
  {
    "paper_id": "2001.00137",
    "question": "Do they test their approach on a dataset without incomplete data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11.",
        "In the intent classification task, we are presented with a corpus that suffers from the opposite problem of the Twitter sentiment classification corpus. In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words. Analysis on this dataset adds value to our work by enabling evaluation of our model's robustness to different rates of data incompleteness."
      ],
      "highlighted_evidence": [
        "For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11.",
        "In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words."
      ]
    }
  },
  {
    "paper_id": "2001.00137",
    "question": "Should their approach be applied only when dealing with incomplete data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences."
      ],
      "highlighted_evidence": [
        "We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. "
      ]
    }
  },
  {
    "paper_id": "2001.00137",
    "question": "Should their approach be applied only when dealing with incomplete data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We propose Stacked Denoising BERT (DeBERT) as a novel encoding scheming for the task of incomplete intent classification and sentiment classification from incorrect sentences, such as tweets and text with STT error. The proposed model, illustrated in Fig. FIGREF4, is structured as a stacking of embedding layers and vanilla transformer layers, similarly to the conventional BERT BIBREF10, followed by layers of novel denoising transformers. The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. By reconstructing these hidden embeddings, we are able to improve the encoding scheme in BERT."
      ],
      "highlighted_evidence": [
        "The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. "
      ]
    }
  },
  {
    "paper_id": "1910.03042",
    "question": "Do they specify the model they use for Gunrock?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1."
      ],
      "highlighted_evidence": [
        "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response.",
        "While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1."
      ]
    }
  },
  {
    "paper_id": "1910.03042",
    "question": "Do they gather explicit user satisfaction data on Gunrock?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets)."
      ],
      "highlighted_evidence": [
        "Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\")."
      ]
    }
  },
  {
    "paper_id": "1809.08731",
    "question": "Is ROUGE their only baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.",
        "We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.",
        "We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as",
        "Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:",
        "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments."
      ],
      "highlighted_evidence": [
        "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.",
        "We compare to the best n-gram-overlap metrics from toutanova2016dataset;",
        "We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length.",
        "Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:",
        "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . "
      ]
    }
  },
  {
    "paper_id": "1909.11189",
    "question": "Is the outcome of the LDA analysis evaluated in any way?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%."
      ],
      "highlighted_evidence": [
        "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%."
      ]
    }
  },
  {
    "paper_id": "2003.08529",
    "question": "Did they propose other metrics?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions."
      ],
      "highlighted_evidence": [
        "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions."
      ]
    }
  },
  {
    "paper_id": "1708.05873",
    "question": "Is the dataset multilingual?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements."
      ],
      "highlighted_evidence": [
        "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . "
      ]
    }
  },
  {
    "paper_id": "1708.05873",
    "question": "Is the dataset multilingual?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements.",
        "FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model."
      ],
      "highlighted_evidence": [
        "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . ",
        "FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model."
      ]
    }
  },
  {
    "paper_id": "1909.12140",
    "question": "Is the semantic hierarchy representation used for any task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Moreover, most current Open IE approaches output only a loose arrangement of extracted tuples that are hard to interpret as they ignore the context under which a proposition is complete and correct and thus lack the expressiveness needed for a proper interpretation of complex assertions BIBREF8. As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks."
      ],
      "highlighted_evidence": [
        "As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks."
      ]
    }
  },
  {
    "paper_id": "1905.00563",
    "question": "Can this adversarial approach be used to directly improve model accuracy?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To evaluate this application, we inject random triples into the graph, and measure the ability of to detect the errors using our optimization. We consider two types of incorrect triples: 1) incorrect triples in the form of $\\langle s^{\\prime }, r, o\\rangle $ where $s^{\\prime }$ is chosen randomly from all of the entities, and 2) incorrect triples in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ where $s^{\\prime }$ and $r^{\\prime }$ are chosen randomly. We choose 100 random triples from the observed graph, and for each of them, add an incorrect triple (in each of the two scenarios) to its neighborhood. Then, after retraining DistMult on this noisy training data, we identify error triples through a search over the neighbors of the 100 facts. The result of choosing the neighbor with the least influence on the target is provided in the Table 7 . When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors."
      ],
      "highlighted_evidence": [
        "When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors."
      ]
    }
  },
  {
    "paper_id": "1909.00542",
    "question": "Did classification models perform better than previous regression one?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels."
      ],
      "highlighted_evidence": [
        "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels."
      ]
    }
  },
  {
    "paper_id": "1810.06743",
    "question": "Do they look for inconsistencies between different languages' annotations in UniMorph?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation."
      ],
      "highlighted_evidence": [
        "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources."
      ]
    }
  },
  {
    "paper_id": "1810.06743",
    "question": "Do they look for inconsistencies between different UD treebanks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The contributions of this work are:"
      ],
      "highlighted_evidence": [
        "The contributions of this work are:"
      ]
    }
  },
  {
    "paper_id": "1909.02764",
    "question": "Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.01252",
    "question": "Does the paper report the results of previous models applied to the same tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Technical and theoretical questions related to the proposed method and infrastructure for the exploration and facilitation of debates will be discussed in three sections. The first section concerns notions of how to define what constitutes a belief or opinion and how these can be mined from texts. To this end, an approach based on the automated extraction of semantic frames expressing causation is proposed. The observatory thus builds on the theoretical premise that expressions of causation such as `global warming causes rises in sea levels' can be revelatory for a person or group's underlying belief systems. Through a further technical description of the observatory's data-analytical components, section two of the paper deals with matters of spatially modelling the output of the semantic frame extractor and how this might be achieved without sacrificing nuances of meaning. The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
      ],
      "highlighted_evidence": [
        "The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
      ]
    }
  },
  {
    "paper_id": "1912.01252",
    "question": "Does the paper report the results of previous models applied to the same tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.13109",
    "question": "Was each text augmentation technique experimented individually?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.13109",
    "question": "Does the dataset contain content from various social media platforms?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Hinglish is a linguistic blend of Hindi (very widely spoken language in India) and English (an associate language of urban areas) and is spoken by upwards of 350 million people in India. While the name is based on the Hindi language, it does not refer exclusively to Hindi, but is used in India, with English words blending with Punjabi, Gujarati, Marathi and Hindi. Sometimes, though rarely, Hinglish is used to refer to Hindi written in English script and mixing with English words or phrases. This makes analyzing the language very interesting. Its rampant usage in social media like Twitter, Facebook, Online blogs and reviews has also led to its usage in delivering hate and abuses in similar platforms. We aim to find such content in the social media focusing on the tweets. Hypothetically, if we can classify such tweets, we might be able to detect them and isolate them for further analysis before it reaches public. This will a great application of AI to the social cause and thus is motivating. An example of a simple, non offensive message written in Hinglish could be:"
      ],
      "highlighted_evidence": [
        "We aim to find such content in the social media focusing on the tweets."
      ]
    }
  },
  {
    "paper_id": "1911.03310",
    "question": "Are language-specific and language-neutral components disjunctive?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings."
      ],
      "highlighted_evidence": [
        "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space."
      ]
    }
  },
  {
    "paper_id": "1910.11471",
    "question": "Do they compare to other models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.09886",
    "question": "Are there datasets with relation tuples annotated, how big are datasets available?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11."
      ],
      "highlighted_evidence": [
        "This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations."
      ]
    }
  },
  {
    "paper_id": "1807.03367",
    "question": "Did the authors use crowdsourcing platforms?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
      ],
      "highlighted_evidence": [
        "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)."
      ]
    }
  },
  {
    "paper_id": "1807.03367",
    "question": "Did the authors use crowdsourcing platforms?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We introduce the Talk the Walk dataset, where the aim is for two agents, a “guide” and a “tourist”, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a map and knows the target location, but does not know where the tourist is; the tourist has a 360-degree view of the world, but knows neither the target location on the map nor the way to it. The agents need to work together through communication in order to successfully solve the task. An example of the task is given in Figure FIGREF3 .",
        "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
      ],
      "highlighted_evidence": [
        "We introduce the Talk the Walk dataset, where the aim is for two agents, a “guide” and a “tourist”, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location.",
        "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
      ]
    }
  },
  {
    "paper_id": "1910.04601",
    "question": "Did they use any crowdsourcing platform?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
      ],
      "highlighted_evidence": [
        "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
      ]
    }
  },
  {
    "paper_id": "1910.04601",
    "question": "Did they use any crowdsourcing platform?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
      ],
      "highlighted_evidence": [
        "We deployed the task on Amazon Mechanical Turk (AMT)"
      ]
    }
  },
  {
    "paper_id": "1610.00879",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 1: Word cloud for drunk tweets"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 1: Word cloud for drunk tweets"
      ]
    }
  },
  {
    "paper_id": "1610.00879",
    "question": "Do the authors mention any confounds to their study?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "A key challenge is to obtain an annotated dataset. We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet. We create three datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction."
      ],
      "highlighted_evidence": [
        "Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work."
      ]
    }
  },
  {
    "paper_id": "1610.00879",
    "question": "Is the data acquired under distant supervision verified by humans at any stage?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0.42 for A1 and A2). Table TABREF20 compares our classifier with humans. Our human annotators perform the task with an average accuracy of 68.8%, while our classifier (with all features) trained on Dataset 2 reaches 64%. The classifier trained on Dataset 2 is better than which is trained on Dataset 1."
      ],
      "highlighted_evidence": [
        "Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober."
      ]
    }
  },
  {
    "paper_id": "1610.00879",
    "question": "Do the authors equate drunk tweeting with drunk texting? ",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The ubiquity of communication devices has made social media highly accessible. The content on these media reflects a user's day-to-day activities. This includes content created under the influence of alcohol. In popular culture, this has been referred to as `drunk-texting'. In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. We refer to tweets written under the influence of alcohol as `drunk tweets', and the opposite as `sober tweets'."
      ],
      "highlighted_evidence": [
        "In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. "
      ]
    }
  },
  {
    "paper_id": "1704.05572",
    "question": "Is an entity linking process used?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.",
        "Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $ &\\textit {tf}(x, q)=1\\; \\textmd {if x} \\in q ; \\textit {idf}(x) = log(1 + N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $"
      ],
      "highlighted_evidence": [
        "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.",
        "Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ ."
      ]
    }
  },
  {
    "paper_id": "1704.05572",
    "question": "Are the OpenIE extractions all triples?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We create an additional table in TableILP with all the tuples in $T$ . Since TableILP uses fixed-length $(subject; predicate; object)$ triples, we need to map tuples with multiple objects to this format. For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table."
      ],
      "highlighted_evidence": [
        "For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table."
      ]
    }
  },
  {
    "paper_id": "1704.05572",
    "question": "Can the method answer multi-hop questions?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
      ],
      "highlighted_evidence": [
        "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
      ]
    }
  },
  {
    "paper_id": "1704.05572",
    "question": "Is their method capable of multi-hop reasoning?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
      ],
      "highlighted_evidence": [
        "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
      ]
    }
  },
  {
    "paper_id": "1804.10686",
    "question": "Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We use two different unsupervised approaches for word sense disambiguation. The first, called `sparse model', uses a straightforward sparse vector space model, as widely used in Information Retrieval, to represent contexts and synsets. The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.",
        "In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. We transform every synset into its dense vector representation by averaging the word embeddings corresponding to each constituent word: DISPLAYFORM0",
        "We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size."
      ],
      "highlighted_evidence": [
        "We use two different unsupervised approaches for word sense disambiguation. ",
        "The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.",
        "In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. ",
        "We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. "
      ]
    }
  },
  {
    "paper_id": "1603.07044",
    "question": "Is the improvement actually coming from using an RNN?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1603.07044",
    "question": "Did they experimnet in other languages?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Moreover, our approach is also language independent. We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a hand-tuned strong baseline from SemEval-2015."
      ],
      "highlighted_evidence": [
        "We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. "
      ]
    }
  },
  {
    "paper_id": "1902.09314",
    "question": "Do they use multi-attention heads?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
      ],
      "highlighted_evidence": [
        "This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
      ]
    }
  },
  {
    "paper_id": "1702.06378",
    "question": "Can SCRF be used to pretrain the model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder. The joint model by multitask learning is slightly more expensive than the stand-alone SRNN model. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model. This is analogous to sequence training of HMM acoustic models, where the network is usually pretrained by the frame-level CE criterion. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model."
      ],
      "highlighted_evidence": [
        "One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder.",
        "To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model.",
        "Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model."
      ]
    }
  },
  {
    "paper_id": "1909.00430",
    "question": "Does the system trained only using XR loss outperform the fully supervised neural system?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
      ]
    }
  },
  {
    "paper_id": "1908.10449",
    "question": "Do they provide decision sequences as supervision while training models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
      ],
      "highlighted_evidence": [
        "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
      ]
    }
  },
  {
    "paper_id": "1912.00871",
    "question": "Does pre-training on general text corpus improve performance?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability.",
        "Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation."
      ],
      "highlighted_evidence": [
        "Our attempt at language pre-training fell short of our expectations in all but one tested dataset.",
        "Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported."
      ]
    }
  },
  {
    "paper_id": "1912.00871",
    "question": "Are the Transformers masked?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We calculate the loss in training according to a mean of the sparse categorical cross-entropy formula. Sparse categorical cross-entropy BIBREF23 is used for identifying classes from a feature set, which assumes a large target classification set. Evaluation between the possible translation classes (all vocabulary subword tokens) and the produced class (predicted token) is the metric of performance here. During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value. We adjust the model's loss according to the mean of the translation accuracy after predicting every determined subword in a translation."
      ],
      "highlighted_evidence": [
        "During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value."
      ]
    }
  },
  {
    "paper_id": "1911.03894",
    "question": "Was CamemBERT compared against multilingual BERT on these tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT). We then compare our models to UDify BIBREF36. UDify is a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS tagging and dependency parsing."
      ],
      "highlighted_evidence": [
        "To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT)."
      ]
    }
  },
  {
    "paper_id": "1912.01673",
    "question": "Is this dataset publicly available?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The corpus is freely available at the following link:",
        "http://hdl.handle.net/11234/1-3123"
      ],
      "highlighted_evidence": [
        "The corpus is freely available at the following link:\n\nhttp://hdl.handle.net/11234/1-3123"
      ]
    }
  },
  {
    "paper_id": "1912.01673",
    "question": "Are some baseline models trained on this dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. Having browsed a number of 2D visualizations (PCA and t-SNE) of the space, we have to conclude that visually, LASER space does not seem to exhibit any of the desired topological properties discussed above, see fig:pca for one example."
      ],
      "highlighted_evidence": [
        "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019."
      ]
    }
  },
  {
    "paper_id": "1912.01673",
    "question": "Do they do any analysis of of how the modifications changed the starting set of sentences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The lack of semantic relations in the LASER space is also reflected in vector similarities, summarized in similarities. The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). Tense changes and some form of negation or banning also keep the vectors very similar."
      ],
      "highlighted_evidence": [
        "The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930)."
      ]
    }
  },
  {
    "paper_id": "1912.01673",
    "question": "Do they use external resources to make modifications to sentences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1706.08032",
    "question": "Are results reported only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .",
        "Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.",
        "Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",
        "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.",
        "For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset."
      ],
      "highlighted_evidence": [
        "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",
        "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. ",
        "For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). "
      ]
    }
  },
  {
    "paper_id": "1811.01399",
    "question": "Apart from using desired properties, do they evaluate their LAN approach in some other way?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1811.01399",
    "question": "Do they evaluate existing methods in terms of desired properties?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.00124",
    "question": "Is the model evaluated against a CNN baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.",
        "The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300."
      ],
      "highlighted_evidence": [
        "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop."
      ]
    }
  },
  {
    "paper_id": "1909.00088",
    "question": "Does the model proposed beat the baseline models for all the values of the masking parameter tested?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT",
        "FLOAT SELECTED: Table 9: Average results by MRT/RRT"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT",
        "FLOAT SELECTED: Table 9: Average results by MRT/RRT"
      ]
    }
  },
  {
    "paper_id": "1909.00088",
    "question": "Has STES been previously used in the literature to evaluate similar tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We propose a pipeline called SMERTI (pronounced `smarty') for STE. Combining entity replacement (ER), similarity masking (SM), and text infilling (TI), SMERTI can modify the semantic content of text. We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change."
      ],
      "highlighted_evidence": [
        "We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change."
      ]
    }
  },
  {
    "paper_id": "1911.01799",
    "question": "Do they experiment with cross-genre setups?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1812.06705",
    "question": "Does the new objective perform better than the original objective bert is trained on?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words predicted through BERT may not be compatible with original labels. The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
      ],
      "highlighted_evidence": [
        "The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
      ]
    }
  },
  {
    "paper_id": "1812.06705",
    "question": "Are other pretrained language models also evaluated for contextual augmentation? ",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
      ]
    }
  },
  {
    "paper_id": "1812.06705",
    "question": "Do the authors report performance of conditional bert on tasks without data augmentation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words predicted through BERT may not be compatible with original labels. The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
      ],
      "highlighted_evidence": [
        "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does."
      ]
    }
  },
  {
    "paper_id": "1905.08949",
    "question": "Do they cover data augmentation papers?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1905.08949",
    "question": "Do they survey visual question generation work?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded questions, i.e., all relevant information for the answer can be found in the input image BIBREF74 . A key purpose of grounded VQG is to support the dataset construction for VQA. To ensure the questions are grounded, existing systems rely on image captions to varying degrees. BIBREF75 and BIBREF76 simply convert image captions into questions using rule-based methods with textual patterns. BIBREF74 proposed a neural model that can generate questions with diverse types for a single image, using separate networks to construct dense image captions and to select question types.",
        "In contrast to grounded QG, humans ask higher cognitive level questions about what can be inferred rather than what can be seen from an image. Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image. These are deep questions that require high cognition such as analyzing and creation. With significant progress in deep generative models, marked by variational auto-encoders (VAEs) and GANs, such models are also used in open-ended VQG to bring “creativity” into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 ."
      ],
      "highlighted_evidence": [
        "Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image.",
        "Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image."
      ]
    }
  },
  {
    "paper_id": "1905.08949",
    "question": "Do they survey multilingual aspects?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1905.08949",
    "question": "Do they survey non-neural methods for question generation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.00170",
    "question": "Do they evaluate on NER data sets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0. Our results in Table 6 and Table 7 demonstrate the power of hypersphere features, which contribute to nearly all of the three types of entities as shown in Table 6, except for a slight drop in the PER type of BIBREF22 on a strong baseline. HS features stably enhance all strong state-of-the-art baselines, BIBREF22 , BIBREF21 and BIBREF23 by 0.33/0.72/0.23 $F_1$ point and 0.13/0.3/0.1 $F_1$ point on both benchmark datasets, CoNLL-2003 and ONTONOTES 5.0. We show that our HS feature is also comparable with previous much more complicated LS feature, and our model surpasses their baseline (without LS feature) by 0.58/0.78 $F_1$ point with only HS features. We establish a new state-of-the-art $F_1$ score of 89.75 on ONTONOTES 5.0, while matching state-of-the-art performance with a $F_1$ score of 92.95 on CoNLL-2003 dataset."
      ],
      "highlighted_evidence": [
        "To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0."
      ]
    }
  },
  {
    "paper_id": "1603.01417",
    "question": "Does the DMN+ model establish state-of-the-art ?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We have proposed new modules for the DMN framework to achieve strong results without supervision of supporting facts. These improvements include the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains."
      ],
      "highlighted_evidence": [
        "Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains."
      ]
    }
  },
  {
    "paper_id": "1911.03385",
    "question": "Is this style generator compared to some baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. To generate in a specific style, we simply set the appropriate embedding. We use genre embeddings of size 850 which is equivalent to the total size of the $K$ feature embeddings in the StyleEQ model."
      ],
      "highlighted_evidence": [
        "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7."
      ]
    }
  },
  {
    "paper_id": "1905.06512",
    "question": "Is there an online demo of their system?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1905.06512",
    "question": "Do they perform manual evaluation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word “旅馆” (hotel), the baseline model fails to generate definition containing the token “旅行者”(tourists). However, by incoporating sememes' information, especially the sememe “旅游” (tour), AAM and SAAM successfully generate “旅行者”(tourists). Manual inspection of others examples also supports our claim."
      ],
      "highlighted_evidence": [
        "Manual inspection of others examples also supports our claim."
      ]
    }
  },
  {
    "paper_id": "1905.06512",
    "question": "Do they compare against Noraset et al. 2017?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework. Without utilizing the information of sememes, it learns a probabilistic mapping $P(y | x)$ from the word $x$ to be defined to a definition $y = [y_1, \\dots , y_T ]$ , in which $y_t$ is the $t$ -th word of definition $y$ ."
      ],
      "highlighted_evidence": [
        "The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework."
      ]
    }
  },
  {
    "paper_id": "2001.07209",
    "question": "Does the paper discuss previous models which have been applied to the same task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16."
      ],
      "highlighted_evidence": [
        "An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16."
      ]
    }
  },
  {
    "paper_id": "1812.07023",
    "question": "Do they use pretrained word vectors for dialogue context embedding?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": [
        "The utterance is concatenated with a special symbol marking the end of the sequence. We initialize our word embeddings using 300-dimensional GloVe BIBREF30 and then fine-tune them during training."
      ]
    }
  },
  {
    "paper_id": "1610.04377",
    "question": "Do the tweets come from any individual?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with <emergency>and <non-emergency>labels for classification as stage one. Our dataset contains 1313 tweet with positive label <emergency>and 1887 tweets with a negative label <non-emergency>. We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc."
      ],
      "highlighted_evidence": [
        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. "
      ]
    }
  },
  {
    "paper_id": "1610.04377",
    "question": "Are the tweets specific to a region?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": [
        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with and labels for classification as stage one. Our dataset contains 1313 tweet with positive label and 1887 tweets with a negative label . We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc."
      ]
    }
  },
  {
    "paper_id": "1906.06448",
    "question": "Do they release MED?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" ). Compared with manual or automatic construction, we can collect naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning."
      ],
      "highlighted_evidence": [
        "To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" )."
      ]
    }
  },
  {
    "paper_id": "1907.00758",
    "question": "Do they compare their neural network against any other model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1907.00758",
    "question": "Does their neural network predict a single offset in a recording?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. The candidate set is independent of the model, and is chosen based on task knowledge (Section SECREF5 )."
      ],
      "highlighted_evidence": [
        "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. "
      ]
    }
  },
  {
    "paper_id": "1904.05862",
    "question": "Do they explore how much traning data is needed for which magnitude of improvement for WER? ",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance."
      ],
      "highlighted_evidence": [
        "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance."
      ]
    }
  },
  {
    "paper_id": "1911.00069",
    "question": "Do they train their own RE model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.",
        "We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.",
        "We apply model ensemble to further improve the accuracy of the Bi-LSTM model. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. This Ensemble approach improves the single model by 0.6-1.9 $F_1$ points, except for Arabic."
      ],
      "highlighted_evidence": [
        "We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.\n\nWe learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.",
        "We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models."
      ]
    }
  },
  {
    "paper_id": "1810.00663",
    "question": "Did the collection process use a WoZ method?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.",
        "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
      ],
      "highlighted_evidence": [
        "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.",
        "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
      ]
    }
  },
  {
    "paper_id": "1810.00663",
    "question": "Did the authors use a crowdsourcing platform?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans."
      ],
      "highlighted_evidence": [
        "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. "
      ]
    }
  },
  {
    "paper_id": "2001.01589",
    "question": "Is the word segmentation method independently evaluated?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.10324",
    "question": "Do they just sum up all the loses the calculate to end up with one single loss?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We have found it beneficial to apply the loss function at several intermediate layers of the network. Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as",
        "where $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. The coefficient $\\lambda $ scales the auxiliary loss and we set $\\lambda = 0.3$ based on our preliminary experiments. We illustrate the auxiliary prediction and loss in Figure FIGREF6."
      ],
      "highlighted_evidence": [
        "Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as\n\nwhere $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. T"
      ]
    }
  },
  {
    "paper_id": "1910.05456",
    "question": "Are agglutinative languages used in the prediction of both prefixing and suffixing languages?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue).",
        "Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing.",
        "Analyzing next the errors concerning affixes, we find that models pretrained on HUN, ITA, DEU, and FRA (in that order) commit the fewest errors. This supports two of our previous hypotheses: First, given that ITA and FRA are both from the same language family as SPA, relatedness seems to be benficial for learning of the second language. Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well.",
        "In Table TABREF39, the errors for Zulu are shown, and Table TABREF19 reveals the relative performance for different source languages: TUR $>$ HUN $>$ DEU $>$ ITA $>$ FRA $>$ NAV $>$ EUS $>$ QVH. Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language."
      ],
      "highlighted_evidence": [
        "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue).",
        "We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing.",
        "Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well.",
        "Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language."
      ]
    }
  },
  {
    "paper_id": "1910.05154",
    "question": "Is the model evaluated against any baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
      ],
      "highlighted_evidence": [
        "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
      ]
    }
  },
  {
    "paper_id": "1910.05154",
    "question": "Does the paper report the accuracy of the model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
      ],
      "highlighted_evidence": [
        "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. "
      ]
    }
  },
  {
    "paper_id": "1806.00722",
    "question": "did they outperform previous methods?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well."
      ],
      "highlighted_evidence": [
        " In almost all genres, DenseNMT models are significantly better than the baselines."
      ]
    }
  },
  {
    "paper_id": "1806.02847",
    "question": "Do they fine-tune their model on the end task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$"
      ],
      "highlighted_evidence": [
        "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. "
      ]
    }
  },
  {
    "paper_id": "1909.00324",
    "question": "Is the model evaluated against other Aspect-Based models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Experiments ::: Baselines",
        "To comprehensively evaluate our AGDT, we compare the AGDT with several competitive models."
      ],
      "highlighted_evidence": [
        "Experiments ::: Baselines\nTo comprehensively evaluate our AGDT, we compare the AGDT with several competitive models."
      ]
    }
  },
  {
    "paper_id": "2003.04032",
    "question": "Do they build a model to recognize discourse relations on their dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.12618",
    "question": "Is there any example where geometric property is visible for context similarity between words?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance)."
      ],
      "highlighted_evidence": [
        "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. "
      ]
    }
  },
  {
    "paper_id": "1901.04899",
    "question": "Did they compare against other systems?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",
        "FLOAT SELECTED: Table 3: Utterance-level Intent Recognition Results (10-fold CV)"
      ],
      "highlighted_evidence": [
        "The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",
        "FLOAT SELECTED: Table 3: Utterance-level Intent Recognition Results (10-fold CV)"
      ]
    }
  },
  {
    "paper_id": "1809.10644",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this paper, we use three data sets from the literature to train and evaluate our own classifier. Although all address the category of hateful speech, they used different strategies of labeling the collected data. Table TABREF5 shows the characteristics of the datasets.",
        "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .",
        "Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. Such examples may be innocuous in isolation but could potentially be sexist or racist in context. While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.",
        "Debra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two.",
        "Along these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:",
        "@LoveAndLonging ...how is that example \"sexism\"?",
        "@amberhasalamb ...in what way?"
      ],
      "highlighted_evidence": [
        "In this paper, we use three data sets from the literature to train and evaluate our own classifier.",
        "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .",
        "Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. ",
        "While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.\n\nDebra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two.\n\nAlong these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:\n\n@LoveAndLonging ...how is that example \"sexism\"?\n\n@amberhasalamb ...in what way?"
      ]
    }
  },
  {
    "paper_id": "1910.03467",
    "question": "Are synonymous relation taken into account in the Japanese-Vietnamese task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Due to the fact that Vietnamese WordNet is not available, we only exploit WordNet to tackle unknown words of Japanese texts in our Japanese$\\rightarrow $Vietnamese translation system. After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. We choose 1-best synonym for each OOV word. Table TABREF18 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3). We observed an improvement of +0.7 BLEU points between (3) and (5) without using data augmentation described in BIBREF21."
      ],
      "highlighted_evidence": [
        "After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. "
      ]
    }
  },
  {
    "paper_id": "1910.03467",
    "question": "Is the supervised morphological learner tested on Japanese?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.",
        "FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems"
      ],
      "highlighted_evidence": [
        "We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.",
        "FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems"
      ]
    }
  },
  {
    "paper_id": "1908.09156",
    "question": "Does the paper consider the use of perplexity in order to identify text anomalies?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": [
        "PERPLEXITY"
      ]
    }
  },
  {
    "paper_id": "1908.09156",
    "question": "Does the paper report a baseline for the task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.00523",
    "question": "Do authors provide any explanation for intriguing patterns of word being echoed?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations."
      ],
      "highlighted_evidence": [
        "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations."
      ]
    }
  },
  {
    "paper_id": "1910.11949",
    "question": "Is machine learning system underneath similar to image caption ML systems?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning. In our case, we focus on generating questions from pictures. Our second architecture is inspired by Neural Conversational Model from BIBREF14 where the author presents an end-to-end approach to generate simple conversations. Building an open-domain conversational agent is a challenging problem. As addressed in BIBREF15 and BIBREF16, the lack of a consistent personality and lack of long-term memory which produces some meaningless responses in these models are still unresolved problems."
      ],
      "highlighted_evidence": [
        "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning."
      ]
    }
  },
  {
    "paper_id": "1907.01413",
    "question": "Do they propose any further additions that could be made to improve generalisation to unseen speakers?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. Recurrent architectures are natural candidates for such systems. Additionally, if using these techniques for speech therapy, the audio signal will be available. An extension of these analyses should not be limited to the ultrasound signal, but instead evaluate whether audio and ultrasound can be complementary. Further work should aim to extend the four classes to more a fine-grained place of articulation, possibly based on phonological processes. Similarly, investigating which classes lead to classification errors might help explain some of the observed results. Although we have looked at variables such as age, gender, or amount of data to explain speaker variation, there may be additional factors involved, such as the general quality of the ultrasound image. Image quality could be affected by probe placement, dry mouths, or other factors. Automatically identifying or measuring such cases could be beneficial for speech therapy, for example, by signalling the therapist that the data being collected is sub-optimal."
      ],
      "highlighted_evidence": [
        "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame."
      ]
    }
  },
  {
    "paper_id": "1907.01413",
    "question": "Do they compare to previous work?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1908.07816",
    "question": "Is some other metrics other then perplexity measured?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The evaluation of chatbots remains an open problem in the field. Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work."
      ],
      "highlighted_evidence": [
        "The evaluation of chatbots remains an open problem in the field. Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work."
      ]
    }
  },
  {
    "paper_id": "1703.03097",
    "question": "Do they evaluate on relation extraction?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.05608",
    "question": "Is the data all in Vietnamese?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs. HSD is required to build a multi-class classification model that is capable of classifying an item to one of 3 classes (hate, offensive, clean). Hate speech (hate): an item is identified as hate speech if it (1) targets individuals or groups on the basis of their characteristics; (2) demonstrates a clear intention to incite harm, or to promote hatred; (3) may or may not use offensive or profane words. Offensive but not hate speech (offensive): an item (posts/comments) may contain offensive words but it does not target individuals or groups on the basis of their characteristics. Neither offensive nor hate speech (clean): normal item, it does not contain offensive language or hate speech.",
        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16."
      ],
      "highlighted_evidence": [
        "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs.",
        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type."
      ]
    }
  },
  {
    "paper_id": "1906.07668",
    "question": "Do the authors report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 1: Topics and top-10 keywords of the corresponding topic",
        "FLOAT SELECTED: Figure 5: Visualization using pyLDAVis. Best viewed in electronic format (zoomed in)."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 1: Topics and top-10 keywords of the corresponding topic",
        "FLOAT SELECTED: Figure 5: Visualization using pyLDAVis. Best viewed in electronic format (zoomed in)."
      ]
    }
  },
  {
    "paper_id": "1805.00760",
    "question": "Do they explore how useful is the detection history and opinion summary?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Ablation Study",
        "To further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed."
      ],
      "highlighted_evidence": [
        "Ablation Study\nTo further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed."
      ]
    }
  },
  {
    "paper_id": "2001.09332",
    "question": "Are the word embeddings tested on a NLP task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To analyse the results we chose to use the test provided by BIBREF10, which consists of $19\\,791$ analogies divided into 19 different categories: 6 related to the “semantic\" macro-area (8915 analogies) and 13 to the “syntactic\" one (10876 analogies). All the analogies are composed by two pairs of words that share a relation, schematized with the equation: $a:a^{*}=b:b^{*}$ (e.g. “man : woman = king : queen\"); where $b^{*}$ is the word to be guessed (“queen\"), $b$ is the word coupled to it (“king\"), $a$ is the word for the components to be eliminated (“man\"), and $a^{*}$ is the word for the components to be added (“woman\")."
      ],
      "highlighted_evidence": [
        "To analyse the results we chose to use the test provided by BIBREF10, which consists of $19\\,791$ analogies divided into 19 different categories: 6 related to the “semantic\" macro-area (8915 analogies) and 13 to the “syntactic\" one (10876 analogies). All the analogies are composed by two pairs of words that share a relation, schematized with the equation: $a:a^{*}=b:b^{*}$ (e.g. “man : woman = king : queen\"); where $b^{*}$ is the word to be guessed (“queen\"), $b$ is the word coupled to it (“king\"), $a$ is the word for the components to be eliminated (“man\"), and $a^{*}$ is the word for the components to be added (“woman\")."
      ]
    }
  },
  {
    "paper_id": "2001.09332",
    "question": "Are the word embeddings evaluated?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).",
        "As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs."
      ],
      "highlighted_evidence": [
        "Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).\n\nAs it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas."
      ]
    }
  },
  {
    "paper_id": "1904.07342",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We henceforth refer to a tweet affirming climate change as a “positive\" sample (labeled as 1 in the data), and a tweet denying climate change as a “negative\" sample (labeled as -1 in the data). All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). We refer to the first data batch as “influential\" tweets, and the second data batch as “event-related\" tweets."
      ],
      "highlighted_evidence": [
        "All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). "
      ]
    }
  },
  {
    "paper_id": "1904.07342",
    "question": "Do the authors mention any confounds to their study?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters."
      ],
      "highlighted_evidence": [
        "There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters."
      ]
    }
  },
  {
    "paper_id": "1603.04553",
    "question": "Are resolution mode variables hand crafted?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:",
        "$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .",
        "$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.",
        "$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
      ],
      "highlighted_evidence": [
        "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
      ]
    }
  },
  {
    "paper_id": "1901.02262",
    "question": "Does their model also take the expected answer style as input?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on the decoder side prevents the passage ranker and answer possibility classifier from depending on the answer style."
      ],
      "highlighted_evidence": [
        "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles."
      ]
    }
  },
  {
    "paper_id": "1901.02262",
    "question": "Is there exactly one \"answer style\" per dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL."
      ],
      "highlighted_evidence": [
        "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question."
      ]
    }
  },
  {
    "paper_id": "1911.12579",
    "question": "Are trained word embeddings used for any other NLP task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a flow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not sufficient to design a language independent or machine learning algorithms. The present work is a first comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
      ],
      "highlighted_evidence": [
        "In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition.",
        "Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
      ]
    }
  },
  {
    "paper_id": "2004.02929",
    "question": "Does the paper mention other works proposing methods to detect anglicisms in Spanish?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In terms of automatic detection of anglicisms, previous approaches in different languages have mostly depended on resource lookup (lexicon or corpus frequencies), character n-grams and pattern matching. alex-2008-comparing combined lexicon lookup and a search engine module that used the web as a corpus to detect English inclusions in a corpus of German texts and compared her results with a maxent Markov model. furiassi2007retrieval explored corpora lookup and character n-grams to extract false anglicisms from a corpus of Italian newspapers. andersen2012semi used dictionary lookup, regular expressions and lexicon-derived frequencies of character n-grams to detect anglicism candidates in the Norwegian Newspaper Corpus (NNC) BIBREF21, while losnegaard2012data explored a Machine Learning approach to anglicism detection in Norwegian by using TiMBL (Tilburg Memory-Based Learner, an implementation of a k-nearest neighbor classifier) with character trigrams as features. garley-hockenmaier-2012-beefmoves trained a maxent classifier with character n-gram and morphological features to identify anglicisms in German online communities. In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish."
      ],
      "highlighted_evidence": [
        "In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish."
      ]
    }
  },
  {
    "paper_id": "1910.10408",
    "question": "Do they conduct any human evaluation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."
      ],
      "highlighted_evidence": [
        "We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."
      ]
    }
  },
  {
    "paper_id": "1910.10408",
    "question": "Do they experiment with combining both methods?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Methods ::: Combining the two methods",
        "We further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length."
      ],
      "highlighted_evidence": [
        "Methods ::: Combining the two methods\nWe further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length."
      ]
    }
  },
  {
    "paper_id": "2002.00876",
    "question": "Does API provide ability to connect to models written in some other deep learning framework?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can request samples $z \\sim \\textsc {CRF}(\\ell )$, probabilities $\\textsc {CRF}(z;\\ell )$, modes $\\arg \\max _z \\textsc {CRF}(\\ell )$, or other distributional properties such as $\\mathbb {H}(\\textsc {CRF}(\\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning."
      ],
      "highlighted_evidence": [
        "The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29."
      ]
    }
  },
  {
    "paper_id": "1605.08675",
    "question": "Do they compare DeepER against other approaches?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid)."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid)."
      ]
    }
  },
  {
    "paper_id": "1907.08501",
    "question": "Do they test performance of their approaches using human judgements?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online.",
        "To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given. INLINEFORM1 is the faction of correct answer (parts) given divided by all correct ones in the gold answer, and INLINEFORM2 is the harmonic mean of INLINEFORM3 and INLINEFORM4 . As an example, if the question is “Where was Albert Einstein born?” (gold answer: “Ulm”), and the system gives two answers “Ulm” and “Bern”, then INLINEFORM5 , INLINEFORM6 and INLINEFORM7 ."
      ],
      "highlighted_evidence": [
        "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online.",
        "To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given."
      ]
    }
  },
  {
    "paper_id": "1910.00825",
    "question": "Is new evaluation metric extension of ROGUE?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. For example, in news summarization the proper nouns are the critical information to retain."
      ],
      "highlighted_evidence": [
        "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities."
      ]
    }
  },
  {
    "paper_id": "1911.03705",
    "question": "Are the models required to also generate rationales?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We explore how to utilize additional commonsense knowledge (i.e. rationales) as the input to the task. Like we mentioned in Section SECREF6, we search relevant sentences from the OMCS corpus as the additional distant rationales, and ground truth rationale sentences for dev/test data. The inputs are no longer the concept-sets themselves, but in a form of “[rationales$|$concept-set]” (i.e. concatenating the rationale sentences and original concept-set strings)."
      ],
      "highlighted_evidence": [
        "We explore how to utilize additional commonsense knowledge (i.e. rationales) as the input to the task. Like we mentioned in Section SECREF6, we search relevant sentences from the OMCS corpus as the additional distant rationales, and ground truth rationale sentences for dev/test data. The inputs are no longer the concept-sets themselves, but in a form of “[rationales$|$concept-set]” (i.e. concatenating the rationale sentences and original concept-set strings)."
      ]
    }
  },
  {
    "paper_id": "1911.03705",
    "question": "Are the rationales generated after the sentences were written?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. The crowd-sourced sentences correlate well with the associated captions, meaning that it is reasonable to use caption sentences as training data although they can be partly noisy. Additionally, we utilize a search engine over the OMCS corpus BIBREF16 for retrieving relevant propositions as distant rationales in training data."
      ],
      "highlighted_evidence": [
        "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes."
      ]
    }
  },
  {
    "paper_id": "1911.03705",
    "question": "Are the sentences in the dataset written by humans who were shown the concept-sets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "It is true that the above-mentioned associated caption sentences for each concept-set are human-written and do describe scenes that cover all given concepts. However, they are created under specific contexts (i.e. an image or a video) and thus might be less representative for common sense. To better measure the quality and interpretability of generative reasoners, we need to evaluate them with scenes and rationales created by using concept-sets only as the signals for annotators.",
        "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. The crowd-sourced sentences correlate well with the associated captions, meaning that it is reasonable to use caption sentences as training data although they can be partly noisy. Additionally, we utilize a search engine over the OMCS corpus BIBREF16 for retrieving relevant propositions as distant rationales in training data."
      ],
      "highlighted_evidence": [
        "It is true that the above-mentioned associated caption sentences for each concept-set are human-written and do describe scenes that cover all given concepts. However, they are created under specific contexts (i.e. an image or a video) and thus might be less representative for common sense. To better measure the quality and interpretability of generative reasoners, we need to evaluate them with scenes and rationales created by using concept-sets only as the signals for annotators.\n\nWe collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes."
      ]
    }
  },
  {
    "paper_id": "1601.06068",
    "question": "Do they evaluate the quality of the paraphrasing model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1709.07916",
    "question": "Do they evaluate only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 . Twitter's APIs provides both historic and real-time data collections. The latter method randomly collects 1% of publicly available tweets. This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. We used the queries to collect approximately 4.5 million related tweets between 06/01/2016 and 06/30/2016. The data will be available in the first author's website. Figure FIGREF3 shows a sample of collected tweets in this research."
      ],
      "highlighted_evidence": [
        "This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. "
      ]
    }
  },
  {
    "paper_id": "1908.05434",
    "question": "Do they use pretrained word embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."
      ],
      "highlighted_evidence": [
        "As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."
      ]
    }
  },
  {
    "paper_id": "1612.05310",
    "question": "Do they experiment with the dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549. This result is what makes this dataset interesting: there is still lots of room for research on this task. Again, the primary goal of this experiment is to help identify the difficult-to-classify instances for analysis in the next section."
      ],
      "highlighted_evidence": [
        "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549. "
      ]
    }
  },
  {
    "paper_id": "1612.05310",
    "question": "Do they use a crowdsourcing platform for annotation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.",
        "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”."
      ],
      "highlighted_evidence": [
        "Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. ",
        "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. "
      ]
    }
  },
  {
    "paper_id": "2003.00576",
    "question": "Is there any evidence that encoders with latent structures work well on other tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. Building on this motivation, our latent structure attention module builds upon BIBREF12 to model the dependencies between sentences in a document. It uses a variant of Kirchhoff’s matrix-tree theorem BIBREF14 to model such dependencies as non-projective tree structures(§SECREF3). The explicit attention module is linguistically-motivated and aims to incorporate sentence-level structures from externally annotated document structures. We incorporate a coreference based sentence dependency graph, which is then combined with the output of the latent structure attention module to produce a hybrid structure-aware sentence representation (§SECREF5)."
      ],
      "highlighted_evidence": [
        "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. "
      ]
    }
  },
  {
    "paper_id": "2004.00139",
    "question": "Is the model evaluated on the graphemes-to-phonemes task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To increase the benefits of our data for ASR systems, we also trained a grapheme-to-phoneme (g2p) model: Out-of-vocabulary words can be a problem for ASR system. For those out-of-vocabulary words we need a model that can generate pronunciations from a written form, in real time. This is why we train a grapheme-to-phoneme (g2p) model that generates a sequence of phonemes for a given word. We train the g2p model using our dictionary and compare its performance with a widely used joint-sequence g2p model, Sequitur BIBREF26. For the g2p model we are using the same architecture as for the p2g model. The only difference is input and output vocabulary. The Sequitur and our model are using the dictionary with the same train (19'898 samples), test (2'412 samples) and validation (2'212 samples) split. Additionally, we also test their performance only on the items from the Zurich and Visp dialect, because most of the samples are from this two dialects. In Table TABREF15 we show the result of the comparison of the two models. We compute the edit distance between the predicted and the true pronunciation and report the number of exact matches. In the first columns we have the result using the whole test set with all the dialects, and in the 2nd and 3rd columns we show the number of exact matches only on the samples from the test set that are from the Zurich and Visp dialect. For here we can clearly see that our model performs better than the Sequitur model. The reason why we have less matches in the Visp dialect compared to Zurich is because most of the our data is from the Zurich dialect."
      ],
      "highlighted_evidence": [
        "Additionally, we also test their performance only on the items from the Zurich and Visp dialect, because most of the samples are from this two dialects. In Table TABREF15 we show the result of the comparison of the two models."
      ]
    }
  },
  {
    "paper_id": "1811.08048",
    "question": "Do all questions in the dataset allow the answers to pick from 2 options?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 )."
      ],
      "highlighted_evidence": [
        "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare."
      ]
    }
  },
  {
    "paper_id": "1904.10500",
    "question": "Are the intent labels imbalanced in the dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For in-cabin intent understanding, we described 4 groups of usages to support various natural commands for interacting with the vehicle: (1) Set/Change Destination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/off, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are identified and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropOff, OpenDoor, and Other. For slot filling task, relevant slots are identified and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., `this', `that', `over there', etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar tasks on Amazon Mechanical Turk BIBREF21 and reached 3418 utterances with intents in total. Intent and slot annotations are obtained on the transcribed utterances by majority voting of 3 annotators. Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",
        "FLOAT SELECTED: Table 2: AMIE Dataset Statistics: Slots and Intent Keywords"
      ],
      "highlighted_evidence": [
        " Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",
        "FLOAT SELECTED: Table 2: AMIE Dataset Statistics: Slots and Intent Keywords"
      ]
    }
  },
  {
    "paper_id": "1711.11221",
    "question": "Did the authors evaluate their system output for coherence?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We want to further study how the proposed cache-based neural model influence coherence in document translation. For this, we follow Lapata2005Automatic to measure coherence as sentence similarity. First, each sentence is represented by the mean of the distributed vectors of its words. Second, the similarity between two sentences is determined by the cosine of their means."
      ],
      "highlighted_evidence": [
        "we follow Lapata2005Automatic to measure coherence as sentence similarity"
      ]
    }
  },
  {
    "paper_id": "1905.06906",
    "question": "Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. This is depicted in Table 1 .",
        "We find that gated architectures vastly outperform non gated CNN model. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation."
      ],
      "highlighted_evidence": [
        "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models.",
        "The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation."
      ]
    }
  },
  {
    "paper_id": "1905.06906",
    "question": "Are there conceptual benefits to using GCNs over more complex architectures like attention?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The proposed model architecture is shown in the Figure 1 . Recurrent Neural Networks like LSTM, GRU update their weights at every timestep sequentially and hence lack parallelization over inputs in training. In case of attention based models, the attention layer has to wait for outputs from all timesteps. Hence, these models fail to take the advantage of parallelism either. Since, proposed model is based on convolution layers and gated mechanism, it can be parallelized efficiently. The convolution layers learn higher level representations for the source domain. The gated mechanism learn the domain agnostic representations. They together control the information that has to flow through further fully connected output layer after max pooling.",
        "We find that gated architectures vastly outperform non gated CNN model. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation.",
        "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. This is depicted in Table 1 ."
      ],
      "highlighted_evidence": [
        "The gated mechanism learn the domain agnostic representations. They together control the information that has to flow through further fully connected output layer after max pooling.",
        "The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation.",
        "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. "
      ]
    }
  },
  {
    "paper_id": "1809.09795",
    "question": "Do they evaluate only on English?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
        "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
        "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
        "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .",
        "In Table TABREF1 , we see a notable difference in terms of size among the Twitter datasets. Given this circumstance, and in light of the findings by BIBREF18 , we are interested in studying how the addition of external soft-annotated data impacts on the performance. Thus, in addition to the datasets introduced before, we use two corpora for augmentation purposes. The first dataset was collected using the Twitter API, targeting tweets with the hashtags #sarcasm or #irony, resulting on a total of 180,000 and 45,000 tweets respectively. On the other hand, to obtain non-sarcastic and non-ironic tweets, we relied on the SemEval 2018 Task 1 dataset BIBREF25 . To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems. We later extract all the hashtags in each target dataset and proceed to augment only using those external tweets that contain any of these hashtags. This allows us to, for each class, add a total of 36,835 tweets for the Ptáček corpus, 8,095 for the Riloff corpus and 26,168 for the SemEval-2018 corpus."
      ],
      "highlighted_evidence": [
        "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
        "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 ",
        "Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
        "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
        "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .",
        " To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems."
      ]
    }
  },
  {
    "paper_id": "1812.03593",
    "question": "Is the model evaluated on other datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We evaluated SDNet on CoQA dataset, which improves the previous state-of-the-art model's result by 1.6% (from 75.0% to 76.6%) overall $F_1$ score. The ensemble model further increase the $F_1$ score to $79.3\\%$ . Moreover, SDNet is the first model ever to pass $80\\%$ on CoQA's in-domain dataset."
      ],
      "highlighted_evidence": [
        "We evaluated SDNet on CoQA dataset"
      ]
    }
  },
  {
    "paper_id": "1812.03593",
    "question": "Is the incorporation of context separately evaluated?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.04006",
    "question": "Do they compare to previous models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6. We then compare two models incorporating the unstructured features. In the \"Baseline+Domain Sentences\" model, we consider whether adding the counts of sentences per EHR that involve each of the seven risk factor domains as identified by our topic extraction model improved the model performance. In the \"Baseline+Clinical Sentiment\" model, we evaluate whether adding clinical sentiment scores for each risk factor domain improved the model performance. We also experimented with combining both sets of features and found no additional improvement."
      ],
      "highlighted_evidence": [
        "To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6."
      ]
    }
  },
  {
    "paper_id": "1904.11942",
    "question": "Do the BERT-based embeddings improve results?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table TABREF25 contains the best hyper-parameters and Table TABREF26 contains micro-average F1 scores for both datasets on dev and test sets. We only consider positive pairs, i.e. correct predictions on NONE pairs are excluded for evaluation. In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance. We now provide more detailed analysis and discussion for each dataset."
      ],
      "highlighted_evidence": [
        "In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance."
      ]
    }
  },
  {
    "paper_id": "1608.04917",
    "question": "Do the authors mention any possible confounds in their study?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration. We observed the same phenomenon recently during the Brexit campaign BIBREF38 . Along our interpretation the Brexit was “won” to some extent due to these social media activities, which are practically non-existent among the pro-EU political groups. The fact that ENL and EFDD are the least cohesive groups in the European Parliament can be attributed to their political focus. It seems more important for the group to agree on its anti-EU stance and to call for independence and sovereignty, and much less important to agree on other issues put forward in the parliament."
      ],
      "highlighted_evidence": [
        "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration."
      ]
    }
  },
  {
    "paper_id": "1608.04917",
    "question": "Does the analysis find that coalitions are formed in the same way for different policy areas?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In the area of Economic and monetary system we see a strong cooperation between EPP and S&D (Fig FIGREF42 A), which is on a par with the cohesion of the most cohesive groups (GUE-NGL, S&D, Greens-EFA, ALDE, and EPP), and is above the cohesion of the other groups. As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B. On one hand, we observe cooperation between S&D, ALDE, EPP, and ECR, and on the other, cooperation between GUE-NGL, Greens-EFA, EFDD, ENL, and NI. This division in blocks is seen again in Fig FIGREF42 C, which shows the strongest disagreements. Here, we observe two blocks composed of S&D, EPP, and ALDE on one hand, and GUE-NGL, EFDD, ENL, and NI on the other, which are in strong opposition to each other.",
        "In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.",
        "The patterns of coalitions forming on Twitter closely resemble those in the European Parliament. In Fig FIGREF42 J we see that the strongest degrees of cooperation on Twitter are within the groups. The only group with low cohesion is the NI, whose members have only seven retweets between them. The coalitions on Twitter follow the seating order in the European Parliament remarkably well (see Fig FIGREF42 K). What is striking is that the same blocks form on the left, center, and on the center-right, both in the European Parliament and on Twitter. The largest difference between the coalitions in the European Parliament and on Twitter is on the far-right, where we observe ENL and NI as isolated blocks."
      ],
      "highlighted_evidence": [
        "As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B.",
        "In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.\n\nThe patterns of coalitions forming on Twitter closely resemble those in the European Parliament."
      ]
    }
  },
  {
    "paper_id": "1608.04917",
    "question": "Do they authors account for differences in usage of Twitter amongst MPs into their model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs. The resulting retweet network is an undirected, weighted network.",
        "We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 . The higher the ratio, the more each MEP (on average) retweets the MEPs from the same political group, hence, the higher the cohesion of the political group. The definition of the average retweeets ( INLINEFORM3 ) of a group INLINEFORM4 is: INLINEFORM5"
      ],
      "highlighted_evidence": [
        "The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs",
        "We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 "
      ]
    }
  },
  {
    "paper_id": "1608.04917",
    "question": "Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1908.06138",
    "question": "Does the use of out-of-domain data improve the performance of the method?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants.",
        "This paper presented the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. We presented the results obtained by our system in translating from Czech to Polish. Our system achieved competitive performance ranking second among ten teams in the competition in terms of BLEU score. The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants."
      ],
      "highlighted_evidence": [
        "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants.",
        "The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants."
      ]
    }
  },
  {
    "paper_id": "1801.09030",
    "question": "Do they impose any grammatical constraints over the generated output?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In the TCM prescription generation task, the textual symptom descriptions can be seen as the question and the aim of the task is to produce a set of TCM herbs that form a prescription as the answer to the question. However, the set of herbs is different from the textual answers to a question in the QA task. A difference that is most evident is that there will not be any duplication of herbs in the prescription. However, the basic seq2seq model sometimes produces the same herb tokens repeatedly when applied to the TCM prescription generation task. This phenomenon can hurt the performance of recall rate even after applying a post-process to eliminate repetitions. Because in a limited length of the prescription , the model would produce the same token over and over again, rather than real and novel ones. Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order. In this paper, we explore to automatically generate TCM prescriptions based on textual symptoms. We propose a soft seq2seq model with coverage mechanism and a novel soft loss function. The coverage mechanism is designed to make the model aware of the herbs that have already been generated while the soft loss function is to relieve the side effect of strict order assumption. In the experiment results, our proposed model beats all the baselines in professional evaluations, and we observe a large increase in both the recall rate and the F1 score compared with the basic seq2seq model."
      ],
      "highlighted_evidence": [
        "Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order."
      ]
    }
  },
  {
    "paper_id": "1804.03396",
    "question": "Can this approach model n-ary relations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. Besides, processing longer documents and improving the quality of our benchmark are all challenging problems as we mentioned previously. We hope this work can provide new thoughts for the area of information extraction.",
        "The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. We ignore the adverbials and only consider the entity pairs and their relations as in standard RE settings. Note that we process the entire document as a whole instead of processing individual sentences separately as in previous systems. As shown in Figure 1 , our QA4IE framework consists of four key steps:"
      ],
      "highlighted_evidence": [
        "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper.",
        "The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation."
      ]
    }
  },
  {
    "paper_id": "1804.03396",
    "question": "Was this benchmark automatically created from an existing dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "As mentioned above, step 1, 2 and 4 in the QA4IE framework can be solved by existing work. Therefore, in this paper, we mainly focus on step 3. According to the recent progress in QA and MRC, deep neural networks are very good at solving this kind of problem with a large-scale dataset to train the network. However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark. Inspired by WikiReading BIBREF33 , a recent large-scale QA benchmark over Wikipedia, we find that the articles in Wikipedia together with the high quality triples in knowledge bases such as Wikidata BIBREF34 and DBpedia can form the supervision we need. Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.",
        "Incorporating DBpedia. Unlike WikiData, DBpedia is constructed automatically without human verification. Relations and properties in DBpedia are coarse and noisy. Thus we fix the existing 636 relation types in WikiData and build a projection from DBpedia relations to these 636 relation types. We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations. Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles."
      ],
      "highlighted_evidence": [
        "However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark.",
        "Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.",
        "We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations."
      ]
    }
  },
  {
    "paper_id": "2001.10179",
    "question": "Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1610.09516",
    "question": "Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Street gang members have established online presences coinciding with their physical occupation of neighborhoods. The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . Gang members typically use social networking sites and social media to develop online respect for their street gang BIBREF5 and to post intimidating, threatening images or videos BIBREF6 . This “Cyber-” or “Internet banging” BIBREF7 behavior is precipitated by the fact that an increasing number of young members of the society are joining gangs BIBREF8 , and these young members have become enamored with technology and with the notion of sharing information quickly and publicly through social media. Stronger police surveillance in the physical spaces where gangs congregate further encourages gang members to seek out virtual spaces such as social media to express their affiliation, to sell drugs, and to celebrate their illegal activities BIBREF9 ."
      ],
      "highlighted_evidence": [
        "The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 ."
      ]
    }
  },
  {
    "paper_id": "1708.02267",
    "question": "Do transferring hurt the performance is the corpora are not related?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We also performe another experiment to examine INIT and MULT method for original WikiQA. The F1-score for this dataset is equal to 33.73; however, the average INIT result for both SQuAD and SelQA as initializers is 30.50. In addition, the average results for MULT and ISS-MULT are 31.76 and 32.65, respectively. The result on original WikiQA indicates that all three transfer learning methods not only do not improve the results but also hurt the F1-score. Therefore, SelQA and SQuAD could not estimate a proper initial point for gradient based optimization method. Moreover, these corpora could not refine the error surface of the original WikiQA dataset during optimization for MULT and ISS-MULT method.",
        "These are because other datasets could not add new information to the original dataset or they apparently add some redundant information which are dissimilar to the target dataset. Although ISS-MULT tries to remove this effect and consequently the result is improved, this method is on top of MULT method, and the result is significantly based on the effectiveness of this method."
      ],
      "highlighted_evidence": [
        "The result on original WikiQA indicates that all three transfer learning methods not only do not improve the results but also hurt the F1-score.",
        "These are because other datasets could not add new information to the original dataset or they apparently add some redundant information which are dissimilar to the target dataset."
      ]
    }
  },
  {
    "paper_id": "1708.02267",
    "question": "Is accuracy the only metric they used to compare systems?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In this paper, two main question answering tasks such as answer selection and answer triggering have been examined. In the answer triggering task, there is not a guarantee to have the correct answer among the list of answers. However, in answer selection, there is at least one correct answer among the candidates. As a result, answer triggering is a more challenging task. To report the result for answer selection, MAP and MRR are used; however, the answer triggering task is evaluated by F1-score. The result for MULT Method is reported in Table. 1."
      ],
      "highlighted_evidence": [
        "To report the result for answer selection, MAP and MRR are used; however, the answer triggering task is evaluated by F1-score. The result for MULT Method is reported in Table. 1."
      ]
    }
  },
  {
    "paper_id": "1811.02076",
    "question": "Will these findings be robust through different datasets and different question answering algorithms?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We evaluate a multi-task approach and three algorithms that explicitly model the task dependencies. We perform experiments on document-level variants of the SQuAD dataset BIBREF1 . The contributions for our papers are:"
      ],
      "highlighted_evidence": [
        "We evaluate a multi-task approach and three algorithms that explicitly model the task dependencies."
      ]
    }
  },
  {
    "paper_id": "2001.05672",
    "question": "Is there information about performance of these conversion methods?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2001.05672",
    "question": "Are there some experiments performed in the paper?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.02711",
    "question": "Do they predict the sentiment of the review summary?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "To address this issue, we further investigate a joint encoder for review and summary, which is demonstrated in Figure FIGREF4. The model works by jointly encoding the review and the summary in a multi-layer structure, incrementally updating the representation of the review by consulting the summary representation at each layer. As shown in Figure FIGREF5, our model consists of a summary encoder, a hierarchically-refined review encoder and an output layer. The review encoder is composed of multiple attention layers, each consisting of a sequence encoding layer and an attention inference layer. Summary information is integrated into the representation of the review content at each attention layer, thus, a more abstract review representation is learned in subsequent layers based on a lower-layer representation. This mechanism allows the summary to better guide the representation of the review in a bottom-up manner for improved sentiment classification.",
        "FLOAT SELECTED: Figure 2: Three model structures for incorporating summary into sentiment classification"
      ],
      "highlighted_evidence": [
        "To address this issue, we further investigate a joint encoder for review and summary, which is demonstrated in Figure FIGREF4. The model works by jointly encoding the review and the summary in a multi-layer structure, incrementally updating the representation of the review by consulting the summary representation at each layer.",
        "FLOAT SELECTED: Figure 2: Three model structures for incorporating summary into sentiment classification"
      ]
    }
  },
  {
    "paper_id": "1909.13362",
    "question": "Is the LSTM bidirectional?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:",
        "Both $\\overrightarrow{h_i}$ and $\\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\\times n$."
      ],
      "highlighted_evidence": [
        "We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:\n\nBoth $\\overrightarrow{h_i}$ and $\\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\\times n$."
      ]
    }
  },
  {
    "paper_id": "1608.06111",
    "question": "Do they use pretrained models as part of their parser?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "As a classifier, we use a feed-forward neural network with two hidden layers of 200 tanh units and learning rate set to 0.1, with linear decaying. The input to the network consists of the concatenation of embeddings for words, POS tags and Stanford parser dependencies, one-hot vectors for named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in Table TABREF27 . The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia. For lexical information, we also extract the leftmost (in the order of the aligned words) child (c), leftmost parent (p) and leftmost grandchild (cc). Leftmost and rightmost items are common features for transition-based parsers BIBREF17 , BIBREF18 but we found only leftmost to be helpful in our case. All POS tags, dependencies and named entities are generated using Stanford CoreNLP BIBREF19 . The accuracy of this classifier on the development set is 84%."
      ],
      "highlighted_evidence": [
        "The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia."
      ]
    }
  },
  {
    "paper_id": "1908.01060",
    "question": "Do they test their approach on large-resource tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We first describe our corpus collection. Table. TABREF3 lists all corpora we used in the experiments. There are 16 corpora from 10 languages. To increase the variety of corpus, we selected 4 English corpora and 4 Mandarin corpora in addition to the low resource language corpora. As the target of this experiment is low resource speech recognition, we only randomly select 100,000 utterances even if there are more in each corpus. All corpora are available in LDC, voxforge, openSLR or other public websites. Each corpus is manually assigned one domain based on its speech style. Specifically, the domain candidates are telephone, read and broadcast."
      ],
      "highlighted_evidence": [
        "To increase the variety of corpus, we selected 4 English corpora and 4 Mandarin corpora in addition to the low resource language corpora."
      ]
    }
  },
  {
    "paper_id": "1803.02839",
    "question": "Is there a formal proof that the RNNs form a representation of the group?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Second, we propose embedding schemes that explicitly embed words as elements of a Lie group. In practice, these embedding schemes would involve representing words as constrained matrices, and optimizing the elements, subject to the constraints, according to a loss function constructed from invariants of the matrices, and then applying the matrix log to obtain Lie vectors. A prototypical implementation, in which the words are assumed to be in the fundamental representation of the special orthogonal group, INLINEFORM0 , and are conditioned on losses sensitive to the relative actions of words, is the subject of another manuscript presently in preparation."
      ],
      "highlighted_evidence": [
        "A prototypical implementation, in which the words are assumed to be in the fundamental representation of the special orthogonal group, INLINEFORM0 , and are conditioned on losses sensitive to the relative actions of words, is the subject of another manuscript presently in preparation."
      ]
    }
  },
  {
    "paper_id": "1912.06670",
    "question": "Is audio data per language balanced in dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The data presented in Table (TABREF12) shows the currently available data. Each of the released languages is available for individual download as a compressed directory from the Mozilla Common Voice website. The directory contains six files with Tab-Separated Values (i.e. TSV files), and a single clips subdirectory which contains all of the audio data. Each of the six TSV files represents a different segment of the voice data, with all six having the following column headers: [client_id, path, sentence, up_votes, down_votes, age, gender, accent]. The first three columns refer to an anonymized ID for the speaker, the location of the audio file, and the text that was read. The next two columns contain information on how listeners judged the $<$audio,transcript$>$ pair. The last three columns represent demographic data which was optionally self-reported by the speaker of the audio.",
        "FLOAT SELECTED: Table 1: Current data statistics for Common Voice. Data in italics is as of yet unreleased. Other numbers refer to the data published in the June 12, 2019 release.",
        "We made dataset splits (c.f. Table (TABREF19)) such that one speaker's recordings are only present in one data split. This allows us to make a fair evaluation of speaker generalization, but as a result some training sets have very few speakers, making this an even more challenging scenario. The splits per language were made as close as possible to 80% train, 10% development, and 10% test.",
        "FLOAT SELECTED: Table 2: Data used in the experiments, from an earlier multilingual version of Common Voice. Number of audio clips and unique speakers."
      ],
      "highlighted_evidence": [
        "The data presented in Table (TABREF12) shows the currently available data. Each of the released languages is available for individual download as a compressed directory from the Mozilla Common Voice website. ",
        "FLOAT SELECTED: Table 1: Current data statistics for Common Voice. Data in italics is as of yet unreleased. Other numbers refer to the data published in the June 12, 2019 release.",
        "We made dataset splits (c.f. Table (TABREF19)) such that one speaker's recordings are only present in one data split. ",
        "FLOAT SELECTED: Table 2: Data used in the experiments, from an earlier multilingual version of Common Voice. Number of audio clips and unique speakers."
      ]
    }
  },
  {
    "paper_id": "1905.11037",
    "question": "Do they literally just treat this as \"predict the next spell that appears in the text\"?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "This paper explores instead a new task: action prediction from natural language descriptions of scenes. The challenge is addressed as follows: given a natural language input sequence describing the scene, such as a piece of a story coming from a transcript, the goal is to infer which action is most likely to happen next."
      ],
      "highlighted_evidence": [
        "The challenge is addressed as follows: given a natural language input sequence describing the scene, such as a piece of a story coming from a transcript, the goal is to infer which action is most likely to happen next."
      ]
    }
  },
  {
    "paper_id": "1710.10609",
    "question": "Do they study frequent user responses to help automate modelling of those?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In order to identify frequent user intents, one can use existing clustering algorithms to group together all the utterances from the users. Here each cluster would correspond to a new intent and each utterance in the cluster would correspond to an example for the intent. Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses. As an example, consider agent utterances A.2 in box A and A.2 in box C in Figure FIGREF5 (a). The utterances “Which operating system do you use?\" and “What OS is installed in your machine\" have no syntactic similarity and therefore may not be grouped together. However the fact that these utterances are adjacent to the similar user utterances “I am unable to start notes email client\" and “Unable to start my email client\" provides some evidence that the agent utterances might be similar. Similarly the user utterances “My system keeps getting rebooted\" and “Machine is booting time and again\" ( box B and D in Figure FIGREF5 (a))- that are syntactically not similar - could be grouped together since the adjacent agent utterances, “Is your machine heating up?\" and “Is the machine heating?\" are similar."
      ],
      "highlighted_evidence": [
        "Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses."
      ]
    }
  },
  {
    "paper_id": "1710.10609",
    "question": "Do they use the same distance metric for both the SimCluster and K-means algorithm?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We used the gaussian index from which an utterance pair was generated as the ground truth label, which served to provide ground truth clusters for computation of the above evaluation metrics. Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204."
      ],
      "highlighted_evidence": [
        "Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204."
      ]
    }
  },
  {
    "paper_id": "1909.05478",
    "question": "Is the filter based feature selection (FSE) a form of regularization?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Feature selection is considered an indispensable task in text classification as it removes redundant and irrelevant features of the corpus BIBREF18. Broadly, feature selection approaches can be divided into three classes namely wrapper, embedded, and filter BIBREF7, BIBREF8. In recent years, researchers have proposed various filter based feature selection methods to raise the performance of document text classification BIBREF19.",
        "To develop the vocabulary of most discriminative features, we remove all punctuation symbols and non-significant words (stop words) as a part of the preprocessing step. Furthermore, in order to rank the terms based on their discriminative power among the classes, we use filter based feature selection method named as Normalized Difference Measure (NDM)BIBREF5. Considering the features contour plot, Rehman et al. BIBREF5 suggested that all those features which exist in top left, and bottom right corners of the contour are extremely significant as compared to those features which exist around diagonals. State-of-the-art filter based feature selection algorithms such as ACC2 treat all those features in the same fashion which exist around the diagonals BIBREF5. For instance, ACC2 assigns same rank to those features which has equal difference ($|t_{pr} -f_{pr}|$) value but different $t_{pr}$ and $f_{pr}$ values. Whereas NDM normalizes the difference ($|t_{pr} -f_{pr}|$) with the minimum of $t_{pr}$ and $f_{pr}$ (min($t_{pr}$, $f_{pr}$)) and assign different rank to those terms which have same difference value. Normalized Difference Measure (NDM) considers those features highly significant which have the following properties:"
      ],
      "highlighted_evidence": [
        " In recent years, researchers have proposed various filter based feature selection methods to raise the performance of document text classification BIBREF19.",
        " Furthermore, in order to rank the terms based on their discriminative power among the classes, we use filter based feature selection method named as Normalized Difference Measure (NDM)BIBREF5. Considering the features contour plot, Rehman et al. BIBREF5 suggested that all those features which exist in top left, and bottom right corners of the contour are extremely significant as compared to those features which exist around diagonals. State-of-the-art filter based feature selection algorithms such as ACC2 treat all those features in the same fashion which exist around the diagonals BIBREF5."
      ]
    }
  },
  {
    "paper_id": "1908.06006",
    "question": "Do they compare to other models appart from HAN?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5. They fall into two categories: topic classification (Yahoo) and fine-grained sentiment analysis (Amazon, Yelp). Dataset statistics are shown in Table TABREF29. Classes are perfectly balanced, for all datasets."
      ],
      "highlighted_evidence": [
        "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5."
      ]
    }
  },
  {
    "paper_id": "1807.07961",
    "question": "Do they evaluate only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 1: Tweet examples with emojis. The sentiment ground truth is given in the second column. The examples show that inconsistent sentiments exist between emojis and texts.",
        "We construct our own Twitter sentiment dataset by crawling tweets through the REST API which consists of 350,000 users and is magnitude larger comparing to previous work. We collect up to 3,200 tweets from each user and follow the standard tweet preprocessing procedures to remove the tweets without emojis and tweets containing less than ten words, and contents including the urls, mentions, and emails.",
        "For acquiring the sentiment annotations, we first use Vader which is a rule-based sentiment analysis algorithm BIBREF17 for text tweets only to generate weak sentiment labels. The algorithm outputs sentiment scores ranging from -1 (negative) to 1 (positive) with neutral in the middle. We consider the sentiment analysis as a binary classification problem (positive sentiment and negative sentiment), we filter out samples with weak prediction scores within INLINEFORM0 and keep the tweets with strong sentiment signals. Emoji occurrences are calculated separately for positive tweets and negative tweets, and threshold is set to 2,000 to further filter out emojis which are less frequently used in at least one type of sentimental text. In the end, we have constructed a dataset with 1,492,065 tweets and 55 frequently used emojis in total. For the tweets with an absolute sentiment score over 0.70, we keep the auto-generated sentiment label as ground truth because the automatic annotation is reliable with high sentiment scores. On the other hand, we select a subset of the tweets with absolute sentiment scores between INLINEFORM1 for manual labeling by randomly sampling, following the distribution of emoji occurrences where each tweet is labeled by two graduate students. Tweets are discarded if the two annotations disagree with each other or they are labeled as neutral. In the end, we have obtained 4,183 manually labeled tweets among which 60% are used for fine-tuning and 40% are used for testing purposes. The remainder of the tweets with automatic annotations are divided into three sets: 60% are used for pre-training the bi-sense and conventional emoji embedding, 10% for validation and 30% are for testing. We do not include a “neutral” class because it is difficult to obtain valid neutral samples. For auto-generated labels, the neutrals are the samples with low absolute confidence scores and their sentiments are more likely to be model failures other than “true neutrals”. Moreover, based on the human annotations, most of the tweets with emojis convey non-neutral sentiment and only few neutral samples are observed during the manual labeling which are excluded from the manually labeled subset."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 1: Tweet examples with emojis. The sentiment ground truth is given in the second column. The examples show that inconsistent sentiments exist between emojis and texts.",
        "We construct our own Twitter sentiment dataset by crawling tweets through the REST API which consists of 350,000 users and is magnitude larger comparing to previous work. We collect up to 3,200 tweets from each user and follow the standard tweet preprocessing procedures to remove the tweets without emojis and tweets containing less than ten words, and contents including the urls, mentions, and emails.",
        "For acquiring the sentiment annotations, we first use Vader which is a rule-based sentiment analysis algorithm BIBREF17 for text tweets only to generate weak sentiment labels. "
      ]
    }
  },
  {
    "paper_id": "1909.02776",
    "question": "Is new approach tested against state of the art?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Another insight gained from these charts is that a random summarizer resulted in scores more than 50% in all measures, and without using document-aware features, the model achieves a small improvement over a random summarizer."
      ],
      "highlighted_evidence": [
        "Another insight gained from these charts is that a random summarizer resulted in scores more than 50% in all measures, and without using document-aware features, the model achieves a small improvement over a random summarizer."
      ]
    }
  },
  {
    "paper_id": "1911.00133",
    "question": "Is the dataset balanced across categories?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We submit 4,000 segments, sampled equally from each domain and uniformly within domains, to Mechanical Turk to be annotated by at least five Workers each and include in each batch one of 50 “check questions” which have been previously verified by two in-house annotators. After removing annotations which failed the check questions, and data points for which at least half of the annotators selected “Can't Tell”, we are left with 3,553 labeled data points from 2,929 different posts. We take the annotators' majority vote as the label for each segment and record the percentage of annotators who agreed. The resulting dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful."
      ],
      "highlighted_evidence": [
        "The resulting dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful."
      ]
    }
  },
  {
    "paper_id": "1709.05413",
    "question": "Do they evaluate only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 1: Example Twitter Customer Service Conversation"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 1: Example Twitter Customer Service Conversation"
      ]
    }
  },
  {
    "paper_id": "1704.00253",
    "question": "Where do they collect the synthetic data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model for En $\\rightarrow $ De and train another NMT model for En $\\rightarrow $ Fr using the WMT'15 En-Fr parallel corpus BIBREF19 . A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together."
      ],
      "highlighted_evidence": [
        "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model for En $\\rightarrow $ De and train another NMT model for En $\\rightarrow $ Fr using the WMT'15 En-Fr parallel corpus BIBREF19 ."
      ]
    }
  },
  {
    "paper_id": "1908.00153",
    "question": "Do they analyze what type of content Arabic bots spread in comparison to English?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1908.00153",
    "question": "Do they propose a new model to better detect Arabic bots specifically?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this work, we build a novel regression model, based on linguistic, content, behavioral and topic features to detect Arabic Twitter bots to understand the impact of bots in spreading religious hatred in Arabic Twitter space. In particular, we quantitatively code and analyze a representative sample of 450 accounts disseminating hate speech from the dataset constructed in our previous work BIBREF18 , BIBREF19 for bot-like behavior. We compare our assigned bot-likelihood scores to those of Botometer BIBREF14 , a well-known machine-learning-based bot detection tool, and we show that Botometer performs a little above average in detecting Arabic bots. Based on our analysis, we build a predictive regression model and train it on various sets of features and show that our regression model outperforms Botometer's by a significant margin (31 points in Spearman's rho). Finally, we provide a large-scale analysis of predictive features that distinguish bots from humans in terms of characteristics and behaviors within the context of social media."
      ],
      "highlighted_evidence": [
        "In this work, we build a novel regression model, based on linguistic, content, behavioral and topic features to detect Arabic Twitter bots to understand the impact of bots in spreading religious hatred in Arabic Twitter space. "
      ]
    }
  },
  {
    "paper_id": "2002.02562",
    "question": "Does model uses pretrained Transformer encoders?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Our Transformer Transducer model architecture has 18 audio and 2 label encoder layers. Every layer is identical for both audio and label encoders. The details of computations in a layer are shown in Figure FIGREF10 and Table TABREF11. All the models for experiments presented in this paper are trained on 8x8 TPU with a per-core batch size of 16 (effective batch size of 2048). The learning rate schedule is ramped up linearly from 0 to $2.5\\mathrm {e}{-4}$ during first 4K steps, it is then held constant till 30K steps and then decays exponentially to $2.5\\mathrm {e}{-6}$ till 200K steps. During training we also added a gaussian noise($\\mu =0,\\sigma =0.01$) to model weights BIBREF24 starting at 10K steps. We train this model to output grapheme units in all our experiments. We found that the Transformer Transducer models trained much faster ($\\approx 1$ day) compared to the an LSTM-based RNN-T model ($\\approx 3.5$ days), with a similar number of parameters."
      ],
      "highlighted_evidence": [
        "Our Transformer Transducer model architecture has 18 audio and 2 label encoder layers. Every layer is identical for both audio and label encoders. The details of computations in a layer are shown in Figure FIGREF10 and Table TABREF11. All the models for experiments presented in this paper are trained on 8x8 TPU with a per-core batch size of 16 (effective batch size of 2048)."
      ]
    }
  },
  {
    "paper_id": "1911.03154",
    "question": "Has there been previous work on SNMT?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Recently, a number of researchers have endeavored to explore methods for simultaneous translation in the context of NMT BIBREF6, BIBREF7, BIBREF8, BIBREF9. Some of them propose sophisticated training frameworks explicitly designed for simultaneous translation BIBREF5, BIBREF10. These approaches are either memory inefficient during training BIBREF5 or hard to implement BIBREF10. Others utilize a full-sentence base model to perform simultaneous translation by modifications to the encoder and the decoding process. To match the incremental source context, they replace the bidirectional encoder with a left-to-right encoder BIBREF3, BIBREF11, BIBREF4, BIBREF12 or recompute the encoder hidden states BIBREF13. On top of that, heuristic algorithms BIBREF3, BIBREF14 or a READ/WRITE model trained with reinforcement learning BIBREF11, BIBREF4, BIBREF12 or supervised learning BIBREF13 are used to decide, at every step, whether to wait for the next source token or output a target token. However, these models either cannot directly use a pretrained vanilla CNMT model with bidirectional encoder as the base model or work in a sub-optimal way in the decoding stage."
      ],
      "highlighted_evidence": [
        "Recently, a number of researchers have endeavored to explore methods for simultaneous translation in the context of NMT BIBREF6, BIBREF7, BIBREF8, BIBREF9. Some of them propose sophisticated training frameworks explicitly designed for simultaneous translation BIBREF5, BIBREF10. "
      ]
    }
  },
  {
    "paper_id": "1805.09959",
    "question": "Do the authors report results only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our study focused primarily on English tweets, since this was the language of our diagnostic training sample. Future studies could incorporate other languages using our proposed framework. It would be important to also expand the API queries with translations of `breast' and `cancer'. This could allow for a cross cultural comparison of how social media influences patients and what patients express on social media."
      ],
      "highlighted_evidence": [
        "Our study focused primarily on English tweets, since this was the language of our diagnostic training sample."
      ]
    }
  },
  {
    "paper_id": "1909.05360",
    "question": "Is this the first paper to propose a joint model for event and temporal relation extraction?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1). The motivation is that if we train the relation classifier with NONE relations between non-events, then it will potentially have the capability of correcting event extraction mistakes. For instance in Fig. FIGREF1, if the relation classifier predicts NONE for (Hutu, war) with a high confidence, then this is a strong signal that can be used by the event classifier to infer that at least one of them is not an event."
      ],
      "highlighted_evidence": [
        "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1)."
      ]
    }
  },
  {
    "paper_id": "1811.01088",
    "question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 1: GLUE results with and without STILTs, fine-tuning on full training data of each target task. Bold marks the best within each section. Strikethrough indicates cases where the intermediate task is the same as the target task—we substitute the baseline result for that cell. A.Ex is the average excluding MNLI and QQP because of the overlap with intermediate tasks. See text for discussion of WNLI results. Test results on STILTs uses the supplementary training regime for each task based on the performance on the development set, corresponding to the numbers shown in Best of Each. The aggregated GLUE scores differ from the public leaderboard because we report performance on QNLIv1."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 1: GLUE results with and without STILTs, fine-tuning on full training data of each target task. Bold marks the best within each section. Strikethrough indicates cases where the intermediate task is the same as the target task—we substitute the baseline result for that cell. A.Ex is the average excluding MNLI and QQP because of the overlap with intermediate tasks. See text for discussion of WNLI results. Test results on STILTs uses the supplementary training regime for each task based on the performance on the development set, corresponding to the numbers shown in Best of Each. The aggregated GLUE scores differ from the public leaderboard because we report performance on QNLIv1."
      ]
    }
  },
  {
    "paper_id": "1811.01088",
    "question": "Does the additional training on supervised tasks hurt performance in some tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Table 1 shows our results on GLUE with and without STILTs. Our addition of supplementary training boosts performance across many of the two sentence tasks. On each of our models trained with STILTs, we show improved overall average GLUE scores on the development set. For MNLI and QNLI target tasks, we observe marginal or no gains, likely owing to the two tasks already having large training sets. For the two single sentence tasks—the syntax-oriented CoLA task and the SST sentiment task—we find somewhat deteriorated performance. For CoLA, this mirrors results reported in BIBREF10 , who show that few pretraining tasks other than language modeling offer any advantage for CoLA. The Overall Best score is computed based on taking the best score for each task."
      ],
      "highlighted_evidence": [
        "For the two single sentence tasks—the syntax-oriented CoLA task and the SST sentiment task—we find somewhat deteriorated performance."
      ]
    }
  },
  {
    "paper_id": "1909.12642",
    "question": "Is the model tested for language identification?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.12642",
    "question": "Is the model compared to a baseline model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.05238",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Dos and Don'ts for the Moral Choice Machine. The verb extraction identifies the most positive and most negative associated verbs in vocabulary, to infer socially desired and neglected behaviour. BIBREF0 (BIBREF0) extracted them with the general positive and negative association sets on the Google Slim embedding. Since those sets are expected to reflect social norms, they are referred as Dos and Don'ts hereafter.",
        "Summarized, even though the contained positive verbs are quite diverse, all of them carry a positive attitude. Some of the verbs are related to celebration or travelling, others to love matters or physical closeness. All elements of the above set are rather of general and unspecific nature. Analogously, some of the negative words just describe inappropriate behaviour, like slur or misdeal, whereas others are real crimes as murder. As BIBREF0 (BIBREF0) describe, the listed words can be accepted as commonly agreed Dos and Don'ts."
      ],
      "highlighted_evidence": [
        "Since those sets are expected to reflect social norms, they are referred as Dos and Don'ts hereafter.",
        "Analogously, some of the negative words just describe inappropriate behaviour, like slur or misdeal, whereas others are real crimes as murder."
      ]
    }
  },
  {
    "paper_id": "1910.11790",
    "question": "was bert used?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "BERT BIBREF6 is a state-of-the-art model, which has been pre-trained on a large corpus and is suitable to be fine-tuned for various downstream NLP tasks. The main innovation between this model and existing language models is in how the model is trained. For BERT, the text conditioning happens on both the left and right context of every word and is therefore bidirectional. In previous models BIBREF7, a unidirectional language model was usually used in the pre-training. With BERT, two fully unsupervised tasks are performed. The Masked Language Model and the Next Sentence Prediction (NSP).",
        "For this study, the NSP is used as a proxy for the relevance of response. Furthermore, in order to improve performance, we fine-tune on a customized dataset which achieved an accuracy of 82.4%. For the main analysis, we used the single-turn dataset, which gave us a correlation of 0.28 between the mean of the AMT evaluation and the BERT NSP. Next, we put each score into a category. For example, if the average score is 2.3, this would be placed in category 2. We then displayed the percentage of positive and negative predictions in a histogram for each of the categories. As seen in Figure FIGREF5, a clear pattern is seen between the higher scores and the positive prediction, and the lower scores and the negative predictions. details of how they are combined to create a final classification layer."
      ],
      "highlighted_evidence": [
        "With BERT, two fully unsupervised tasks are performed. The Masked Language Model and the Next Sentence Prediction (NSP).\n\nFor this study, the NSP is used as a proxy for the relevance of response."
      ]
    }
  },
  {
    "paper_id": "2003.01472",
    "question": "Did they experiment with the tool?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We present two use cases on which Seshat was developped: clinical interviews, and daylong child-centered recordings."
      ],
      "highlighted_evidence": [
        "We present two use cases on which Seshat was developped: clinical interviews, and daylong child-centered recordings."
      ]
    }
  },
  {
    "paper_id": "2003.01472",
    "question": "Is this software available to the public?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Setting up a modern fully-fledged web service is a arduous task, usually requiring a seasoned system administrator as well as sometimes having very precise system requirements. Luckily, the Docker virtualisation platform ensures that anyone with a recent-enough install of that software can set up Seshat in about one command (while still allowing some flexibility via a configuration file). For those willing to have a more tightly-controlled installation of Seshat on their system, we also fully specify the manual installation steps in our online documentation)."
      ],
      "highlighted_evidence": [
        "For those willing to have a more tightly-controlled installation of Seshat on their system, we also fully specify the manual installation steps in our online documentation)."
      ]
    }
  },
  {
    "paper_id": "1806.02908",
    "question": "Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2001.02380",
    "question": "Are some models evaluated using this metric, what are the findings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "As a framework, we use the sentence classifier configuration of FLAIR BIBREF46 with a biLSTM encoder/classifier architecture fed by character and word level representations composed of a concatenation of fixed 300 dimensional GloVe embeddings BIBREF47, pre-trained contextualized FLAIR word embeddings, and pre-trained contextualized character embeddings from AllenNLP BIBREF48 with FLAIR's default hyperparameters. The model's architecture is shown in Figure FIGREF30."
      ],
      "highlighted_evidence": [
        "As a framework, we use the sentence classifier configuration of FLAIR BIBREF46 with a biLSTM encoder/classifier architecture fed by character and word level representations composed of a concatenation of fixed 300 dimensional GloVe embeddings BIBREF47, pre-trained contextualized FLAIR word embeddings, and pre-trained contextualized character embeddings from AllenNLP BIBREF48 with FLAIR's default hyperparameters."
      ]
    }
  },
  {
    "paper_id": "1912.10167",
    "question": "Are any machine translation sysems tried with these embeddings, what is the performance?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.10167",
    "question": "Are any experiments performed to try this approach to word embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In the literature, two main types of datasets are used for machine translation: Word-aligned data and Sentence-aligned data. The first one is basically a dictionary between the two languages, where there is a direct relation between same words in different languages. The second one has the relation between corresponding sentences in the two languages. We decided to start with the sentence aligned corpus, since it was more interesting to infer dependency from contexts among words. For our experiment we decided to use the Europarl dataset, using the data from the WMT11 .The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It includes versions in 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek. For this experience, we used the English-French parallel corpus, which contains 2,007,723 sentences and the English-Italian corpus, that contains 1,909,115 sentences."
      ],
      "highlighted_evidence": [
        "For our experiment we decided to use the Europarl dataset, using the data from the WMT11 ."
      ]
    }
  },
  {
    "paper_id": "1805.11535",
    "question": "Is this a task other people have worked on?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We propose a novel problem of relationship recommendation (RSR). Different from the reciprocal recommendation problem on DSNs, our RSR task operates on regular social networks (RSN), estimating long-term and serious relationship compatibility based on social posts such as tweets."
      ],
      "highlighted_evidence": [
        "We propose a novel problem of relationship recommendation (RSR). Different from the reciprocal recommendation problem on DSNs, our RSR task operates on regular social networks (RSN), estimating long-term and serious relationship compatibility based on social posts such as tweets."
      ]
    }
  },
  {
    "paper_id": "1804.08094",
    "question": "Did they experiment with pre-training schemes?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2004.04228",
    "question": "Do they use crowdsourcing to collect human judgements?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We collect human judgments on Amazon Mechanical Turk via ParlAI BIBREF18. We present summaries one sentence at a time, along with the entire article. For each summary sentence, the annotator makes a binary decision as to whether the sentence is factually consistent with the article. Workers are instructed to mark non-grammatical sentences as not consistent, and copies of article sentences as consistent. Workers are paid $1 per full summary annotated. See Appendix SECREF10 for further details."
      ],
      "highlighted_evidence": [
        "We collect human judgments on Amazon Mechanical Turk via ParlAI BIBREF18."
      ]
    }
  },
  {
    "paper_id": "1903.07398",
    "question": "Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.",
        "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .",
        "Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality."
      ],
      "highlighted_evidence": [
        "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models.",
        "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 ",
        "Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality."
      ]
    }
  },
  {
    "paper_id": "1909.00161",
    "question": "Do they use pretrained models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this work, we make use of the widely-recognized state of the art entailment technique – BERT BIBREF18, and train it on three mainstream entailment datasets: MNLI BIBREF19, GLUE RTE BIBREF20, BIBREF21 and FEVER BIBREF22, respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “non-entailment”.",
        "For our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all $\\textsc {0shot-tc}$ aspects. For label-partially-unseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, then fine-tune on the provided training data."
      ],
      "highlighted_evidence": [
        "In this work, we make use of the widely-recognized state of the art entailment technique – BERT BIBREF18, and train it on three mainstream entailment datasets: MNLI BIBREF19, GLUE RTE BIBREF20, BIBREF21 and FEVER BIBREF22, respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “non-entailment”.\n\nFor our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all $\\textsc {0shot-tc}$ aspects. For label-partially-unseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, then fine-tune on the provided training data."
      ]
    }
  },
  {
    "paper_id": "1910.03484",
    "question": "Do they use attention?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The NLG model is a seq2seq model with attention as described in section SECREF2. It takes as input a MR and generates a natural language text. The objective is to find the model parameters $\\theta ^{nlg}$ such that they minimize the loss which is defined as follows:"
      ],
      "highlighted_evidence": [
        "The NLG model is a seq2seq model with attention as described in section SECREF2."
      ]
    }
  },
  {
    "paper_id": "1808.10113",
    "question": "Did they compare to Transformer based large language models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We compared our models with the following state-of-the-art baselines:",
        "Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .",
        "Hierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.",
        "HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.",
        "HLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.",
        "HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
      ],
      "highlighted_evidence": [
        "We compared our models with the following state-of-the-art baselines:\n\nSequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\nHierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\nHLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\nHLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\nHLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
      ]
    }
  },
  {
    "paper_id": "1708.00111",
    "question": "Do they provide a framework for building a sub-differentiable for any final loss metric?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We introduce a surrogate training objective that avoids these problems and as a result is fully continuous. In order to accomplish this, we propose a continuous relaxation to the composition of our final loss metric, INLINEFORM0 , and our decoder function, INLINEFORM1 : INLINEFORM2",
        "Specifically, we form a continuous function softLB that seeks to approximate the result of running our decoder on input INLINEFORM0 and then evaluating the result against INLINEFORM1 using INLINEFORM2 . By introducing this new module, we are now able to construct our surrogate training objective: DISPLAYFORM0"
      ],
      "highlighted_evidence": [
        "We introduce a surrogate training objective that avoids these problems and as a result is fully continuous. ",
        "Specifically, we form a continuous function softLB that seeks to approximate the result of running our decoder on input INLINEFORM0 and then evaluating the result against INLINEFORM1 using INLINEFORM2 ."
      ]
    }
  },
  {
    "paper_id": "1708.00111",
    "question": "Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "However, to reduce the gap between the training procedure and test procedure, we also experimented with soft beam search decoding. This decoding approach closely follows Algorithm SECREF7 , but along with soft back pointers, we also compute hard back pointers at each time step. After computing all the relevant quantities like model score, loss etc., we follow the hard backpointers to obtain the best sequence INLINEFORM0 . This is very different from hard beam decoding because at each time step, the selection decisions are made via our soft continuous relaxation which influences the scores, LSTM hidden states and input embeddings at subsequent time-steps. The hard backpointers are essentially the MAP estimate of the soft backpointers at each step. With small, finite INLINEFORM1 , we observe differences between soft beam search and hard beam search decoding in our experiments."
      ],
      "highlighted_evidence": [
        "This decoding approach closely follows Algorithm SECREF7 , but along with soft back pointers, we also compute hard back pointers at each time step. After computing all the relevant quantities like model score, loss etc., we follow the hard backpointers to obtain the best sequence INLINEFORM0 ."
      ]
    }
  },
  {
    "paper_id": "1911.03562",
    "question": "Are the academically younger authors cited less than older?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 30 Aggregate citation statistics by academic age."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 30 Aggregate citation statistics by academic age."
      ]
    }
  },
  {
    "paper_id": "1903.00058",
    "question": "Does their combination of a non-parametric retrieval and neural network get trained end-to-end?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The Transformer baselines are trained on 16 GPUs, with the learning rate, warm-up schedule and batching scheme described in BIBREF6 . The semi-parametric models were trained on 32 GPUs with each replica split over 2 GPUs, one to train the translation model and the other for computing the CSTM. We used a conservative learning rate schedule (3, 40K) BIBREF8 to train the semi-parametric models."
      ],
      "highlighted_evidence": [
        "The semi-parametric models were trained on 32 GPUs with each replica split over 2 GPUs, one to train the translation model and the other for computing the CSTM."
      ]
    }
  },
  {
    "paper_id": "1909.04181",
    "question": "Does the paper report F1-scores for the age and language variety tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.04181",
    "question": "Are the models compared to some baseline models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i.e., $W \\sim N(0,1)$. We use a maximum sequence length of 50 tokens, and choose an arbitrary vocabulary size of 100,000 types, where we use the 100,000 most frequent words in TRAIN. To avoid over-fitting, we use dropout BIBREF2 with a rate of 0.5 on the hidden layer. For the training, we use the Adam BIBREF3 optimizer with a fixed learning rate of $1e-3$. We employ batch training with a batch size of 32 for this model. We train the network for 15 epochs and save the model at the end of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs."
      ],
      "highlighted_evidence": [
        "Our baseline is a GRU network for each of the three tasks."
      ]
    }
  },
  {
    "paper_id": "1901.03438",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We can see at least three reasons for these observed correlations. First, some correlations can be attributed to overlapping feature definitions. For instance, expletive arguments (e.g. There are birds singing) are, by definition, non-canonical arguments, and thus are a subset of add arg. However, some added arguments, such as benefactives (Bo baked Mo a cake), are not expletives. Second, some correlations can be attributed to grammatical properties of the relevant constructions. For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?). Third, some correlations may be a consequence of the sources sampled in CoLA and the phenomena they focus on. For instance, the unusually high correlation of Emb-Q and ellipsis/anaphor can be attributed to BIBREF18 , which is an article about the sluicing construction involving ellipsis of an embedded interrogative (e.g. I saw someone, but I don't know who).",
        "Expletives, or “dummy” arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives. Arguments are usually selected for by the head, and they are generally not optional. In this case, the expletive occupies a syntactic argument slot, but it is not semantically selected by the verb, and there is often a syntactic variation without the expletive. See [p.170-172]adger2003core and [p.82-83]kim2008syntax."
      ],
      "highlighted_evidence": [
        "For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?). ",
        "Expletives, or “dummy” arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives."
      ]
    }
  },
  {
    "paper_id": "1808.09920",
    "question": "Did they use a relation extraction method to construct the edges in the graph?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "To each node $v_i$ , we associate a continuous annotation $\\mathbf {x}_i \\in \\mathbb {R}^D$ which represents an entity in the context where it was mentioned (details in Section \"Node annotations\" ). We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges). Note that MATCH edges when connecting mentions in the same document are mostly included in the set of edges predicted by the coreference system. Having the two types of edges lets us distinguish between less reliable edges provided by the coreference system and more reliable (but also more sparse) edges given by the exact-match heuristic. We treat these three types of connections as three different types of relations. See Figure 2 for an illustration. In addition to that, and to prevent having disconnected graphs, we add a fourth type of relation (COMPLEMENT edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with respect to a fully connected graph."
      ],
      "highlighted_evidence": [
        "We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges)."
      ]
    }
  },
  {
    "paper_id": "2002.08899",
    "question": "Does having constrained neural units imply word meanings are fixed across different context?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In a natural language translation setting, suppose that an input word corresponds to a set of output tokens independently of its context. Even though this information might be useful to determine the syntax of the input utterance in the first place, the syntax does not determine this knowledge at all (by supposition). So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories. This scenario is refuted by cognitive and neural theories. We incorporate the regularization with neural units that can separate representations of word meaning and arrangement."
      ],
      "highlighted_evidence": [
        "So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories."
      ]
    }
  },
  {
    "paper_id": "2002.08899",
    "question": "Do they perform a quantitative analysis of their model displaying knowledge distortions?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Additionally, we provide evidence that the model learns knowledge of a separation between syntax and the lexicon that is similar to that of a human. Figure FIGREF6 displays the learned $\\sigma (w)$ embeddings for some input words, across the domains. To avoid cherry-picking the results, we chose the input words arbitrarily, subject to the following constraint. We considered each word to typically have a different syntactic category than the other choices from that domain. This constraint was used to present a diverse selection of words. Table TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia. To avoid cherry-picking the results, we arbitrarily chose an input for each domain, subject to the following constraint. The input is not in the train set and the undamaged LLA-LSTM model produces a translation that we judge to be correct. For all inputs that we chose, damage to the analog of Broca's area (the LSTMs) results in an output that describes content only if it is described by the input. However, the output does not show understanding of the input's syntax. In the naturalistic domains, damage to the analog of Wernicke's area (the Lexicon Unit) results in an output with incorrect content that would be acceptable if the input had different words but the same syntax. These knowledge distortions are precisely those that are expected in the respective human aphasics BIBREF0. We also provide corpus-level results from the damaged models by presenting mean precision on the test sets. Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision."
      ],
      "highlighted_evidence": [
        "TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia.",
        "Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision."
      ]
    }
  },
  {
    "paper_id": "1809.08652",
    "question": "Do all the instances contain code-switching?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary. To deal with the spelling variations, we manually added some common variations of popular Hinglish words. Final dictionary comprised of 7200 word pairs. Additionally, to deal with profane words, which are not present in Xlit-Crowd, we had to make a profanity dictionary (with 209 profane words) as well. Table TABREF3 gives some examples from the dictionary."
      ],
      "highlighted_evidence": [
        "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . ",
        "This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary. "
      ]
    }
  },
  {
    "paper_id": "1809.08652",
    "question": "Do they perform some annotation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary. To deal with the spelling variations, we manually added some common variations of popular Hinglish words. Final dictionary comprised of 7200 word pairs. Additionally, to deal with profane words, which are not present in Xlit-Crowd, we had to make a profanity dictionary (with 209 profane words) as well. Table TABREF3 gives some examples from the dictionary.",
        "Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). Some examples from the dataset are shown in Table TABREF4 . We use a LSTM based classifier model for training our model to classify these tweets into these three categories. An overview of the model is given in the Figure FIGREF12 . The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. Categorical crossentropy loss was used for the last layer due to the presence of multiple classes. We use Adam optimizer along with L2 regularisation to prevent overfitting. As indicated by the Figure FIGREF12 , the model was initially trained on the dataset provided by BIBREF1 , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search."
      ],
      "highlighted_evidence": [
        "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . ",
        "Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign)"
      ]
    }
  },
  {
    "paper_id": "1809.08652",
    "question": "Do they use dropout?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). Some examples from the dataset are shown in Table TABREF4 . We use a LSTM based classifier model for training our model to classify these tweets into these three categories. An overview of the model is given in the Figure FIGREF12 . The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. Categorical crossentropy loss was used for the last layer due to the presence of multiple classes. We use Adam optimizer along with L2 regularisation to prevent overfitting. As indicated by the Figure FIGREF12 , the model was initially trained on the dataset provided by BIBREF1 , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search."
      ],
      "highlighted_evidence": [
        "The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. "
      ]
    }
  },
  {
    "paper_id": "2002.06053",
    "question": "Are datasets publicly available?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups. The use of FAIR (Findable, Accessible, Interoperable and Reusable) (meta)data principles can guide the management of scientific data BIBREF125. Automated workflows that are easy to use and do not require programming knowledge encourage the flow of information from one discipline to the other. Platform-free solutions such as Docker (docker.com) in which an image of the source code is saved and can be opened without requiring further installation could accelerate the reproduction process. A recent initiative to provide a unified-framework for predictive models in genomics can quickly be adopted by the medicinal chemistry community BIBREF126."
      ],
      "highlighted_evidence": [
        "Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups."
      ]
    }
  },
  {
    "paper_id": "2002.06053",
    "question": "Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The Word2Vec architecture has inspired a great deal of research in the bio/cheminformatics domains. The Word2Vec algorithm has been successfully applied for determining protein classes BIBREF44 and protein-protein interactions (PPI) BIBREF56. BIBREF44 treated 3-mers as the words of the protein sequence and observed that 3-mers with similar biophysical and biochemical properties clustered together when their embeddings were mapped onto the 2D space. BIBREF56, on the other hand, utilized BPE-based word segmentation (i.e. bio-words) to determine the words. The authors argued that the improved performance for bio-words in the PPI prediction task might be due to the segmentation-based model providing more distinct words than $k$-mers, which include repetitive segments. Another recent study treated multi-domain proteins as sentences in which each domain was recognized as a word BIBREF60. The Word2Vec algorithm was trained on the domains (i.e. PFAM domain identifiers) of eukaryotic protein sequences to learn semantically interpretable representations of them. The domain representations were then investigated in terms of the Gene Ontology (GO) annotations that they inherit. The results indicated that semantically similar domains share similar GO terms."
      ],
      "highlighted_evidence": [
        "The Word2Vec architecture has inspired a great deal of research in the bio/cheminformatics domains. The Word2Vec algorithm has been successfully applied for determining protein classes BIBREF44 and protein-protein interactions (PPI) BIBREF56."
      ]
    }
  },
  {
    "paper_id": "1712.03547",
    "question": "Do the authors analyze what kinds of cases their new embeddings fail in where the original, less-interpretable embeddings didn't?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1703.05916",
    "question": "did they use a crowdsourcing platform for annotations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset."
      ],
      "highlighted_evidence": [
        "Crowdsourced annotators assigned similarity to word pairs during the word similarity task. "
      ]
    }
  },
  {
    "paper_id": "1909.04453",
    "question": "Does the performance necessarily drop when more control is desired?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Performance INLINEFORM0 Trade-off: To see if the selector affects performance, we also ask human annotators to judge the text fluency. The fluency score is computed as the average number of text being judged as fluent. We include generations from the standard Enc-Dec model. Table TABREF32 shows the best fluency is achieved for Enc-Dec. Imposing a content selector always affects the fluency a bit. The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text. If the selector is not perfectly trained, the fluency will inevitably be influenced. When the controllability is weaker, like in RS, the fluency is more stable because it will not be affected much by the selection mask. For SS and Bo.Up, the drop of fluency is significant because of the gap of soft approximation and the independent training procedure. In general, VRS does properly decouple content selection from the enc-dec architecture, with only tiny degrade on the fluency."
      ],
      "highlighted_evidence": [
        "The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text. If the selector is not perfectly trained, the fluency will inevitably be influenced. When the controllability is weaker, like in RS, the fluency is more stable because it will not be affected much by the selection mask."
      ]
    }
  },
  {
    "paper_id": "1909.04556",
    "question": "Is this auto translation tool based on neural networks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "CodeInternational: A tool which can translate code between human languages, powered by Google Translate."
      ],
      "highlighted_evidence": [
        "CodeInternational: A tool which can translate code between human languages, powered by Google Translate.\n\n"
      ]
    }
  },
  {
    "paper_id": "2002.09637",
    "question": "Is the proposed method compared to previous methods?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases."
      ],
      "highlighted_evidence": [
        "Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases."
      ]
    }
  },
  {
    "paper_id": "1811.01001",
    "question": "Are the unobserved samples from the same distribution as the training data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In the present work, we address these limitations by providing a more nuanced evaluation of the learning capabilities of RNNs. In particular, we investigate the effects of three different aspects of a network's generalization: data distribution, length-window, and network capacity. We define an informative protocol for assessing the performance of RNNs: Instead of training a single network until it has learned its training set and then evaluating it on its test set, as BIBREF9 do in their study, we monitor and test the network's performance at each epoch during the entire course of training. This approach allows us to study the stability of the solutions reached by the network. Furthermore, we do not restrict ourselves to a test set of sequences of fixed lengths during testing. Rather, we exhaustively enumerate all the sequences in a language by their lengths and then go through the sequences in the test set one by one until our network errs $k$ times, thereby providing a more fine-grained evaluation criterion of its generalization capabilities.",
        "Previous studies have examined various length distribution models to generate appropriate training sets for each formal language: BIBREF16 , BIBREF11 , BIBREF12 , for instance, used length distributions that were skewed towards having more short sequences than long sequences given a training length-window, whereas BIBREF9 used a uniform distribution scheme to generate their training sets. The latter briefly comment that the distribution of lengths of sequences in the training set does influence the generalization ability and convergence speed of neural networks, and mention that training sets containing abundant numbers of both short and long sequences are learned by networks much more quickly than uniformly distributed regimes. Nevertheless, they do not systematically compare or explicitly report their findings. To study the effect of various length distributions on the learning capability and speed of LSTM models, we experimented with four discrete probability distributions supported on bounded intervals (Figure 2 ) to sample the lengths of sequences for the languages. We briefly recall the probability distribution functions for discrete uniform and Beta-Binomial distributions used in our data generation procedure."
      ],
      "highlighted_evidence": [
        "Furthermore, we do not restrict ourselves to a test set of sequences of fixed lengths during testing. Rather, we exhaustively enumerate all the sequences in a language by their lengths and then go through the sequences in the test set one by one until our network errs $k$ times, thereby providing a more fine-grained evaluation criterion of its generalization capabilities.",
        "To study the effect of various length distributions on the learning capability and speed of LSTM models, we experimented with four discrete probability distributions supported on bounded intervals (Figure 2 ) to sample the lengths of sequences for the languages. "
      ]
    }
  },
  {
    "paper_id": "2002.02427",
    "question": "Do the authors identify any cultural differences in irony use?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "From a semantic perspective, despite the language and cultural differences between Arabic and French languages, CNN results show a high performance comparing to the other languages pairs when we train on each of these two languages and test on the other one. Similarly, for the French and English pair, but when we train on French they are quite lower. We have a similar case when we train on Arabic and test on English. We can justify that by, the language presentation of the Arabic and French tweets are quite informal and have many dialect words that may not exist in the pretrained embeddings we used comparing to the English ones (lower embeddings coverage ratio), which become harder for the CNN to learn a clear semantic pattern. Another point is the presence of Arabic dialects, where some dialect words may not exist in the multilingual pretrained embedding model that we used. On the other hand, from the text-based perspective, the results show that the text-based features can help in the case when the semantic aspect shows weak detection; this is the case for the $Ar\\longrightarrow En$ configuration. It is worthy to mention that the highest result we get in this experiment is from the En$\\rightarrow $Fr pair, as both languages use Latin characters. Finally, when investigating the relatedness between European vs. non European languages (cf. (En/Fr)$\\rightarrow $Ar), we obtain similar results than those obtained in the monolingual experiment (macro F-score 62.4 vs. 68.0) and best results are achieved by Ar $\\rightarrow $(En/Fr). This shows that there are pragmatic devices in common between both sides and, in a similar way, similar text-based patterns in the narrative way of the ironic tweets."
      ],
      "highlighted_evidence": [
        " Finally, when investigating the relatedness between European vs. non European languages (cf. (En/Fr)$\\rightarrow $Ar), we obtain similar results than those obtained in the monolingual experiment (macro F-score 62.4 vs. 68.0) and best results are achieved by Ar $\\rightarrow $(En/Fr). This shows that there are pragmatic devices in common between both sides and, in a similar way, similar text-based patterns in the narrative way of the ironic tweets."
      ]
    }
  },
  {
    "paper_id": "1710.08396",
    "question": "Does the proposed method outperform a baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1807.09671",
    "question": "Do they evaluate their framework on content of low lexical variety?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1610.08597",
    "question": "Do the authors report on English datasets only?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work BIBREF9 . It was developed by querying the Followerwonk Web service API with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every YouTube video shared by them. Table 1 provides statistics about the number of words found in each type of feature in the dataset. It includes a total of 821,412 tweets from gang members and 7,238,758 tweets from non-gang members."
      ],
      "highlighted_evidence": [
        "It was developed by querying the Followerwonk Web service API with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles."
      ]
    }
  },
  {
    "paper_id": "1809.08386",
    "question": "Do they evaluate ablated versions of their CNN+RNN model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1611.00514",
    "question": "Do they single out a validation set from the fixed SRE training set?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Since the introduction of i-vectors in BIBREF0 , the speaker recognition community has seen a significant increase in recognition performance. i-Vectors are low-dimensional representations of Baum-Welch statistics obtained with respect to a GMM, referred to as universal background model (UBM), in a single subspace which includes all characteristics of speaker and inter-session variability, named total variability matrix BIBREF0 . We trained on each acoustic feature a full covariance, gender-independent UBM model with 2048 Gaussians followed by a 600-dimensional i-vector extractor to establish our MFCC- and PLP-based i-vector systems. The unlabeled set of development data was used in the training of both the UBM and the i-vector extractor. The open-source Kaldi software has been used for all these processing steps BIBREF12 ."
      ],
      "highlighted_evidence": [
        " The unlabeled set of development data was used in the training of both the UBM and the i-vector extractor."
      ]
    }
  },
  {
    "paper_id": "1709.10445",
    "question": "Does this approach perform better than context-based word embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To verify the word embeddings learned by our model we use the task of synonym discovery, whereby we analyze if it is possible to identify a pair of words as synonyms only through their embedding vectors. Synonym discovery is a common task in research; and it has been used before to test word embedding schemes BIBREF0 . We compare the performance of our Chinese word embedding vectors in the task of synonym discovery against another set of embedding vectors that was constructed with a co-occurrence model BIBREF1 . We also investigate the performance of synonym discovery with the Sino-Korean word embeddings by our method. Our test results shows that our approach out-performs the previous model.",
        "Our embeddings also proved to perform better than our benchmark dataset. Figure shows the distribution of the similarity measure between pairs of synonyms and random pairs of words in the benchmark dataset. In this sample, almost 32% of synonyms show a similarity score that places them away from zero, while 5% of random pairs of words are placed outside of that range. Table compares performance, and dimensionality in both strategies to learn embeddings."
      ],
      "highlighted_evidence": [
        "We compare the performance of our Chinese word embedding vectors in the task of synonym discovery against another set of embedding vectors that was constructed with a co-occurrence model BIBREF1 .",
        "Our embeddings also proved to perform better than our benchmark dataset."
      ]
    }
  },
  {
    "paper_id": "1709.10445",
    "question": "Have the authors tried this approach on other languages?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We believe that our model can help expand our understanding of word embedding; and also help reevaluate the value of etymology in data mining and machine learning. We are excited to see etymological graphs used in other ways to extract knowledge. We also are especially interested in seeing this model applied to different languages."
      ],
      "highlighted_evidence": [
        "We also are especially interested in seeing this model applied to different languages."
      ]
    }
  },
  {
    "paper_id": "1911.11933",
    "question": "Do they trim the search space of possible output sequences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.11933",
    "question": "Do they compare simultaneous translation performance to regular machine translation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1611.09441",
    "question": "Do the authors report only on English language data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We segment a hashtag into meaningful English phrases. The `#' character is removed from the tweet text. As for example, #killthebill is transformed into kill the bill.",
        "In order to achieve this, we use a dictionary of English words. We recursively break the hashtagged phrase into segments and match the segments in the dictionary until we get a complete set of meaningful words. This is important since many users tend to post tweets where the actual message of the tweet is expressed in form of terse hashtagged phrases.",
        "The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . It predicts sentiment for each sentence within the article. We calculate the fraction of sentences that are negative, positive, and neutral and use these three values as features."
      ],
      "highlighted_evidence": [
        "We segment a hashtag into meaningful English phrases.",
        "In order to achieve this, we use a dictionary of English words.",
        "The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . "
      ]
    }
  },
  {
    "paper_id": "2004.04315",
    "question": "Is the dataset focused on a region?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 1: The location of geotagged tweets"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 1: The location of geotagged tweets"
      ]
    }
  },
  {
    "paper_id": "2004.04315",
    "question": "Are the tweets location-specific?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 1: The location of geotagged tweets"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 1: The location of geotagged tweets"
      ]
    }
  },
  {
    "paper_id": "1901.00570",
    "question": "Do the authors suggest any future extensions to this work?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The main contributions of this paper are (1) to overcome twitter challenges of acronyms, short text, ambiguity and synonyms, (2) to identify the set of word-pairs to be used as features for live event detection, (3) to build an end-to-end framework that can detect the events lively according to the word counts. This work can be applied to similar problems, where specific tweets can be associated with life events such as disease outbreak or stock market fluctuation. This work can be extended to predict future events with one day in advance, where we will use the same method for feature selection in addition to to time series analysis of the historical patterns of the word-pairs."
      ],
      "highlighted_evidence": [
        ". This work can be extended to predict future events with one day in advance, where we will use the same method for feature selection in addition to to time series analysis of the historical patterns of the word-pairs."
      ]
    }
  },
  {
    "paper_id": "1901.00570",
    "question": "Were any other word similar metrics, besides Jaccard metric, tested?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: TABLE II: A comparison of classification AVCs using word-pairs extracted by different feature selection methods"
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: TABLE II: A comparison of classification AVCs using word-pairs extracted by different feature selection methods"
      ]
    }
  },
  {
    "paper_id": "2002.05058",
    "question": "Do the authors suggest that proposed metric replace human evaluation on this task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1906.06045",
    "question": "Does their approach require a dataset of unanswerable questions mapped to similar answerable questions?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for another paragraph, and using it as an unanswerable question. However, it would be trivial to determine whether the retrieved question is answerable by using word-overlap heuristics, because the question is irrelevant to the context BIBREF6 . In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model."
      ],
      "highlighted_evidence": [
        "In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model."
      ]
    }
  },
  {
    "paper_id": "2004.03329",
    "question": "Did they experiment on this dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1602.00812",
    "question": "Does Grail accept Prolog inputs?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In its general form, a type-logical grammar consists of following components:"
      ],
      "highlighted_evidence": [
        "In its general form, a type-logical grammar consists of following components:"
      ]
    }
  },
  {
    "paper_id": "1704.02686",
    "question": "Do they test their word embeddings on downstream tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Outlier Detection. The Outlier Detection task BIBREF0 is to determine which word in a list INLINEFORM0 of INLINEFORM1 words is unrelated to the other INLINEFORM2 which were chosen to be related. For each INLINEFORM3 , one can compute its compactness score INLINEFORM4 , which is the compactness of INLINEFORM5 . INLINEFORM6 is explicitly computed as the mean similarity of all word pairs INLINEFORM7 . The predicted outlier is INLINEFORM8 , as the INLINEFORM9 related words should form a compact cluster with high mean similarity.",
        "Sentiment analysis. We also consider sentiment analysis as described by BIBREF31 . We use the suggested Large Movie Review dataset BIBREF32 , containing 50,000 movie reviews."
      ],
      "highlighted_evidence": [
        "Outlier Detection. The Outlier Detection task BIBREF0 is to determine which word in a list INLINEFORM0 of INLINEFORM1 words is unrelated to the other INLINEFORM2 which were chosen to be related. For each INLINEFORM3 , one can compute its compactness score INLINEFORM4 , which is the compactness of INLINEFORM5 .",
        "Sentiment analysis. We also consider sentiment analysis as described by BIBREF31 ."
      ]
    }
  },
  {
    "paper_id": "1704.02686",
    "question": "Do they measure computation time of their factorizations compared to other word embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "When considering going from two dimensions to three, it is perhaps necessary to discuss the computational issues in such a problem size increase. However, it should be noted that the creation of pre-trained embeddings can be seen as a pre-processing step for many future NLP tasks, so if the training can be completed once, it can be used forever thereafter without having to take training time into account. Despite this, we found that the training of our embeddings was not considerably slower than the training of order-2 equivalents such as SGNS. Explicitly, our GPU trained CBOW vectors (using the experimental settings found below) in 3568 seconds, whereas training CP-S and JCP-S took 6786 and 8686 seconds respectively."
      ],
      "highlighted_evidence": [
        "Despite this, we found that the training of our embeddings was not considerably slower than the training of order-2 equivalents such as SGNS. Explicitly, our GPU trained CBOW vectors (using the experimental settings found below) in 3568 seconds, whereas training CP-S and JCP-S took 6786 and 8686 seconds respectively."
      ]
    }
  },
  {
    "paper_id": "1705.08142",
    "question": "Do sluice networks outperform non-transfer learning approaches?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 3: Accuracy scores on in-domain and out-of-domain test sets for chunking (main task) with POS tagging as auxiliary task for different target domains for baselines and Sluice networks. Out-of-domain results for each target domain are averages across the 6 remaining source domains. Average error reduction over single-task performance is 12.8% for in-domain; 8.9% for out-of-domain. In-domain error reduction over hard parameter sharing is 14.8%."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 3: Accuracy scores on in-domain and out-of-domain test sets for chunking (main task) with POS tagging as auxiliary task for different target domains for baselines and Sluice networks. Out-of-domain results for each target domain are averages across the 6 remaining source domains. Average error reduction over single-task performance is 12.8% for in-domain; 8.9% for out-of-domain. In-domain error reduction over hard parameter sharing is 14.8%."
      ]
    }
  },
  {
    "paper_id": "1709.09119",
    "question": "Do they translate metadata from Japanese papers to English?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.03270",
    "question": "Does the paper report the performance on the task of a Neural Machine Translation model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.03270",
    "question": "Is the RNN model evaluated against any baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. The probability of a sequence of words is the product of the probabilities of each word, given the word’s context: the preceding word. As in the following equation:"
      ],
      "highlighted_evidence": [
        "For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8."
      ]
    }
  },
  {
    "paper_id": "1707.04913",
    "question": "Do they assume sentence-level supervision?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations.",
        "Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract. If there are multiple outputs for a single field, e.g. multiple destination cities, we join them with a comma. For the ATIS data set, we choose the 10 most common labels, and we use all the labels for the movie and restaurant corpus. The movie data set has 12 fields and the restaurant has 8. See Table 2 for an example of the E2E ATIS data set."
      ],
      "highlighted_evidence": [
        "To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations.",
        "Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract."
      ]
    }
  },
  {
    "paper_id": "1908.05763",
    "question": "Are recurrent neural networks trained on perturbed data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "For evaluation, we used the widely popular dialog act and intent prediction datasets. MRDA BIBREF12 is a dialog corpus of multi-party meetings with 6 classes, 78K training and 15K test data; ATIS BIBREF13 is intent prediction dataset for flight reservations with 21 classes, 4.4K training and 893 test examples; and SWDA BIBREF14, BIBREF15 is an open domain dialog corpus between two speakers with 42 classes, 193K training and 5K test examples. For fair comparison, we train LSTM baseline with sub-words and 240 vocabulary size on MRDA, ATIS and SWDA. We uniformly randomly initialized the input word embeddings. We also trained the on-device SGNN model BIBREF6. Then, we created test sets with varying levels of perturbation operations - $\\lbrace 20\\%,40\\%,60\\%\\rbrace $."
      ],
      "highlighted_evidence": [
        "Then, we created test sets with varying levels of perturbation operations - $\\lbrace 20\\%,40\\%,60\\%\\rbrace $."
      ]
    }
  },
  {
    "paper_id": "1801.07804",
    "question": "Do they consider relations other than binary relations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1801.07804",
    "question": "Are the grammar clauses manually created?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1801.07804",
    "question": "Do they use an NER system in their pipeline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2001.08051",
    "question": "Are any of the utterances ungrammatical?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "It is worth mentioning that the collected texts contain a large quantity of errors of several types: orthographic, syntactic, code-switched words (i.e. words not in the required language), jokes, etc. Hence, the original written sentences have been processed in order to produce “cleaner” versions, in order to make the data usable for some research purposes (e.g. to train language models, to extract features for proficiency assessment, ...)."
      ],
      "highlighted_evidence": [
        "It is worth mentioning that the collected texts contain a large quantity of errors of several types: orthographic, syntactic, code-switched words (i.e. words not in the required language), jokes, etc. Hence, the original written sentences have been processed in order to produce “cleaner” versions, in order to make the data usable for some research purposes (e.g. to train language models, to extract features for proficiency assessment, ...)."
      ]
    }
  },
  {
    "paper_id": "1804.04225",
    "question": "Do they use any knowledge base to expand abbreviations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The overview of our approach is shown in Figure FIGREF6 . Within ICU notes (e.g., text example in top-left box in Figure 2), we first identify all abbreviations using regular expressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates. We train word embeddings using the clinical notes data with task-oriented resources such as Wikipedia articles of candidates and medical scientific papers and compute the semantic similarity between an abbreviation and its candidate expansions based on their embeddings (vector representations of words)."
      ],
      "highlighted_evidence": [
        "Within ICU notes (e.g., text example in top-left box in Figure 2), we first identify all abbreviations using regular expressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates."
      ]
    }
  },
  {
    "paper_id": "1804.04225",
    "question": "In their used dataset, do they study how many abbreviations are ambiguous?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1612.07843",
    "question": "Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure. The latter will be used to analyze and compare the relevance decompositions or explanations obtained with the neural network and the BoW/SVM classifier. Both types of evaluations will be carried out in section \"Results\" ."
      ],
      "highlighted_evidence": [
        "In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure."
      ]
    }
  },
  {
    "paper_id": "1612.07843",
    "question": "Does the LRP method work in settings that contextualize the words with respect to one another?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Layer-wise relevance propagation (LRP) BIBREF12 , BIBREF32 is a recently introduced technique for estimating which elements of a classifier input are important to achieve a certain classification decision. It can be applied to bag-of-words SVM classifiers as well as to layer-wise structured neural networks. For every input data point and possible target class, LRP delivers one scalar relevance value per input variable, hereby indicating whether the corresponding part of the input is contributing for or against a specific classifier decision, or if this input variable is rather uninvolved and irrelevant to the classification task at all."
      ],
      "highlighted_evidence": [
        "For every input data point and possible target class, LRP delivers one scalar relevance value per input variable, hereby indicating whether the corresponding part of the input is contributing for or against a specific classifier decision, or if this input variable is rather uninvolved and irrelevant to the classification task at all."
      ]
    }
  },
  {
    "paper_id": "1905.10044",
    "question": "did they use other pretrained language models besides bert?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our Recurrent +ELMo model uses the language model from BIBREF9 to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors.",
        "Our OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from BIBREF27 , which has been pre-trained as a language model on the Books corpus BIBREF36 ."
      ],
      "highlighted_evidence": [
        "Our Recurrent +ELMo model uses the language model from BIBREF9 to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors.\n\nOur OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from BIBREF27 , which has been pre-trained as a language model on the Books corpus BIBREF36 ."
      ]
    }
  },
  {
    "paper_id": "1808.02113",
    "question": "Do they explain model predictions solely on attention weights?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our application is the escalation of Internet chats. To maintain quality of service, users are transferred to human representatives when their conversations with an intelligent virtual assistant (IVA) fail to progress. These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. This helps the representative quickly scan the conversation history and determine the best course of action based on problematic turns."
      ],
      "highlighted_evidence": [
        "To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision."
      ]
    }
  },
  {
    "paper_id": "1702.06589",
    "question": "Does a neural scoring function take both the question and the logical form as inputs?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Let $u,v \\in \\mathbb {R}^{d}$ be the sentence embeddings of question $q$ and of paraphrase $t$ . We experiment with the following similarity scores: i) DOTPRODUCT : $u^{T}v$ ; ii) BILIN : $u^{T}Sv$ , with $S\\in \\mathbb {R}^{d\\times d}$ being a trainable matrix; iii) FC: u and v concatenated, followed by two sequential fully connected layers with ELU non-linearities; iv) FC-BILIN: weighted average of BILIN and FC. These models define parametrized similarity scoring functions $: Q\\times T\\rightarrow \\mathbb {R}$ , where $Q$ is the set of natural language questions and $T$ is the set of paraphrases of logical forms."
      ],
      "highlighted_evidence": [
        "These models define parametrized similarity scoring functions $: Q\\times T\\rightarrow \\mathbb {R}$ , where $Q$ is the set of natural language questions and $T$ is the set of paraphrases of logical forms."
      ]
    }
  },
  {
    "paper_id": "1702.06589",
    "question": "Does the dataset they use differ from the one used by Pasupat and Liang, 2015?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Dataset: For training and testing we use the train-validation-test split of WikiTableQuestions BIBREF0 , a dataset containing 22,033 pairs of questions and answers based on 2,108 Wikipedia tables. This dataset is also used by our baselines, BIBREF0 , BIBREF3 . Tables are not shared across these splits, which requires models to generalize to unseen data. We obtain about 3.8 million training triples $(q,t,l)$ , where $l$ is a binary indicator of whether the logical form gives the correct gold answer when executed on the corresponding table. 76.7% of the questions have at least one correct candidate logical form when generated with the model of BIBREF0 ."
      ],
      "highlighted_evidence": [
        "For training and testing we use the train-validation-test split of WikiTableQuestions BIBREF0 , a dataset containing 22,033 pairs of questions and answers based on 2,108 Wikipedia tables. This dataset is also used by our baselines, BIBREF0 , BIBREF3 ."
      ]
    }
  },
  {
    "paper_id": "1710.11154",
    "question": "Did they experiment on this corpus?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1912.10806",
    "question": "Is the model compared against a linear regression baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Figure FIGREF29 shows the $\\text{MPAs}$ of the proposed DP-LSTM and vanilla LSTM for comparison. In Table TABREF30, we give the mean MPA results for the prediction prices, which shows the accuracy performance of DP-LSTM is 0.32% higer than the LSTM with news. The result means the DP framework can make the prediction result more accuracy and robustness."
      ],
      "highlighted_evidence": [
        "Figure FIGREF29 shows the $\\text{MPAs}$ of the proposed DP-LSTM and vanilla LSTM for comparison. "
      ]
    }
  },
  {
    "paper_id": "1911.03912",
    "question": "Is their model fine-tuned also on all available data, what are results?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1607.00410",
    "question": "Did they only experiment with captioning task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In the latter part of this paper, we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics. In the datasets, the source and target have different word distributions, and thus adaptation of output parameters is important. We augment the output parameters to facilitate adaptation. Although we use captioning models in the experiments, our method can be applied to any neural networks trained with a cross-entropy loss."
      ],
      "highlighted_evidence": [
        "we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics. "
      ]
    }
  },
  {
    "paper_id": "1910.08772",
    "question": "Do they beat current state-of-the-art on SICK?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery). We also introduce a supplementary version of SICK that corrects several common annotation mistakes (e.g., asymmetrical inference annotations) based on previous work by kalouli2017entail,kalouli2018. Positive results on both these datasets show the ability of lightweight monotonicity models to handle many of the inferences found in current NLI datasets, hence putting a more reliable lower-bound on what results the simplest logical approach is capable of achieving on this benchmark."
      ],
      "highlighted_evidence": [
        "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery)."
      ]
    }
  },
  {
    "paper_id": "1610.05243",
    "question": "Do they train the NMT model on PBMT outputs?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In a second step, we will train a neural monolingual translation system, that translates from the output of the PBMT system INLINEFORM0 to a better target sentence INLINEFORM1 ."
      ],
      "highlighted_evidence": [
        " ",
        "In a second step, we will train a neural monolingual translation system, that translates from the output of the PBMT system INLINEFORM0 to a better target sentence INLINEFORM1 ."
      ]
    }
  },
  {
    "paper_id": "1909.11467",
    "question": "Is the corpus annotated?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.11467",
    "question": "Is the corpus annotated with a phonetic transcription?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.11467",
    "question": "Is the corpus annotated with Part-of-Speech tags?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1907.11062",
    "question": "Have the candidates given their consent to have their videos used for the research?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We have decided to focus on only one specific type of job: sales positions. After filtering based on specific job titles from the ROME Database, a list of positions was selected and verified by the authors and an expert from the Human Resources (HR). Finally, in a collaboration with an HR industry actor, we have obtained a dataset of French video interviews comprising more than 475 positions and 7938 candidates. As they watch candidates' videos, recruiters can like, dislike, shortlist candidates, evaluate them on predefined criteria, or write comments. To simplify the task, we set up a binary classification: candidates who have been liked or shortlisted are considered part of the hirable class and others part of the not hirable class. If multiple annotators have annotated the same candidates, we proceed with a majority vote. In case of a draw, the candidate is considered hirable. It is important to note that the videos are quite different from what could be produced in a laboratory setup. Videos can be recorded from a webcam, a smartphone or a tablet., meaning noisy environments and low quality equipment are par for the course. Due to these real conditions, feature extraction may fail for a single modality during a candidate's entire answer. One example is the detection of action units when the image has lighting problems. We decided to use all samples available in each modality separately. Some statistics about the dataset are available in Table TABREF33 . Although the candidates agreed to the use of their interviews, the dataset will not be released to public outside of the scope of this study due to the videos being personal data subject to high privacy constraints."
      ],
      "highlighted_evidence": [
        "Although the candidates agreed to the use of their interviews, the dataset will not be released to public outside of the scope of this study due to the videos being personal data subject to high privacy constraints."
      ]
    }
  },
  {
    "paper_id": "1907.11062",
    "question": "Do they analyze if their system has any bias?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."
      ],
      "highlighted_evidence": [
        "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."
      ]
    }
  },
  {
    "paper_id": "1907.11062",
    "question": "Is there any ethical consideration in the research?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."
      ],
      "highlighted_evidence": [
        "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."
      ]
    }
  },
  {
    "paper_id": "1909.05438",
    "question": "Are the rules dataset specific?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%.",
        "Our rule for KBQA is simple without using a curated mapping dictionary. First, we detect an entity from the question using strict string matching, with the constraint that only one entity from the KB has the same surface string and that the question contains only one entity. After that, we get the connected relations of the detected entity, and assign the relation as the one with maximum number of co-occurred words. The coverage of our rule on training set is 16.0%, with an accuracy of 97.3% for relation prediction.",
        "The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. Table TABREF23 shows the additional word-level mapping table used in SequentialQA. The coverage of our rule on training set is 75.5%, with an accuracy of 38.5%."
      ],
      "highlighted_evidence": [
        "We describe our rules for WikiSQL here.",
        "Our rule for KBQA is simple without using a curated mapping dictionary.",
        "The pipeline of rules in SequentialQA is similar to that of WikiSQL."
      ]
    }
  },
  {
    "paper_id": "1909.00338",
    "question": "Do they allow for messages with vaccination-related key terms to be of neutral stance?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The stance towards vaccination was categorized into `Negative’, `Neutral’, `Positive’ and `Not clear’. The latter category was essential, as some posts do not convey enough information about the stance of the writer. In addition to the four-valued stance classes we included separate classes grouped under relevance, subject and sentiment as annotation categories. With these additional categorizations we aimed to obtain a precise grasp of all possibly relevant tweet characteristics in relation to vaccination, which could help in a machine learning setting."
      ],
      "highlighted_evidence": [
        "The stance towards vaccination was categorized into `Negative’, `Neutral’, `Positive’ and `Not clear’."
      ]
    }
  },
  {
    "paper_id": "1909.00361",
    "question": "Is this a span-based (extractive) QA task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29."
      ],
      "highlighted_evidence": [
        "We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29."
      ]
    }
  },
  {
    "paper_id": "1906.01840",
    "question": "Do they measure how well they perform on longer sequences specifically?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length. The performance peaked around length 20, then starts to drop, indicating a moderate attention span is more preferable. Similar results are observed on other datasets (results not shown). Experimental details on the ablation study can be found in the SM."
      ],
      "highlighted_evidence": [
        "We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length. "
      ]
    }
  },
  {
    "paper_id": "1907.00455",
    "question": "Do they compare results against state-of-the-art language models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "All models outperform previously reported results for mlstm BIBREF8 despite lower parameter counts. This is likely due to our relatively small batch size. However, they perform fairly similarly. Encouraged by these results, we built an mgru with both hidden and intermediate state sizes set to that of the original mlstm (700). This version highly surpasses the previous state of the art while still having fewer parameters than previous work."
      ],
      "highlighted_evidence": [
        "This version highly surpasses the previous state of the art while still having fewer parameters than previous work."
      ]
    }
  },
  {
    "paper_id": "1906.01910",
    "question": "did they compare with other evaluation metrics?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 5: Comparison of nex-cv and Human-Rater Accuracy. The six datasets from pseudonymous chatbots tested had a different number of questions (examples) and categories (classes), as shown in the bottom row. The human-rater estimate of accuracy (top left, blue) is consistently more lenient than any of the automated measures (top right). The (0; 0.15) setting (top right, blue) is not consistently more or less optimistic than the other settings."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 5: Comparison of nex-cv and Human-Rater Accuracy. The six datasets from pseudonymous chatbots tested had a different number of questions (examples) and categories (classes), as shown in the bottom row. The human-rater estimate of accuracy (top left, blue) is consistently more lenient than any of the automated measures (top right). The (0; 0.15) setting (top right, blue) is not consistently more or less optimistic than the other settings."
      ]
    }
  },
  {
    "paper_id": "1910.11768",
    "question": "Do they do quantitative quality analysis of learned embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The high nearest neighbours accuracy indicates that syntax information was successfully captured by the embeddings. Table TABREF22 also shows that the syntactic information of multiple languages was captured by a single embedding model."
      ],
      "highlighted_evidence": [
        "The high nearest neighbours accuracy indicates that syntax information was successfully captured by the embeddings."
      ]
    }
  },
  {
    "paper_id": "1910.11768",
    "question": "Do they evaluate on downstream tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Many NLP tasks utilize POS as features, but human annotated POS sequences are difficult and expensive to obtain. Thus, it is important to know if we can learn sentences-level syntactic embeddings for low-sources languages without treebanks.",
        "We performed zero-shot transfer of the syntactic embeddings for French, Portuguese and Indonesian. French and Portuguese are simulated low-resource languages, while Indonesian is a true low-resource language. We reported the 1-NN and 5-NN accuracies for all languages using the same evaluation setting as described in the previous section. The results are shown in Table TABREF31 (top)."
      ],
      "highlighted_evidence": [
        "Many NLP tasks utilize POS as features, but human annotated POS sequences are difficult and expensive to obtain. Thus, it is important to know if we can learn sentences-level syntactic embeddings for low-sources languages without treebanks.\n\nWe performed zero-shot transfer of the syntactic embeddings for French, Portuguese and Indonesian. French and Portuguese are simulated low-resource languages, while Indonesian is a true low-resource language."
      ]
    }
  },
  {
    "paper_id": "1602.07618",
    "question": "Do they argue that all words can be derived from other (elementary) words?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In order to understand what INLINEFORM0 is, we need to understand the mathematics of grammar. The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest..."
      ],
      "highlighted_evidence": [
        "The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest..."
      ]
    }
  },
  {
    "paper_id": "1602.07618",
    "question": "Do they break down word meanings into elementary particles as in the standard model of quantum theory?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In order to understand what INLINEFORM0 is, we need to understand the mathematics of grammar. The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...",
        "On the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory!"
      ],
      "highlighted_evidence": [
        "The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...\n\nOn the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory!"
      ]
    }
  },
  {
    "paper_id": "1911.03584",
    "question": "Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks? ",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1711.10124",
    "question": "Are their corpus and software public?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our system, including software and corpus, is available as an open source project for free research purpose and we believe that it is a good baseline for the development and comparison of future Vietnamese SRL systems. We plan to integrate this tool to Vitk, an open-source toolkit for processing Vietnamese text, which contains fundamental processing tools and are readily scalable for processing very large text data."
      ],
      "highlighted_evidence": [
        "Our system, including software and corpus, is available as an open source project for free research purpose and we believe that it is a good baseline for the development and comparison of future Vietnamese SRL systems. "
      ]
    }
  },
  {
    "paper_id": "1702.02363",
    "question": "Did they experiment with the dataset on some tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this work, we publish TWNERTC dataset in which named entities and categories of sentences have been automatically annotated. We use Turkish Wikipedia dumps as the text source and Freebase to construct a large-scale gazetteers to map fine-grained types to entities. To overcome noisy and ambiguous data, we leverage domain information which is given by Freebase and develop domain-independent and domain-dependent methodologies. All versions of datasets can be downloaded from our project web-page. Our main contributions are (1) the publication of Turkish corpus for coarse-grained and fine-grained NER, and TC research, (2) six different versions of corpus according to noise reduction methodology and entity types, (3) an analysis of the corpus and (4) benchmark comparisons for NER and TC tasks against human annotators. To the best of our knowledge, these datasets are the largest datasets available for Turkish NER ad TC tasks."
      ],
      "highlighted_evidence": [
        "Our main contributions are (1) the publication of Turkish corpus for coarse-grained and fine-grained NER, and TC research, (2) six different versions of corpus according to noise reduction methodology and entity types, (3) an analysis of the corpus and (4) benchmark comparisons for NER and TC tasks against human annotators. "
      ]
    }
  },
  {
    "paper_id": "1811.00266",
    "question": "Do they use pretrained word embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this study, we tackle a task of describing (defining) a phrase when given its local context as BIBREF2 , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) BIBREF0 , BIBREF1 . We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen."
      ],
      "highlighted_evidence": [
        "We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description."
      ]
    }
  },
  {
    "paper_id": "1606.08495",
    "question": "Do they use skipgram version of word2vec?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this paper we focus on the skipgram approach with random negative examples proposed in BIBREF0 . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors BIBREF7 , BIBREF0 . Given a corpus consisting of a sequence of sentences INLINEFORM0 each comprising a sequence of words INLINEFORM1 , the objective is to maximize the log likelihood: DISPLAYFORM0"
      ],
      "highlighted_evidence": [
        "In this paper we focus on the skipgram approach with random negative examples proposed in BIBREF0 . "
      ]
    }
  },
  {
    "paper_id": "1606.08495",
    "question": "Do they perform any morphological tokenization?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "This step entails counting occurrences of all words in the training corpus and sorting them in order of decreasing occurrence. As mentioned, the vocabulary is taken to be the INLINEFORM0 most frequently occurring words, that occur at least some number INLINEFORM1 times. It is implemented in Spark as a straight-forward map-reduce job."
      ],
      "highlighted_evidence": [
        "This step entails counting occurrences of all words in the training corpus and sorting them in order of decreasing occurrence. As mentioned, the vocabulary is taken to be the INLINEFORM0 most frequently occurring words, that occur at least some number INLINEFORM1 times. It is implemented in Spark as a straight-forward map-reduce job."
      ]
    }
  },
  {
    "paper_id": "1810.03459",
    "question": "Do they report BLEU scores?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.06061",
    "question": "Did they evaluate against baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Our contributions are as follows: We propose to cluster the input words with the help of additional, unlabeled data. Based on this partition of the feature space, we obtain different confusion matrices that describe the relationship between clean and noisy labels. We evaluate our newly proposed models and related baselines in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise. The advanced modeling of the noisy labels substantially improves the performance up to 36% over methods without noise-handling and up to 9% over all other noise-handling baselines."
      ],
      "highlighted_evidence": [
        "We evaluate our newly proposed models and related baselines in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise. The advanced modeling of the noisy labels substantially improves the performance up to 36% over methods without noise-handling and up to 9% over all other noise-handling baselines."
      ]
    }
  },
  {
    "paper_id": "1810.10254",
    "question": "Did they use other evaluation metrics?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this section, we present the experimental settings for pointer-generator network and language model. Our experiment, our pointer-generator model has 500-dimensional hidden states and word embeddings. We use 50k words as our vocabulary for source and target. We evaluate our pointer-generator performance using BLEU score. We take the best model as our generator and during the decoding stage, we generate 1-best and 3-best using beam search with a beam size of 5. For the input, we build a parallel monolingual corpus by translating the mixed language sequence using Google NMT to English ( INLINEFORM0 ) and Mandarin ( INLINEFORM1 ) sequences. Then, we concatenate the translated English and Mandarin sequences and assign code-switching sequences as the labels ( INLINEFORM2 ).",
        "The baseline language model is trained using RNNLM BIBREF23 . Then, we train our 2-layer LSTM models with a hidden size of 500 and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size for weight tying. We optimize our model using SGD with initial learning rates of INLINEFORM1 . If there is no improvement during the evaluation, we reduce the learning rate by a factor of 0.75. In each time step, we apply dropout to both embedding layer and recurrent network. The gradient is clipped to a maximum of 0.25. Perplexity measure is used in the evaluation."
      ],
      "highlighted_evidence": [
        "We evaluate our pointer-generator performance using BLEU score.",
        "The baseline language model is trained using RNNLM BIBREF23 .",
        "Perplexity measure is used in the evaluation.\n\n"
      ]
    }
  },
  {
    "paper_id": "1808.03815",
    "question": "Are there syntax-agnostic SRL models before?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks BIBREF5 , BIBREF6 . Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints BIBREF7 , BIBREF8 , which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications BIBREF9 , BIBREF10 . However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, marcheggiani2017 presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task BIBREF11 ."
      ],
      "highlighted_evidence": [
        "Accordingly, marcheggiani2017 presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task BIBREF11 ."
      ]
    }
  },
  {
    "paper_id": "1702.06777",
    "question": "Do the authors mention any possible confounds in their study?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. Importantly, we emphasize that both distance measures (cosine similarity and Jensen-Shanon) give rise to the same result, with little discrepancies on the numerical values that are not significant. The presence of two Twitter superdialects (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes."
      ],
      "highlighted_evidence": [
        "Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas.",
        " In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc."
      ]
    }
  },
  {
    "paper_id": "1907.04152",
    "question": "Do they fine-tune the used word embeddings on their medical texts?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Three most common classic non-contextual approaches to obtain word embeddings are skip-gram, Continuous Bag of Words (two algorithms from BIBREF19 ) and GloVe (Global Vectors, BIBREF20 , where higher accuracy than in previous algorithms was proved). Some authors use pretrained embeddings (especially when their data set is too small to train their own embeddings) or try to modify these embeddings and adjust to their set. But the biggest drawback of these approaches is that the corpus for training embeddings can be not related to the specific task where embeddings are utilized. A lot of medical concepts are not contained in well-known embeddings bases. Furthermore, the similarity of words may vary in different contexts.",
        "Then, we compute embeddings of concepts (by GloVe) for interview descriptions and for examination descriptions separately. We compute two separate embeddings, because we want to catch the similarity between terms in their specific context, i.e. words similar in the interview may not be similar in the examination description (for example we computed that the nearest words to cough in interview descriptions was runny nose, sore throat, fever, dry cough but in examination description it was rash, sunny, laryngeal, dry cough)."
      ],
      "highlighted_evidence": [
        "Some authors use pretrained embeddings (especially when their data set is too small to train their own embeddings) or try to modify these embeddings and adjust to their set. But the biggest drawback of these approaches is that the corpus for training embeddings can be not related to the specific task where embeddings are utilized. A lot of medical concepts are not contained in well-known embeddings bases. Furthermore, the similarity of words may vary in different contexts.",
        "Then, we compute embeddings of concepts (by GloVe) for interview descriptions and for examination descriptions separately."
      ]
    }
  },
  {
    "paper_id": "1907.04152",
    "question": "Do they explore similarity of texts across different doctors?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Clustering was performed separately for each specialty of doctors. Figure FIGREF11 illustrates two-dimensional projections of visit embeddings coloured by clusters. The projections were created by t-SNE algorithm BIBREF25 . For some domains clusters are very clear and separated (Figure FIGREF11 ). This corresponds with the high stability of clustering measured by Rand index.",
        "We also examined the distribution of doctors' IDs in the obtained clusters. It turned out that some clusters covered almost exactly descriptions written by one doctor. This situation took place in the specialties where clusters are separated with large margins (e.g. psychiatry, pediatrics, cardiology). Figure FIGREF13 (a) shows correspondence analysis between doctors' IDs and clusters for psychiatry clustering."
      ],
      "highlighted_evidence": [
        "Clustering was performed separately for each specialty of doctors.",
        "We also examined the distribution of doctors' IDs in the obtained clusters. It turned out that some clusters covered almost exactly descriptions written by one doctor. This situation took place in the specialties where clusters are separated with large margins (e.g. psychiatry, pediatrics, cardiology)."
      ]
    }
  },
  {
    "paper_id": "1909.01958",
    "question": "Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The current configuration of Aristo comprises of eight solvers, described shortly, each of which attempts to answer a multiple choice question. To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community.",
        "The solvers can be loosely grouped into:",
        "Statistical and information retrieval methods",
        "Reasoning methods",
        "Large-scale language model methods",
        "Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods.",
        "The field of NLP has advanced substantially with the advent of large-scale language models such as ELMo (BID6), ULMFit (BID37), GPT (BID38), BERT (BID7), and RoBERTa (BID8). These models are trained to perform various language prediction tasks such as predicting a missing word or the next sentence, using large amounts of text (e.g., BERT was trained on Wikipedia + the Google Book Corpus of 10,000 books). They can also be fine-tuned to new language prediction tasks, such as question-answering, and have been remarkably successful in the few months that they have been available.",
        "We apply BERT to multiple choice questions by treating the task as classification: Given a question $q$ with answer options $a_{i}$ and optional background knowledge $K_{i}$, we provide it to BERT as:",
        "[CLS] $K_i$ [SEP] $q$ [SEP] $a_{i}$ [SEP]",
        "The AristoBERT solver uses three methods to apply BERT more effectively. First, we retrieve and supply background knowledge along with the question when using BERT. This provides the potential for BERT to “read” that background knowledge and apply it to the question, although the exact nature of how it uses background knowledge is more complex and less interpretable. Second, we fine-tune BERT using a curriculum of several datasets, including some that are not science related. Finally, we ensemble different variants of BERT together."
      ],
      "highlighted_evidence": [
        "The current configuration of Aristo comprises of eight solvers, described shortly, each of which attempts to answer a multiple choice question. To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community.\n\nThe solvers can be loosely grouped into:\n\nStatistical and information retrieval methods\n\nReasoning methods\n\nLarge-scale language model methods",
        "Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods.",
        "The field of NLP has advanced substantially with the advent of large-scale language models such as ELMo (BID6), ULMFit (BID37), GPT (BID38), BERT (BID7), and RoBERTa (BID8). These models are trained to perform various language prediction tasks such as predicting a missing word or the next sentence, using large amounts of text (e.g., BERT was trained on Wikipedia + the Google Book Corpus of 10,000 books). They can also be fine-tuned to new language prediction tasks, such as question-answering, and have been remarkably successful in the few months that they have been available.",
        "We apply BERT to multiple choice questions by treating the task as classification: Given a question $q$ with answer options $a_{i}$ and optional background knowledge $K_{i}$, we provide it to BERT as:\n\n[CLS] $K_i$ [SEP] $q$ [SEP] $a_{i}$ [SEP]",
        "The AristoBERT solver uses three methods to apply BERT more effectively. First, we retrieve and supply background knowledge along with the question when using BERT. This provides the potential for BERT to “read” that background knowledge and apply it to the question, although the exact nature of how it uses background knowledge is more complex and less interpretable. Second, we fine-tune BERT using a curriculum of several datasets, including some that are not science related. Finally, we ensemble different variants of BERT together."
      ]
    }
  },
  {
    "paper_id": "1909.11879",
    "question": "Does the dataset contain non-English reviews?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For this aspect and opinion term extraction task, we use tokenized and annotated hotel reviews on Airy Rooms provided by BIBREF1. The dataset consists of 5000 reviews in bahasa Indonesia. The dataset is divided into training and test sets of 4000 and 1000 reviews respectively. The label distribution of the tokens in BIO scheme can be seen in Table TABREF3. In addition, we also see this case as on entity level, i.e. ASPECT, SENTIMENT, and OTHER labels."
      ],
      "highlighted_evidence": [
        "The dataset consists of 5000 reviews in bahasa Indonesia."
      ]
    }
  },
  {
    "paper_id": "1909.11879",
    "question": "Does the paper report the performance of the method when is trained for more than 8 epochs?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1701.04653",
    "question": "On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "As the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics."
      ],
      "highlighted_evidence": [
        "terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . "
      ]
    }
  },
  {
    "paper_id": "1909.00153",
    "question": "Do any of the evaluations show that adversarial learning improves performance in at least two different language families?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In Figure FIGREF15 , we plot the zero-resource German and Japanese test set accuracy as a function of the number of steps taken, with and without adversarial training. The plot shows that the variation in the test accuracy is reduced with adversarial training, which suggests that the cross-lingual performance is more consistent when adversarial training is applied. (We note that the batch size and learning rates are the same for all the languages in MLDoc, so the variation seen in Figure FIGREF15 are not affected by those factors.)"
      ],
      "highlighted_evidence": [
        "In Figure FIGREF15 , we plot the zero-resource German and Japanese test set accuracy as a function of the number of steps taken, with and without adversarial training. The plot shows that the variation in the test accuracy is reduced with adversarial training, which suggests that the cross-lingual performance is more consistent when adversarial training is applied."
      ]
    }
  },
  {
    "paper_id": "1909.03023",
    "question": "do they use a crowdsourcing platform?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We carried out a reliability study for the proposed scheme using two pairs of expert annotators, P1 and P2. The annotators were trained by coding one transcript at a time and discussing disagreements. Five text-based discussions were used for testing reliability after training: pair P1 annotated discussions of The Bluest Eye, Death of a Salesman, and Macbeth, while pair P2 annotated two separate discussions of Ain't I a Woman. 250 argument moves (discussed by over 40 students and consisting of over 8200 words) were annotated. Inter-rater reliability was assessed using Cohen's kappa: unweighted for argumentation and knowledge domain, but quadratic-weighted for specificity given its ordered labels."
      ],
      "highlighted_evidence": [
        "We carried out a reliability study for the proposed scheme using two pairs of expert annotators, P1 and P2. "
      ]
    }
  },
  {
    "paper_id": "1909.11297",
    "question": "Is the model evaluated against the baseline also on single-aspect sentences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Secondly, we compare the performance of three BERT-related methods. The performance of BERT-Original and BERT-Soft are similar by comparing their average scores. The reason may be that the original BERT has already modeled the deep relationships between the sentence and the aspect. BERT-Original can be thought of as a kind of soft-selection approach as BERT-Soft. We also observe that the snippet selection by reinforcement learning improves the performance over soft-selection approaches in almost all settings. However, the improvement of BERT-Hard over BERT-Soft is marginal. The average score of BERT-Hard is better than BERT-Soft by 0.68%. The improvement percentages are between 0.36% and 1.49%, while on the Laptop dataset, the performance of BERT-Hard is slightly weaker than BERT-Soft. The main reason is that the datasets only contain a small portion of multi-aspect sentences with different polarities. The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities."
      ],
      "highlighted_evidence": [
        "The main reason is that the datasets only contain a small portion of multi-aspect sentences with different polarities. The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities."
      ]
    }
  },
  {
    "paper_id": "1909.11297",
    "question": "Is the accuracy of the opinion snippet detection subtask reported?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1810.07091",
    "question": "Do they differentiate insights where they are dealing with learned or engineered representations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens). Bag of INLINEFORM0 -grams features are extracted on the word and POS level. We use frequency cut-offs of INLINEFORM1 for INLINEFORM2 -grams from 1 to 5 respectively for the smaller datasets and ten times higher for the Yahoo! and Amazon datasets. For POS INLINEFORM3 -grams we use cut-offs 10 for unigrams and 20 for bigrams and higher. For the Yahoo! and Amazon datasets we use cut-offs of INLINEFORM4 . The INLINEFORM5 -grams features are then also extracted using the hashing trick with the same cut-offs to reduce the final feature vector size when combined with other features. scikit-learn's BIBREF14 FeatureHasher is used with output vectors sizes of INLINEFORM6 INLINEFORM7 INLINEFORM8 for ngrams from INLINEFORM9 respectively and INLINEFORM10 INLINEFORM11 INLINEFORM12 are used for POS ngrams. We extract lexical and POS level Language model features based on external language models, namely sentence log probabilities, perplexities, and surprisal in units of bits. Building the language model and extracting the features is done by providing the path to the compiled binaries for kenlm BIBREF15 . Finally we extract N-gram Frequency Quantile Distribution features with the same cut-offs as in the bag of ngrams features, with 4 quantiles and an OOV quantile. NLTK BIBREF16 is used for tokenization and POS tagging.",
        "We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag."
      ],
      "highlighted_evidence": [
        "We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens).",
        "We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag."
      ]
    }
  },
  {
    "paper_id": "1810.07091",
    "question": "Do they show an example of usage for INFODENS?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The framework can be used as a standalone toolkit without any modifications given the implemented features and classifiers. For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors. Extending the framework with new feature extractors or classifiers is as simple as a drag and drop placement of the new code files into the feature_extractor and classifer directories respectively. The framework will then detect the new extensions dynamically at runtime. In this section we explore how each use case is handled.",
        "Since a main use case for the framework is extracting engineered and learned features, it was designed such that developing a new feature extractor would require minimal effort. Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length. More complicated features and learned features are provided in the repository which can be used as a guide for developers. Documentation for adding classifiers and format writers is described in the Wiki of the repository but is left out of this paper due to the limited space."
      ],
      "highlighted_evidence": [
        "For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors.",
        "Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length."
      ]
    }
  },
  {
    "paper_id": "1801.02243",
    "question": "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Figure FIGREF6 shows the relationship between Tesla stock return and stock sentiment score. According the distribution of the sentiment score, the sentiment on Tesla is slightly skewed towards positive during the testing period. The price has been increased significantly during the testing period, which reflected the positive sentiment. The predicting power of sentiment score is more significant when the sentiment is more extreme and less so when the sentiment is neutral."
      ],
      "highlighted_evidence": [
        "According the distribution of the sentiment score, the sentiment on Tesla is slightly skewed towards positive during the testing period."
      ]
    }
  },
  {
    "paper_id": "1808.04614",
    "question": "Do they conduct a user study where they show an NL interface with and without their explanation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We have examined the effect to which our query explanations can help users improve the correctness of a baseline NL interface. Our user study compares the correctness of three scenarios:",
        "Parser correctness - our baseline is the percentage of examples where the top query returned by the semantic parser was correct.",
        "User correctness - the percentage of examples where the user selected a correct query from the top-7 generated by the parser.",
        "Hybrid correctness - correctness of queries returned by a combination of the previous two scenarios. The system returns the query marked by the user as correct; if the user marks all queries as incorrect it will return the parser's top candidate."
      ],
      "highlighted_evidence": [
        "Our user study compares the correctness of three scenarios:\n\nParser correctness - our baseline is the percentage of examples where the top query returned by the semantic parser was correct.\n\nUser correctness - the percentage of examples where the user selected a correct query from the top-7 generated by the parser.\n\nHybrid correctness - correctness of queries returned by a combination of the previous two scenarios. The system returns the query marked by the user as correct; if the user marks all queries as incorrect it will return the parser's top candidate."
      ]
    }
  },
  {
    "paper_id": "1805.04033",
    "question": "Are results reported only for English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model. We also analyze the output text and the output label distribution of the models, showing the power of the proposed approach. Finally, we show the cases where the correspondences learned by the proposed approach are still problematic, which can be explained based on the approach we adopt.",
        "Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo. The whole dataset is split into three parts, with 2,400,591 pairs in PART I for training, 10,666 pairs in PART II for validation, and 1,106 pairs in PART III for testing. The authors of the dataset have manually annotated the relevance scores, ranging from 1 to 5, of the text-summary pairs in PART II and PART III. They suggested that only pairs with scores no less than three should be used for evaluation, which leaves 8,685 pairs in PART II, and 725 pairs in PART III. From the statistics of the PART II and PART III, we can see that more than 20% of the pairs are dropped to maintain semantic quality. It indicates that the training set, which has not been manually annotated and checked, contains a huge quantity of unrelated text-summary pairs."
      ],
      "highlighted_evidence": [
        "We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model.",
        "Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo. "
      ]
    }
  },
  {
    "paper_id": "1910.08418",
    "question": "Does the paper report any alignment-only baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Recall that our goal is to discover words in an unsegmented stream of target characters (or phonemes) in the under-resourced language. In this section, we first describe a baseline method inspired by the “align to segment” of BIBREF12, BIBREF13. We then propose two extensions providing the model with a signal relevant to the segmentation process, so as to move towards a joint learning of segmentation and alignment."
      ],
      "highlighted_evidence": [
        "In this section, we first describe a baseline method inspired by the “align to segment” of BIBREF12, BIBREF13. We then propose two extensions providing the model with a signal relevant to the segmentation process, so as to move towards a joint learning of segmentation and alignment."
      ]
    }
  },
  {
    "paper_id": "2001.06785",
    "question": "Is the model evaluated against a baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1602.01208",
    "question": "Does their model start with any prior knowledge of words?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The objectives of this study were to build a robot that learns words related to places and efficiently utilizes this learned vocabulary in self-localization. Lexical acquisition related to places is expected to enable a robot to improve its spatial cognition. A schematic representation depicting the target task of this study is shown in Fig. FIGREF3 . This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes. The robot then performs self-localization while moving around in the environment, as shown in Fig. FIGREF3 (a). An utterer speaks a sentence including the name of the place to the robot, as shown in Fig. FIGREF3 (b). For the purposes of this study, we need to consider the problems of self-localization and lexical acquisition simultaneously."
      ],
      "highlighted_evidence": [
        "This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes."
      ]
    }
  },
  {
    "paper_id": "1905.12260",
    "question": "Could you learn such embedding simply from the image annotations and without using visual information?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Another approach for generating query and image representations is treating images as a black box. Without using pixel data, how well can we do? Given the statistics of our dataset (3B query, image pairs with 220M unique queries and 900M unique images), we know that different queries co-occur with the same images. Intuitively, if a query $q_1$ co-occurs with many of the same images as query $q_2$ , then $q_1$ and $q_2$ are likely to be semantically similar, regardless of the visual content of the shared images. Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries. This method serves as a baseline to our initial approach leveraging image understanding."
      ],
      "highlighted_evidence": [
        "Another approach for generating query and image representations is treating images as a black box.",
        "Given the statistics of our dataset (3B query, image pairs with 220M unique queries and 900M unique images), we know that different queries co-occur with the same images. Intuitively, if a query $q_1$ co-occurs with many of the same images as query $q_2$ , then $q_1$ and $q_2$ are likely to be semantically similar, regardless of the visual content of the shared images. Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries."
      ]
    }
  },
  {
    "paper_id": "1811.11136",
    "question": "Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. Summary description of other datasets for validation are also as below:",
        "We have provided baseline results for the accuracy of other models against datasets (as shown in Table 1 ) For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. We also test our model on various models and calculate metrics such as accuracy, precision and recall and show the results are in Table 2 . Table 3 , Table 4 , Table 5 , Table 6 and Table 7 . Table 8 are more detail information with precisions and recall of our models against other datasets."
      ],
      "highlighted_evidence": [
        "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. ",
        "For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. "
      ]
    }
  },
  {
    "paper_id": "2001.08845",
    "question": "Is is known whether Sina Weibo posts are censored by humans or some automatic classifier?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In cooperation with the ruling regime, Weibo sets strict control over the content published under its service BIBREF0. According to Zhu et al. zhu-etal:2013, Weibo uses a variety of strategies to target censorable posts, ranging from keyword list filtering to individual user monitoring. Among all posts that are eventually censored, nearly 30% of them are censored within 5–30 minutes, and nearly 90% within 24 hours BIBREF1. We hypothesize that the former are done automatically, while the latter are removed by human censors."
      ],
      "highlighted_evidence": [
        "In cooperation with the ruling regime, Weibo sets strict control over the content published under its service BIBREF0. According to Zhu et al. zhu-etal:2013, Weibo uses a variety of strategies to target censorable posts, ranging from keyword list filtering to individual user monitoring. Among all posts that are eventually censored, nearly 30% of them are censored within 5–30 minutes, and nearly 90% within 24 hours BIBREF1. We hypothesize that the former are done automatically, while the latter are removed by human censors."
      ]
    }
  },
  {
    "paper_id": "1608.02195",
    "question": "Do changes in policies of the political actors account for all of the mistakes the model made?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In order to investigate the errors the models made confusion matrices were extracted for the predictions on the out-of-domain evaluation data for sentence level predictions (see tab:confusion) as well as topic level predictions (see tab:confusiontopic). One example illustrates that the mistakes the model makes can be associated with changes in the party policy. The green party has been promoting policies for renewable energy and against nuclear energy in their manifestos prior to both legislative periods. Yet the statements of the green party are more often predicted to be from the government parties than from the party that originally promoted these green ideas, reflecting the trend that these legislative periods governing parties took over policies from the green party. This effect is even more pronounced in the topic level predictions: a model trained on data from the 18th Bundestag predicts all manifesto topics of the green party to be from one of the parties of the governing coalition, CDU/CSU or SPD."
      ],
      "highlighted_evidence": [
        "One example illustrates that the mistakes the model makes can be associated with changes in the party policy."
      ]
    }
  },
  {
    "paper_id": "1804.08050",
    "question": "Does each attention head in the decoder calculate the same output?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "On the other hand, in the case of MHD, instead of the integration at attention level, we assign multiple decoders for each head and then integrate their outputs to get a final output. Since each attention decoder captures different modalities, it is expected to improve the recognition performance with an ensemble effect. The calculation of the attention weight at the head INLINEFORM0 in Eq. ( EQREF21 ) is replaced with following equation: DISPLAYFORM0"
      ],
      "highlighted_evidence": [
        "On the other hand, in the case of MHD, instead of the integration at attention level, we assign multiple decoders for each head and then integrate their outputs to get a final output."
      ]
    }
  },
  {
    "paper_id": "2002.06424",
    "question": "Do they repot results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture."
      ],
      "highlighted_evidence": [
        "We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks."
      ]
    }
  },
  {
    "paper_id": "1912.02761",
    "question": "Do they propose any solution to debias the embeddings?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We have presented the first study on social bias in KG embeddings, and proposed a new metric for measuring such bias. We demonstrated that differences in the distributions of entities in real-world knowledge graphs (there are many more male bankers in Wikidata than female) translate into harmful biases related to professions being encoded in embeddings. Given that KGs are formed of real-world entities, we cannot simply equalize the counts; it is not possible to correct history by creating female US Presidents, etc. In light of this, we suggest that care is needed when applying graph embeddings in NLP pipelines, and work needed to develop robust methods to debias such embeddings."
      ],
      "highlighted_evidence": [
        "In light of this, we suggest that care is needed when applying graph embeddings in NLP pipelines, and work needed to develop robust methods to debias such embeddings."
      ]
    }
  },
  {
    "paper_id": "1909.05246",
    "question": "Is human evaluation performed?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "BLEU: We use the Bilingual Evaluation Understudy (BLEU) BIBREF34 metric which is commonly used in machine translation tasks. The BLEU metric can be used to evaluate dialogue generation models as in BIBREF5, BIBREF35. The BLEU metric is a word-overlap metric which computes the co-occurrence of N-grams in the reference and the generated response and also applies the brevity penalty which tries to penalize far too short responses which are usually not desired in task-oriented chatbots. We compute the BLEU score using all generated responses of our systems.",
        "Per-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.",
        "Per-Dialogue Accuracy: We calculate per-dialogue accuracy as used in BIBREF8, BIBREF5. For this metric, we consider all the system generated responses and compare them to the target responses. A dialogue is considered to be true if all the turns in the system generated responses match the corresponding turns in the target responses. Note that this is a very strict metric in which all the utterances in the dialogue should be the same as the target and in the right order.",
        "F1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses."
      ],
      "highlighted_evidence": [
        "BLEU: We use the Bilingual Evaluation Understudy (BLEU) BIBREF34 metric which is commonly used in machine translation tasks. The BLEU metric can be used to evaluate dialogue generation models as in BIBREF5, BIBREF35. The BLEU metric is a word-overlap metric which computes the co-occurrence of N-grams in the reference and the generated response and also applies the brevity penalty which tries to penalize far too short responses which are usually not desired in task-oriented chatbots. We compute the BLEU score using all generated responses of our systems.\n\nPer-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.\n\nPer-Dialogue Accuracy: We calculate per-dialogue accuracy as used in BIBREF8, BIBREF5. For this metric, we consider all the system generated responses and compare them to the target responses. A dialogue is considered to be true if all the turns in the system generated responses match the corresponding turns in the target responses. Note that this is a very strict metric in which all the utterances in the dialogue should be the same as the target and in the right order.\n\nF1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses."
      ]
    }
  },
  {
    "paper_id": "1810.11118",
    "question": "Did they experiment with the corpus?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this section, we propose new simple disentanglement models that perform better than prior methods, and re-examine prior work. The models we consider are:"
      ],
      "highlighted_evidence": [
        "In this section, we propose new simple disentanglement models that perform better than prior methods, and re-examine prior work."
      ]
    }
  },
  {
    "paper_id": "1903.08237",
    "question": "Does the paper describe experiments with real humans?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Thus, when size adjectives are noisier than color adjectives, the model produces overinformative referring expressions with color, but not with size – precisely the pattern observed in the literature BIBREF5 , BIBREF0 . Note also that no difference in adjective cost is necessary for obtaining the overinformativeness asymmetry, though assuming a greater cost for size than for color does further increase the observed asymmetry. We defer a discussion of costs to Section \"Experiment 1: scene variation in modified referring expressions\" , where we infer the best parameter values for both the costs and the semantic values of size and color, given data from a reference game experiment.",
        "We recruited 58 pairs of participants (116 participants total) over Amazon's Mechanical Turk who were each paid $1.75 for their participation. Data from another 7 pairs who prematurely dropped out of the experiment and who could therefore not be compensated for their work, were also included. Here and in all other experiments reported in this paper, participants' IP address was limited to US addresses and only participants with a past work approval rate of at least 95% were accepted."
      ],
      "highlighted_evidence": [
        "Experiment 1: scene variation in modified referring expressions",
        "We recruited 58 pairs of participants (116 participants total) over Amazon's Mechanical Turk who were each paid $1.75 for their participation."
      ]
    }
  },
  {
    "paper_id": "1901.10826",
    "question": "Do they compare computational time of AM-softmax versus Softmax?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1901.10826",
    "question": "Do they visualize the difference between AM-Softmax and regular softmax?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In the epoch 96, the proposed method has already an FER more than INLINEFORM0 better than the original SincNet for almost every value of INLINEFORM1 excluding INLINEFORM2 . The difference keeps increasing over the epochs, and at epoch 352 the proposed method has an FER of INLINEFORM3 ( INLINEFORM4 ) against INLINEFORM5 from SincNet, which means that at this epoch AM-SincNet has a Frame Error Rate approximately INLINEFORM6 better than traditional SincNet. The Figure FIGREF7 plots the Frame Error Rate on the test data for both methods along the training epochs. For the AM-SincNet, we used the margin parameter INLINEFORM7 .",
        "FLOAT SELECTED: Fig. 3. Comparison of Frame Error Rate (%) from SincNet and AM-SincNet (m=0.50) over the training epochs for TIMIT dataset."
      ],
      "highlighted_evidence": [
        "The Figure FIGREF7 plots the Frame Error Rate on the test data for both methods along the training epochs. ",
        "FLOAT SELECTED: Fig. 3. Comparison of Frame Error Rate (%) from SincNet and AM-SincNet (m=0.50) over the training epochs for TIMIT dataset."
      ]
    }
  },
  {
    "paper_id": "1810.01570",
    "question": "Do they use BERT?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.12129",
    "question": "Is the origin of the dialogues in corpus some video game and what game is that?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1811.01786",
    "question": "Do they build a generative probabilistic language model for sign language?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1710.00341",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 3: Example from the cQA forum dataset."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 3: Example from the cQA forum dataset."
      ]
    }
  },
  {
    "paper_id": "1606.06361",
    "question": "Do they evaluate the syntactic parses?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1906.10551",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed."
      ],
      "highlighted_evidence": [
        "Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources."
      ]
    }
  },
  {
    "paper_id": "2004.03925",
    "question": "Do the authors give examples of positive and negative sentiment with regard to the virus?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2004.03925",
    "question": "Do they specify which countries they collected twitter data from?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2004.03925",
    "question": "Do they collect only English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In this section, we present the details of the analysis performed on the data obtained pertaining to Twitter messages from January 2020 upto now, that is the time since the news of the Coronavirus outbreak in China was spread across nations. The word frequency data corresponding to the twitter messages has been taken from BIBREF16. The data source indicates that during March 11th to March 30th there were over 4 million tweets a day with the surge in the awareness. Also, the data prominently captures the tweets in English, Spanish, and French languages. A total of four datasets have been used to carry out the study."
      ],
      "highlighted_evidence": [
        "In this section, we present the details of the analysis performed on the data obtained pertaining to Twitter messages from January 2020 upto now, that is the time since the news of the Coronavirus outbreak in China was spread across nations.",
        "Also, the data prominently captures the tweets in English, Spanish, and French languages."
      ]
    }
  },
  {
    "paper_id": "1708.01776",
    "question": "Does the Agent ask for a value of a variable using natural language generated text?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Figure 1. The User-Agent Interaction",
        "In Example UID13 , illustrating a successful interaction, the Agent asks for the value of $V0 and the User responds with the answer (Silvia) as well as an explanation indicating that it was correct (helpful) and why. Specifically, in this instance it was helpful because it enabled an inference which reduced the possible answer set (and reduced the set of relevant variables). On the other hand, in Example UID30 , we see an example of a bad query and corresponding critical explanation."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Figure 1. The User-Agent Interaction",
        "In Example UID13 , illustrating a successful interaction, the Agent asks for the value of $V0 and the User responds with the answer (Silvia) as well as an explanation indicating that it was correct (helpful) and why."
      ]
    }
  },
  {
    "paper_id": "1901.01911",
    "question": "Is this an English-language dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Within this scenario, it is crucial to analyse people attitudes towards rumours in social media and to resolve their veracity as soon as possible. Several approaches have been proposed to check the rumour veracity in social media BIBREF1 . This paper focus on a stance-based analysis of event-related rumours, following the approach proposed at SemEval-2017 in the new RumourEval shared task (Task 8, sub-task A) BIBREF2 . In this task English tweets from conversation threads, each associated to a newsworthy event and the rumours around it, are provided as data. The goal is to determine whether a tweet in the thread is supporting, denying, querying, or commenting the original rumour which started the conversation. It can be considered a stance classification task, where we have to predict the user's stance towards the rumour from a tweet, in the context of a given thread. This task has been defined as open stance classification task and is conceived as a key step in rumour resolution, by providing an analysis of people reactions towards an emerging rumour BIBREF0 , BIBREF3 . The task is also different from detecting stance towards a specific target entity BIBREF4 ."
      ],
      "highlighted_evidence": [
        " In this task English tweets from conversation threads, each associated to a newsworthy event and the rumours around it, are provided as data. The goal is to determine whether a tweet in the thread is supporting, denying, querying, or commenting the original rumour which started the conversation. "
      ]
    }
  },
  {
    "paper_id": "2002.02800",
    "question": "Do they report results only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Using the Twitter Application Program Interface (API) and the IUNI OSoMeBIBREF46 (a service which provides searchable access to the Twitter “Gardenhose”, a 10% sample of all daily tweets), we search for tweets that matched both “diagnos*” and “depress*.” The resulting set of tweets are then filtered for matching the expressions “i”, “diagnos*”, “depres*” in that order in a case-insensitive manner allowing insertions to match the greatest variety of diagnosis statements, e.g. a tweet that states “I was in fact just diagnosed with clinical depression” would match. Finally, to ensure we are only including true self-referential statements of a depression diagnosis, a team of 3 experts manually removed quotes, jokes, and external references. For each qualifying diagnosis tweet we retrieve the timeline of the corresponding Twitter user using the Twitter user_timeline API endpoint . Subsequently, we remove all non-English tweets (Twitter API machine-detected“lang” field), all retweets, and tweets that contain “diagnos*” or “depress*”, but not a valid diagnosis statement. The resulting Depressed cohort contains 1,207 individuals and 1,759,644 tweets ranging from from May 2008 to September 2018."
      ],
      "highlighted_evidence": [
        "For each qualifying diagnosis tweet we retrieve the timeline of the corresponding Twitter user using the Twitter user_timeline API endpoint . Subsequently, we remove all non-English tweets (Twitter API machine-detected“lang” field), all retweets, and tweets that contain “diagnos*” or “depress*”, but not a valid diagnosis statement. The resulting Depressed cohort contains 1,207 individuals and 1,759,644 tweets ranging from from May 2008 to September 2018."
      ]
    }
  },
  {
    "paper_id": "1805.07513",
    "question": "Do they compare with the MAML algorithm?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We compare our method to the following baselines: (1) Single-task CNN: training a CNN model for each task individually; (2) Single-task FastText: training one FastText model BIBREF23 with fixed embeddings for each individual task; (3) Fine-tuned the holistic MTL-CNN: a standard transfer-learning approach, which trains one MTL-CNN model on all the training tasks offline, then fine-tunes the classifier layer (i.e. $\\mathrm {M}^{(cls)}$ Figure 1 (a)) on each target task; (4) Matching Network: a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 9 ; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above single-task encoders with Eq. ( 24 ). This baseline can be viewed as a variation of our method without task clustering. We initialize all models with pre-trained 100-dim Glove embeddings (trained on 6B corpus) BIBREF24 ."
      ],
      "highlighted_evidence": [
        "We compare our method to the following baselines: (1) Single-task CNN: training a CNN model for each task individually; (2) Single-task FastText: training one FastText model BIBREF23 with fixed embeddings for each individual task; (3) Fine-tuned the holistic MTL-CNN: a standard transfer-learning approach, which trains one MTL-CNN model on all the training tasks offline, then fine-tunes the classifier layer (i.e. $\\mathrm {M}^{(cls)}$ Figure 1 (a)) on each target task; (4) Matching Network: a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 9 ; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above single-task encoders with Eq. ( 24 ). "
      ]
    }
  },
  {
    "paper_id": "2004.01670",
    "question": "Have any baseline model been trained on this abusive language dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1710.09589",
    "question": "is the dataset balanced across the four languages?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."
      ],
      "highlighted_evidence": [
        "An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."
      ]
    }
  },
  {
    "paper_id": "1705.01991",
    "question": "Do they only test on one dataset?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The data set we evaluate on in this work is WMT English-French NewsTest2014, which has 380M words of parallel training data and a 3003 sentence test set. The NewsTest2013 set is used for validation. In order to compare our architecture to past work, we train a word-based system without any data augmentation techniques. The network architecture is very similar to BIBREF4 , and specific details of layer size/depth are provided in subsequent sections. We use an 80k source/target vocab and perform standard unk-replacement BIBREF1 on out-of-vocabulary words. Training is performed using an in-house toolkit."
      ],
      "highlighted_evidence": [
        "The data set we evaluate on in this work is WMT English-French NewsTest2014, which has 380M words of parallel training data and a 3003 sentence test set. The NewsTest2013 set is used for validation."
      ]
    }
  },
  {
    "paper_id": "1703.02507",
    "question": "Do they report results only on English data?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.",
        "Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method."
      ],
      "highlighted_evidence": [
        "Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. ",
        "Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. "
      ]
    }
  },
  {
    "paper_id": "1810.05334",
    "question": "Did they use a crowdsourcing platform for the summaries?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles. Each article has the title, category, source (e.g., CNN Indonesia, Kumparan), URL to the original article, and an abstractive summary which was created manually by a total of 2 native speakers of Indonesian. There are 6 categories in total: Entertainment, Inspiration, Sport, Showbiz, Headline, and Tech. A sample article-summary pair is shown in Fig. FIGREF4 ."
      ],
      "highlighted_evidence": [
        "We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles. Each article has the title, category, source (e.g., CNN Indonesia, Kumparan), URL to the original article, and an abstractive summary which was created manually by a total of 2 native speakers of Indonesian"
      ]
    }
  },
  {
    "paper_id": "1710.09340",
    "question": "Do they measure the number of created No-Arc long sequences?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2003.04967",
    "question": "Do they evaluate only on English datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Twitter data: We used the Twitter API to scrap tweets with hashtags. For instance, for Bitcoin, the #BTC and #Bitcoin tags were used. The Twitter API only allows a maximum of 450 requests per 15 minute and historical data up to 7 days. Throughout our project we collect data for almost 30 days. Bitcoin had about 25000 tweets per day amounting to a total of approximately 10 MB of data daily. For each tweet, the ID, text, username, number of followers, number of retweets, creation date and time was also stored. All non-English tweets were filtered out by the API. We further processed the full tweet text by removing links, images, videos and hashtags to feed in to the algorithm."
      ],
      "highlighted_evidence": [
        "Twitter data: We used the Twitter API to scrap tweets with hashtags.",
        "All non-English tweets were filtered out by the API."
      ]
    }
  },
  {
    "paper_id": "1910.02334",
    "question": "Is the dataset multimodal?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function. The last single neuron with no activation function was added at the end to predict the hate speech detection score."
      ],
      "highlighted_evidence": [
        "The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function."
      ]
    }
  },
  {
    "paper_id": "1911.05343",
    "question": "Do they compare against state of the art text generation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We compare our HR-VAE model with three strong baselines using VAE for text modelling:",
        "VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;",
        "VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;",
        "vMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5."
      ],
      "highlighted_evidence": [
        "We compare our HR-VAE model with three strong baselines using VAE for text modelling:\n\nVAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;\n\nVAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;\n\nvMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5."
      ]
    }
  },
  {
    "paper_id": "1809.01500",
    "question": "Was the system only evaluated over the second shared task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "In recent years, there has been a rapid growth in the usage of social media. People post their day-to-day happenings on regular basis. BIBREF0 propose four tasks for detecting drug names, classifying medication intake, classifying adverse drug reaction and detecting vaccination behavior from tweets. We participated in the Task2 and Task4."
      ],
      "highlighted_evidence": [
        "In recent years, there has been a rapid growth in the usage of social media. People post their day-to-day happenings on regular basis. BIBREF0 propose four tasks for detecting drug names, classifying medication intake, classifying adverse drug reaction and detecting vaccination behavior from tweets. We participated in the Task2 and Task4."
      ]
    }
  },
  {
    "paper_id": "1704.04539",
    "question": "Do the authors test their annotation projection techniques on tasks other than AMR?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Cross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis BIBREF29 but it has also been used for dependency parsing BIBREF30 , role labeling BIBREF31 , BIBREF32 and semantic parsing BIBREF26 . Another common thread of cross-lingual work is model transfer, where parameters are shared across languages BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 ."
      ],
      "highlighted_evidence": [
        "The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis BIBREF29 but it has also been used for dependency parsing BIBREF30 , role labeling BIBREF31 , BIBREF32 and semantic parsing BIBREF26 ."
      ]
    }
  },
  {
    "paper_id": "1908.11365",
    "question": "Is the proposed layer smaller in parameters than a Transformer?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "FLOAT SELECTED: Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task. #Param: number of model parameters. 4Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. 4Train: training time (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3 runs using Tensorflow on a single TITAN X (Pascal). “-”: optimization failed and no result. “?”: the same as model 1©. † and ‡: comparison against 11© and 14© respectively rather than 1©. Base: the baseline Transformer with base setting. Bold indicates best BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens.",
        "FLOAT SELECTED: Table 5: Translation results on different tasks. Settings for BLEU score is given in Section 7.1. Numbers in bracket denote chrF score. Our model outperforms the vanilla base Transformer on all tasks. “Ours”: DS-Init+MAtt."
      ],
      "highlighted_evidence": [
        "FLOAT SELECTED: Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task. #Param: number of model parameters. 4Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. 4Train: training time (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3 runs using Tensorflow on a single TITAN X (Pascal). “-”: optimization failed and no result. “?”: the same as model 1©. † and ‡: comparison against 11© and 14© respectively rather than 1©. Base: the baseline Transformer with base setting. Bold indicates best BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens.",
        "FLOAT SELECTED: Table 5: Translation results on different tasks. Settings for BLEU score is given in Section 7.1. Numbers in bracket denote chrF score. Our model outperforms the vanilla base Transformer on all tasks. “Ours”: DS-Init+MAtt."
      ]
    }
  },
  {
    "paper_id": "1911.08370",
    "question": "How are the clusters related to security, violence and crime identified?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.05960",
    "question": "Are there some results better than state of the art on these tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To verify the effectiveness of our CRU model, we utilize it into two different NLP tasks: sentiment classification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model. To further demonstrate the effectiveness of our model, we also tested our CRU in reading comprehension tasks with a strengthened baseline system originated from Attention-over-Attention Reader (AoA Reader) BIBREF10. Experimental results on public datasets show that our CRU model could substantially outperform various systems by a large margin, and set up new state-of-the-art performances on related datasets. The main contributions of our work are listed as follows."
      ],
      "highlighted_evidence": [
        "Experimental results on public datasets show that our CRU model could substantially outperform various systems by a large margin, and set up new state-of-the-art performances on related datasets."
      ]
    }
  },
  {
    "paper_id": "1911.05960",
    "question": "Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The experimental results are shown in Table TABREF35. As we mentioned before, all RNNs in these models are bi-directional, because we wonder if our bi-CRU could still give substantial improvements over bi-GRU which could capture both history and future information. As we can see that, all variants of our CRU model could give substantial improvements over the traditional GRU model, where a maximum gain of 2.7%, 1.0%, and 1.9% can be observed in three datasets, respectively. We also found that though we adopt a straightforward classification model, our CRU model could outperform the state-of-the-art systems by 0.6%, 0.7%, and 0.8% gains respectively, which demonstrate its effectiveness. By employing more sophisticated architecture or introducing task-specific features, we think there is still much room for further improvements, which is beyond the scope of this paper."
      ],
      "highlighted_evidence": [
        "As we can see that, all variants of our CRU model could give substantial improvements over the traditional GRU model, where a maximum gain of 2.7%, 1.0%, and 1.9% can be observed in three datasets, respectively."
      ]
    }
  },
  {
    "paper_id": "1805.11189",
    "question": "So we do not use pre-trained embedding in this case?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "2002.01861",
    "question": "Was the structure of regulatory filings exploited when training the model? ",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We extend BERT Base-Chinese (12-layer, 768-hidden, 12-heads, 110M parameters) for sequence labeling. All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end. All inputs are then padded to a length of 256 tokens. After feeding through BERT, we obtain the hidden state of the final layer, denoted as ($h_{1}$, $h_{2}$, ... $h_{N}$) where $N$ is the max length setting. We add a fully-connected layer and softmax on top, and the final prediction is formulated as:"
      ],
      "highlighted_evidence": [
        "All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end."
      ]
    }
  },
  {
    "paper_id": "1909.08090",
    "question": "Do they compare their algorithm to voting without weights?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1910.10869",
    "question": "Is this approach compared to some baseline?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Table TABREF24 gives the UAR for each feature subset individually, for all features combined, and for a combination in which one feature subset in turn is left out. The one-feature-set-at-time results suggest that prosody, speech activity and words are of increasing importance in that order. The leave-one-out analysis agrees that the words are the most important (largest drop in accuracy when removed), but on that criterion the prosodic features are more important than speech-activity. The combination of all features is 0.4% absolute better than any other subset, showing that all feature subsets are partly complementary.",
        "FLOAT SELECTED: Table 2. Hot spot classification results with individual feature subsets, all features, and with individual feature sets left out."
      ],
      "highlighted_evidence": [
        "Table TABREF24 gives the UAR for each feature subset individually, for all features combined, and for a combination in which one feature subset in turn is left out. The one-feature-set-at-time results suggest that prosody, speech activity and words are of increasing importance in that order. ",
        "FLOAT SELECTED: Table 2. Hot spot classification results with individual feature subsets, all features, and with individual feature sets left out."
      ]
    }
  },
  {
    "paper_id": "1907.04380",
    "question": "Is such bias caused by bad annotation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Natural Language Inference (NLI) is often used to gauge a model's ability to understand a relationship between two texts BIBREF0 , BIBREF1 . In NLI, a model is tasked with determining whether a hypothesis (a woman is sleeping) would likely be inferred from a premise (a woman is talking on the phone). The development of new large-scale datasets has led to a flurry of various neural network architectures for solving NLI. However, recent work has found that many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts BIBREF2 , BIBREF3 , BIBREF4 . For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases."
      ],
      "highlighted_evidence": [
        "However, recent work has found that many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts BIBREF2 , BIBREF3 , BIBREF4 . For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases."
      ]
    }
  },
  {
    "paper_id": "1904.09545",
    "question": "Do they experiment with language modeling on large datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1908.02322",
    "question": "Did they crowdsource the annotations?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices."
      ],
      "highlighted_evidence": [
        "To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices."
      ]
    }
  },
  {
    "paper_id": "1911.02086",
    "question": "Do they compare executionttime of their model against other models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "The base model composed of DSConv layers without grouping achieves the state-of-the-art accuracy of 96.6% on the Speech Commands test set. The low-parameter model with GDSConv achieves almost the same accuracy of 96.4% with only about half the parameters. This validates the effectiveness of GDSConv for model size reduction. Table TABREF15 lists these results in comparison with related work. Compared to the DSConv network in BIBREF1, our network is more efficient in terms of accuracy for a given parameter count. Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters. Choi et al. BIBREF3 has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters. They are using 1D convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least KWS.",
        "FLOAT SELECTED: Table 1. Comparison of results on the Speech Commands dataset [19].",
        "FLOAT SELECTED: Table 2. Results on Speech Commands version 2 [19]."
      ],
      "highlighted_evidence": [
        "The base model composed of DSConv layers without grouping achieves the state-of-the-art accuracy of 96.6% on the Speech Commands test set. The low-parameter model with GDSConv achieves almost the same accuracy of 96.4% with only about half the parameters. ",
        "FLOAT SELECTED: Table 1. Comparison of results on the Speech Commands dataset [19].",
        "FLOAT SELECTED: Table 2. Results on Speech Commands version 2 [19]."
      ]
    }
  },
  {
    "paper_id": "1911.08829",
    "question": "Are PIEs extracted automatically subjected to human evaluation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For parser-based extraction, systems with and without in-context parsing, ignoring labels, and ignoring directionality are tested. For the three string-based extraction methods, varying numbers of intervening words and case sensitivity are evaluated. Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.",
        "Since these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score. The exact spans are ignored, because the spans annotated in the evaluation corpus are not completely reliable. These were automatically generated during candidate extraction, as described in Section SECREF45. Rather, we count an extraction as a true positive if it finds the correct PIE type in the correct sentence."
      ],
      "highlighted_evidence": [
        "Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.\n\nSince these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score."
      ]
    }
  },
  {
    "paper_id": "1909.09524",
    "question": "Are experiments performed with any other pair of languages, how did proposed method perform compared to other models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We evaluate the proposed transfer learning techniques in two non-English language pairs of WMT 2019 news translation tasks: French$\\rightarrow $German and German$\\rightarrow $Czech."
      ],
      "highlighted_evidence": [
        "We evaluate the proposed transfer learning techniques in two non-English language pairs of WMT 2019 news translation tasks: French$\\rightarrow $German and German$\\rightarrow $Czech."
      ]
    }
  },
  {
    "paper_id": "1909.09524",
    "question": "Is pivot language used in experiments English or some other language?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Nonetheless, the main caveat of this basic pre-training is that the source encoder is trained to be used by an English decoder, while the target decoder is trained to use the outputs of an English encoder — not of a source encoder. In the following, we propose three techniques to mitigate the inconsistency of source$\\rightarrow $pivot and pivot$\\rightarrow $target pre-training stages. Note that these techniques are not exclusive and some of them can complement others for a better performance of the final model."
      ],
      "highlighted_evidence": [
        "Nonetheless, the main caveat of this basic pre-training is that the source encoder is trained to be used by an English decoder, while the target decoder is trained to use the outputs of an English encoder — not of a source encoder."
      ]
    }
  },
  {
    "paper_id": "1910.14443",
    "question": "Is model compared against state of the art models on these datasets?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Deep convolutional neural networks (CNNs) with 2D convolutions and small kernels BIBREF1, have achieved state-of-the-art results for several speech recognition tasks BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6. The accuracy of those models grows with their complexity, leading to redundant latent representations. Several approaches have been proposed in the literature to reduce this redundancy BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, and therefore to improve their efficiency.",
        "Models: Our baseline CNN model BIBREF21 consists of 15 convolutional and one fully-connected layer. We use $3\\times 3$ kernels throughout the network. We start with 64 output channels in the first layer and double them after 3 and 9 layers. We use batch normalization in every convolutional layer, and ReLU afterwards (unless a reverse order is noted). The initial learning rate is 0.001. We use early stopping for training."
      ],
      "highlighted_evidence": [
        "Deep convolutional neural networks (CNNs) with 2D convolutions and small kernels BIBREF1, have achieved state-of-the-art results for several speech recognition tasks BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6.",
        "r",
        "Models: Our baseline CNN model BIBREF21 consists of 15 convolutional and one fully-connected layer."
      ]
    }
  },
  {
    "paper_id": "1601.06738",
    "question": "Does this paper address the variation among English dialects regarding these hedges?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1904.02815",
    "question": "Do the authors do manual evaluation?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.",
        "Analysis: As is evident from the experiments on both the versions of SWBD, our model (HN-SA) outperforms traditional feature based topic spotting models and deep learning based document classification models. It is interesting to see that simple BoW and n-gram baselines are quite competitive and outperform some of the deep learning based document classification model. Similar observation has also been reported by BIBREF31 ( BIBREF31 ) for the task of sentiment analysis. The task of topic spotting is arguably more challenging than document classification. In the topic spotting task, the number of output classes (66/42 classes) is much more than those in document classification (5/6 classes), which is done mainly on the texts from customer reviews. Dialogues in SWBD have on an average 200 utterances and are much longer texts than customer reviews. Additionally, the number of dialogues available for training the model is significantly lesser than customer reviews. We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2. For most of the classes the classifier is able to predict accurately. However, the model gets confused between the classes which are semantically close (w.r.t. terms used) to each other, for example, the model gets confused between pragmatically similar topics e.g. HOBBIES vs GARDENING, MOVIES vs TV PROGRAMSâ, RIGHT TO PRIVACY vs DRUG TESTING."
      ],
      "highlighted_evidence": [
        "We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.",
        "We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2."
      ]
    }
  },
  {
    "paper_id": "1909.05890",
    "question": "Was performance of the weakly-supervised model compared to the performance of a supervised model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The precision when labeling the first x ranked tweets as attack tweet is shown in the figure FIGREF39. The x-axis is the number of ranked tweets treated as attack tweets. And the y-axis is the corresponding precision. The straight line in figures FIGREF39, FIGREF43 and FIGREF51 is the result of a supervised LDA algorithm which is used as a baseline. Supervised LDA achieved 96.44 percent precision with 10 fold cross validation."
      ],
      "highlighted_evidence": [
        "The straight line in figures FIGREF39, FIGREF43 and FIGREF51 is the result of a supervised LDA algorithm which is used as a baseline. "
      ]
    }
  },
  {
    "paper_id": "1912.06927",
    "question": "Do the tweets come from a specific region?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Table TABREF14 presents the distribution of the tweets by country before and after the filtering process. A large portion of the samples is from India because the MeToo movement has peaked towards the end of 2018 in India. There are very few samples from Russia likely because of content moderation and regulations on social media usage in the country. Figure FIGREF15 gives a geographical distribution of the curated dataset."
      ],
      "highlighted_evidence": [
        "Table TABREF14 presents the distribution of the tweets by country before and after the filtering process. A large portion of the samples is from India because the MeToo movement has peaked towards the end of 2018 in India. There are very few samples from Russia likely because of content moderation and regulations on social media usage in the country."
      ]
    }
  },
  {
    "paper_id": "1909.01247",
    "question": "Did they experiment with the corpus?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The corpus creation process involved a small number of people that have voluntarily joined the initiative, with the authors of this paper directing the work. Initially, we searched for NER resources in Romanian, and found none. Then we looked at English resources and read the in-depth ACE guide, out of which a 16-class draft evolved. We then identified a copy-right free text from which we hand-picked sentences to maximize the amount of entities while maintaining style balance. The annotation process was a trial-and-error, with cycles composed of annotation, discussing confusing entities, updating the annotation guide schematic and going through the corpus section again to correct entities following guide changes. The annotation process was done online, in BRAT. The actual annotation involved 4 people, has taken about 6 months (as work was volunteer-based, we could not have reached for 100% time commitment from the people involved), and followed the steps:"
      ],
      "highlighted_evidence": [
        "The annotation process was a trial-and-error, with cycles composed of annotation, discussing confusing entities, updating the annotation guide schematic and going through the corpus section again to correct entities following guide changes."
      ]
    }
  },
  {
    "paper_id": "1706.01723",
    "question": "Do they jointly tackle multiple tagging problems?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We evaluate our method on three tagging tasks: POS tagging (Pos), morphological tagging (Morph) and supertagging (Stag).",
        "We select these tasks as examples for tagging applications because they differ strongly in tag set sizes. Generally, the Pos set sizes for all the languages are no more than 17 and Stag set sizes are around 200. When treating morphological features as a string (i.e. not splitting into key-value pairs), the sizes of the Morph tag sets range from about 100 up to 2000.",
        "The test results for the three tasks are shown in Table TABREF17 in three groups. The first group of seven columns are the results for Pos, where both LSTM and CNN have three variations of input features: word only ( INLINEFORM0 ), character only ( INLINEFORM1 ) and both ( INLINEFORM2 ). For Morph and Stag, we only use the INLINEFORM3 setting for both LSTM and CNN."
      ],
      "highlighted_evidence": [
        "We evaluate our method on three tagging tasks: POS tagging (Pos), morphological tagging (Morph) and supertagging (Stag).",
        "We select these tasks as examples for tagging applications because they differ strongly in tag set sizes.",
        "The test results for the three tasks are shown in Table TABREF17 in three groups. The first group of seven columns are the results for Pos, where both LSTM and CNN have three variations of input features: word only ( INLINEFORM0 ), character only ( INLINEFORM1 ) and both ( INLINEFORM2 ). For Morph and Stag, we only use the INLINEFORM3 setting for both LSTM and CNN."
      ]
    }
  },
  {
    "paper_id": "1811.01299",
    "question": "Does their solution involve connecting images and text?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Once the S-V-O is generated, Text2Visual provides users with visual components that convey the S-V-O text meanings."
      ],
      "highlighted_evidence": [
        "Once the S-V-O is generated, Text2Visual provides users with visual components that convey the S-V-O text meanings."
      ]
    }
  },
  {
    "paper_id": "1911.03514",
    "question": "Did they experiment on the proposed task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1911.03514",
    "question": "Is annotation done manually?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The biggest difference between discourse parsing for well-written document and dialogues is that discourse relations can exist on two nonadjacent utterances in dialogues. When we annotate dialogues, we should read dialogues from begin to the end. For each utterance, we should find its one parent node at least from all its previous utterances. We assume that the discourse structure is a connected graph and no utterance is isolated.",
        "We propose three questions for eache dialogue and annotate the span of answers in the input dialogue. As we know, our dataset is the first corpus for multi-party dialogues reading comprehension.",
        "We construct following questions and answers for the dialogue in Example 1:",
        "Q1: When does Bdale leave?",
        "A1: Fri morning",
        "Q2: How to get people love Mark in Mjg59's opinion.",
        "A2: Hire people to work on reverse-engineering closed drivers.",
        "On the other hand, to improve the difficulty of the task, we propose $ \\frac{1}{6}$ to $ \\frac{1}{3}$ unanswerable questions in our dataset. We annotate unanswerable questions and their plausible answers (PA). Each plausible answer comes from the input dialogue, but is not the answer for the plausible question.",
        "Q1: Whis is the email of daniels?",
        "PA: +61 403 505 896"
      ],
      "highlighted_evidence": [
        "When we annotate dialogues, we should read dialogues from begin to the end. For each utterance, we should find its one parent node at least from all its previous utterances. We assume that the discourse structure is a connected graph and no utterance is isolated.",
        "We propose three questions for eache dialogue and annotate the span of answers in the input dialogue. As we know, our dataset is the first corpus for multi-party dialogues reading comprehension.\n\nWe construct following questions and answers for the dialogue in Example 1:\n\nQ1: When does Bdale leave?\n\nA1: Fri morning\n\nQ2: How to get people love Mark in Mjg59's opinion.\n\nA2: Hire people to work on reverse-engineering closed drivers.\n\nOn the other hand, to improve the difficulty of the task, we propose $ \\frac{1}{6}$ to $ \\frac{1}{3}$ unanswerable questions in our dataset. We annotate unanswerable questions and their plausible answers (PA). Each plausible answer comes from the input dialogue, but is not the answer for the plausible question.\n\nQ1: Whis is the email of daniels?\n\nPA: +61 403 505 896"
      ]
    }
  },
  {
    "paper_id": "1705.00861",
    "question": "Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "For all experiments, the dimensions of word embeddings and recurrent hidden states are both set to 512. The dimension of INLINEFORM0 is also of size 512. Note that our network is more narrow than most previous work where hidden states of dimmention 1024 is used. we initialize parameters by sampling each element from the Gaussian distribution with mean 0 and variance INLINEFORM1 ."
      ],
      "highlighted_evidence": [
        "For all experiments, the dimensions of word embeddings and recurrent hidden states are both set to 512."
      ]
    }
  },
  {
    "paper_id": "1902.00756",
    "question": "So this paper turns unstructured text inputs to parameters that GNNs can read?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this section, we will introduce the general framework of GP-GNNs. GP-GNNs first build a fully-connected graph $\\mathcal {G} = (\\mathcal {V}, \\mathcal {E})$ , where $\\mathcal {V}$ is the set of entities, and each edge $(v_i, v_j) \\in \\mathcal {E}, v_i, v_j \\in \\mathcal {V}$ corresponds to a sequence $s = x_0^{i,j}, x_1^{i,j}, \\dots , x_{l-1}^{i,j}$ extracted from the text. After that, GP-GNNs employ three modules including (1) encoding module, (2) propagation module and (3) classification module to proceed relational reasoning, as shown in Fig. 2 ."
      ],
      "highlighted_evidence": [
        "GP-GNNs first build a fully-connected graph $\\mathcal {G} = (\\mathcal {V}, \\mathcal {E})$ , where $\\mathcal {V}$ is the set of entities, and each edge $(v_i, v_j) \\in \\mathcal {E}, v_i, v_j \\in \\mathcal {V}$ corresponds to a sequence $s = x_0^{i,j}, x_1^{i,j}, \\dots , x_{l-1}^{i,j}$ extracted from the text. "
      ]
    }
  },
  {
    "paper_id": "2003.09520",
    "question": "Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%. This result is not satisfactory on an absolute scale, however it is more than encouraging taking into account the small size of our data. This result means that less than 3 tokens, on average, out of 10, must be corrected to increase the size of our corpus. With this model we automatically transcribed into Arabic morphemes, roughly, 5,000 additional tokens, corresponding to the second annotation block. This can be manually annotated in at least 7,5 days, but thanks to the automatic annotation accuracy, it was manually corrected into 3 days. The accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set. The manually-corrected additional tokens were added to the training data of our neural model, and a new block was automatically annotated and manually corrected. Both accuracy on the test set and on the annotation block remained at around 70%. This is because the block added to the training data was significantly different from the previous and from the third. Adding the third block to the training data and annotating a fourth block with the new trained model gave in contrast an accuracy of roughly 80%. This incremental, semi-automatic transcription procedure is in progress for the remaining blocks, but it is clear that it will make the corpus annotation increasingly easier and faster as the amount of training data will grow up."
      ],
      "highlighted_evidence": [
        "The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%. "
      ]
    }
  },
  {
    "paper_id": "1807.09000",
    "question": "Did participants behave unexpectedly?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Experiment 1 directly tested the hypothesis that speakers increase their specificity in contexts with asymmetry in visual access. We found that speakers are not only context-sensitive in choosing referring expressions that distinguish target from distractors in a shared context, but are occlusion-sensitive, adaptively compensating for uncertainty. Critically, this resulted in systematic differences in behavior across the occlusion conditions that are difficult to explain under an egocentric theory: in the presence of occlusions, speakers were spontaneously willing to spend additional time and keystrokes to give further information beyond what they produce in the corresponding unoccluded contexts, even though that information is equally redundant given the visible objects in their display.",
        "These results strongly suggest that the speaker's informativity influences listener accuracy. In support of this hypothesis, we found a strong negative correlation between informativity and error rates across items and conditions: listeners make fewer errors when utterances are a better fit for the target relative to the distractor ( $\\rho = -0.81$ , bootstrapped 95% CI $= [-0.9, -0.7]$ ; Fig. 6 B). This result suggests that listener behavior is driven by an expectation of speaker informativity: listeners interpret utterances proportionally to how well they fit objects in context.",
        "Are human adults expert mind-readers, or fundamentally egocentric? The longstanding debate over the role of theory of mind in communication has largely centered around whether listeners (or speakers) with private information consider their partner's perspective BIBREF30 , BIBREF16 . Our work presents a more nuanced picture of how a speaker and a listener use theory of mind to modulate their pragmatic expectations. The Gricean cooperative principle emphasizes a natural division of labor in how the joint effort of being cooperative is shared BIBREF4 , BIBREF60 . It can be asymmetric when one partner is expected to, and able to, take on more complex reasoning than the other, in the form of visual perspective-taking, pragmatic inference, or avoiding further exchanges of clarification and repair. One such case is when the speaker has uncertainty over what the listener can see, as in the director-matcher task. Our Rational Speech Act (RSA) formalization of cooperative reasoning in this context predicts that speakers (directors) naturally increase the informativity of their referring expressions to hedge against the increased risk of misunderstanding; Exp. 1 presents direct evidence in support of this hypothesis.",
        "Importantly, when the director (speaker) is expected to be appropriately informative, communication can be successful even when the matcher (listener) does not reciprocate the effort. If visual perspective-taking is effortful and cognitively demanding BIBREF39 , the matcher will actually minimize joint effort by not taking the director's visual perspective. This suggests a less egocentric explanation of when and why listeners neglect the speaker's visual perspective; they do so when they expect the speaker to disambiguate referents sufficiently. While adaptive in most natural communicative contexts, such neglect might backfire and lead to errors when the speaker (inexplicably) violates this expectation. From this point of view, the “failure” of listener theory of mind in these tasks is not really a failure; instead, it suggests that both speakers and listeners may use theory of mind to know when (and how much) they should expect others to be cooperative and informative, and subsequently allocate their resources accordingly BIBREF36 . Exp. 2 is consistent with this hypothesis; when directors used underinformative scripted instructions (taken from prior work), listeners made significantly more errors than when speakers were allowed to provide referring expressions at their natural level of informativity, and speaker informativeness strongly modulated listener error rates."
      ],
      "highlighted_evidence": [
        "Experiment 1 directly tested the hypothesis that speakers increase their specificity in contexts with asymmetry in visual access. We found that speakers are not only context-sensitive in choosing referring expressions that distinguish target from distractors in a shared context, but are occlusion-sensitive, adaptively compensating for uncertainty.",
        "These results strongly suggest that the speaker's informativity influences listener accuracy. In support of this hypothesis, we found a strong negative correlation between informativity and error rates across items and conditions: listeners make fewer errors when utterances are a better fit for the target relative to the distractor ( $\\rho = -0.81$ , bootstrapped 95% CI $= [-0.9, -0.7]$ ; Fig. 6 B). This result suggests that listener behavior is driven by an expectation of speaker informativity: listeners interpret utterances proportionally to how well they fit objects in context.",
        "Our Rational Speech Act (RSA) formalization of cooperative reasoning in this context predicts that speakers (directors) naturally increase the informativity of their referring expressions to hedge against the increased risk of misunderstanding; Exp. 1 presents direct evidence in support of this hypothesis.",
        "Exp. 2 is consistent with this hypothesis; when directors used underinformative scripted instructions (taken from prior work), listeners made significantly more errors than when speakers were allowed to provide referring expressions at their natural level of informativity, and speaker informativeness strongly modulated listener error rates."
      ]
    }
  },
  {
    "paper_id": "1807.09000",
    "question": "Was this experiment done in a lab?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "We recruited 102 pairs of participants from Amazon Mechanical Turk and randomly assigned speaker and listener roles. After we removed 7 games that disconnected part-way through and 12 additional games according to our pre-registered exclusion criteria (due to being non-native English speakers, reporting confusion about the instructions, or clearly violating the instructions), we were left with a sample of 83 full games."
      ],
      "highlighted_evidence": [
        "We recruited 102 pairs of participants from Amazon Mechanical Turk and randomly assigned speaker and listener roles"
      ]
    }
  },
  {
    "paper_id": "1702.06700",
    "question": "Does the new system utilize pre-extracted bounding boxes and/or features?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "In this section, we elaborate our model consisting of four parts: (a) image feature pre-selection part which models the tendency where people focus to ask questions, (b) question encoding part which encodes the question words as a condensed semantic embedding, (c) attention-based feature fusion part performs second selection on image features and (d) answer generation part which gives the answer output.",
        "We propose to perform saliency-like pre-selection operation to alleviate the problems and model the RoI patterns. The image is first divided into $g\\times g$ grids as illustrated in Figure. 2 . Taking $m\\times m$ grids as a region, with $s$ grids as the stride, we obtain $n\\times n$ regions, where $n=\\left\\lfloor \\frac{g-m}{s}\\right\\rfloor +1$ . We then feed the regions to a pre-trained ResNet BIBREF24 deep convolutional neural network to produce $n\\times n\\times d_I$ -dimensional region features, where $d_I$ is the dimension of feature from the layer before the last fully-connected layer."
      ],
      "highlighted_evidence": [
        "image feature pre-selection part which models the tendency where people focus to ask questions",
        "We propose to perform saliency-like pre-selection operation to alleviate the problems and model the RoI patterns. The image is first divided into $g\\times g$ grids as illustrated in Figure. 2 . Taking $m\\times m$ grids as a region, with $s$ grids as the stride, we obtain $n\\times n$ regions, where $n=\\left\\lfloor \\frac{g-m}{s}\\right\\rfloor +1$ . We then feed the regions to a pre-trained ResNet BIBREF24 deep convolutional neural network to produce $n\\times n\\times d_I$ -dimensional region features, where $d_I$ is the dimension of feature from the layer before the last fully-connected layer."
      ]
    }
  },
  {
    "paper_id": "1909.02151",
    "question": "Do they consider other tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1606.00189",
    "question": "Do they use pretrained word representations in their neural network models?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not limited to specific error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction BIBREF0 , BIBREF1 , BIBREF2 . In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners' errors.",
        "We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT systems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems BIBREF3 , BIBREF4 . These neural networks are able to capture non-linear relationships between source and target sentences and can encode contextual information more effectively. Our experiments show that the addition of these two neural networks leads to significant improvements over a strong baseline and outperforms the current state of the art.",
        "To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) BIBREF14 . The latest version of Moses can incorporate NNJM trained using NPLM as a feature while decoding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes. We use rectified linear units (ReLU) as the activation function. We train NNJM with noise contrastive estimation with 100 noise samples per training instance, which are obtained from a unigram distribution. The neural network is trained for 30 epochs using stochastic gradient descent optimization with a mini-batch size of 128 and learning rate of 0.1."
      ],
      "highlighted_evidence": [
        "Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. ",
        "We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT systems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems BIBREF3 , BIBREF4 .",
        "To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) BIBREF14 . The latest version of Moses can incorporate NNJM trained using NPLM as a feature while decoding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes. "
      ]
    }
  },
  {
    "paper_id": "2004.03061",
    "question": "Does the work explicitly study the relationship between model complexity and linguistic structure encoding?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1909.00574",
    "question": "Does the training dataset provide logical form supervision?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "The dataset MSParS is published by NLPCC 2019 evaluation task. The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. Metric for this dataset is the exactly matching accuracy on both full test set and hard test subset. Each sample is composed of the question, the logical form, the parameters(entity/value/type) and question type as the Table TABREF3 demonstrates."
      ],
      "highlighted_evidence": [
        "The dataset MSParS is published by NLPCC 2019 evaluation task. The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. Metric for this dataset is the exactly matching accuracy on both full test set and hard test subset. Each sample is composed of the question, the logical form, the parameters(entity/value/type) and question type as the Table TABREF3 demonstrates."
      ]
    }
  },
  {
    "paper_id": "1711.06288",
    "question": "Is there any human evaluation involved in evaluating this famework?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Due to the lack of available models for the task, we compare our framework with a previous model developed for image-to-image translation as baseline, which colorizes images without text descriptions. We carried out two human evaluations using Mechanical Turk to compare the performance of our model and the baseline. For each experiment, we randomly sampled 1,000 images from the test set and then turned these images into black and white. For each image, we generated a pair of two images using our model and the baseline, respectively. Our model took into account the caption in generation while the baseline did not. Then we randomly permuted the 2,000 generated images. In the first experiment, we presented to human annotators the 2,000 images, together with their original captions, and asked humans to rate the consistency between the generated images and the captions in a scale of 0 and 1, with 0 indicating no consistency and 1 indicating consistency. In the second experiment, we presented to human annotators the same 2,000 images without captions, but asked human annotators to rate the quality of each image without providing its original caption. The quality was rated in a scale of 0 and 1, with 0 indicating low quality and 1 indicating high quality."
      ],
      "highlighted_evidence": [
        "We carried out two human evaluations using Mechanical Turk to compare the performance of our model and the baseline."
      ]
    }
  },
  {
    "paper_id": "1707.06878",
    "question": "Do they use a neural model for their task?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Figure 1 presents architecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach BIBREF8 implemented using the Apache Spark framework, enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words BIBREF9 . Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst:92 patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to BIBREF10 . Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.",
        "Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.",
        "Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.",
        "Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class."
      ],
      "highlighted_evidence": [
        "Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.\n\nWord senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.\n\nSuper senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.\n\nSuper senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class."
      ]
    }
  },
  {
    "paper_id": "1608.04207",
    "question": "Do they analyze ELMo?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1606.07298",
    "question": "Do the experiments explore how various architectures and layers contribute towards certain decisions?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "As a document classifier we employ a word-based CNN similar to Kim consisting of the following sequence of layers: $ \\texttt {Conv} \\xrightarrow{} \\texttt {ReLU} \\xrightarrow{} \\texttt {1-Max-Pool} \\xrightarrow{} \\texttt {FC} \\\\ $",
        "Future work would include applying LRP to other neural network architectures (e.g. character-based or recurrent models) on further NLP tasks, as well as exploring how relevance information could be taken into account to improve the classifier's training procedure or prediction performance."
      ],
      "highlighted_evidence": [
        "As a document classifier we employ a word-based CNN similar to Kim consisting of the following sequence of layers: $ \\texttt {Conv} \\xrightarrow{} \\texttt {ReLU} \\xrightarrow{} \\texttt {1-Max-Pool} \\xrightarrow{} \\texttt {FC} \\\\ $",
        "Future work would include applying LRP to other neural network architectures (e.g. character-based or recurrent models) on further NLP tasks, as well as exploring how relevance information could be taken into account to improve the classifier's training procedure or prediction performance."
      ]
    }
  },
  {
    "paper_id": "1908.11279",
    "question": "Do the authors perform experiments using their proposed method?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "We illustrate the concept by discussing some instantiations that we have recently experimented with.",
        "Unlike the Visual Dialogue setting discussed above, this setting ensures informational symmetry between the participants (both have access to the same type of information; but not the same information, as they can't “see” each other). More importantly, however, the constraint that the game only ends if they both agree ensures a “committment symmetry”, where the success of the game must be ensured by both participants. The design also provides for a clear “relevance place” at which an opportunity arises for semantic negotiation, namely, before the final decision is made. An example of this is shown in the example below. (The number in the parentheses indicate the time, relative to the beginning of the interaction, when the utterance was made.)",
        "The MatchIt Game (Ilinykh et al., forthcoming) is a yet further simplified visual game. Here, the goal simply is to decide whether you and your partner are both looking at the same image (of the same genre as in MeetUp). In that sense, it is a reduction of the MeetUP game to the final stage, taking out the navigation aspect. As example SECREF12 shows, this can similarly lead to meta-semantic interaction, where classifications are revised. As SECREF12 shows, even in cases where a decision can be reached quickly, there can be an explicit mutual confirmation step, before the (silent) decision signal is sent.",
        "A third setting that we have explored BIBREF19 brings conceptual negotiation more clearly into the foreground. In that game, the players are presented with images of birds of particular species and are tasked with coming up with a description of common properties. Again, the final answer has to be approved by both participants. As SECREF13 shows, this can lead to an explicit negotiation of conceptual content."
      ],
      "highlighted_evidence": [
        "We illustrate the concept by discussing some instantiations that we have recently experimented with.",
        "The design also provides for a clear “relevance place” at which an opportunity arises for semantic negotiation, namely, before the final decision is made. An example of this is shown in the example below. ",
        " As SECREF12 shows, even in cases where a decision can be reached quickly, there can be an explicit mutual confirmation step, before the (silent) decision signal is sent.",
        "A third setting that we have explored BIBREF19 brings conceptual negotiation more clearly into the foreground. In that game, the players are presented with images of birds of particular species and are tasked with coming up with a description of common properties. Again, the final answer has to be approved by both participants. As SECREF13 shows, this can lead to an explicit negotiation of conceptual content."
      ]
    }
  },
  {
    "paper_id": "1702.03274",
    "question": "Does the latent dialogue state heklp their model?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Recently, end-to-end approaches have trained recurrent neural networks (RNNs) directly on text transcripts of dialogs. A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels. However, end-to-end methods lack a general mechanism for injecting domain knowledge and constraints. For example, simple operations like sorting a list of database results or updating a dictionary of entities can expressed in a few lines of software, yet may take thousands of dialogs to learn. Moreover, in some practical settings, programmed constraints are essential – for example, a banking dialog system would require that a user is logged in before they can retrieve account information."
      ],
      "highlighted_evidence": [
        "A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels."
      ]
    }
  },
  {
    "paper_id": "1702.03274",
    "question": "Do the authors test on datasets other than bAbl?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [],
      "highlighted_evidence": []
    }
  },
  {
    "paper_id": "1812.10860",
    "question": "Do some pretraining objectives perform better than others for sentence level understanding tasks?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": true,
      "free_form_answer": "",
      "evidence": [
        "Looking to other target tasks, the grammar-related CoLA task benefits dramatically from ELMo pretraining: The best result without language model pretraining is less than half the result achieved with such pretraining. In contrast, the meaning-oriented textual similarity benchmark STS sees good results with several kinds of pretraining, but does not benefit substantially from the use of ELMo."
      ],
      "highlighted_evidence": [
        "Looking to other target tasks, the grammar-related CoLA task benefits dramatically from ELMo pretraining: The best result without language model pretraining is less than half the result achieved with such pretraining. In contrast, the meaning-oriented textual similarity benchmark STS sees good results with several kinds of pretraining, but does not benefit substantially from the use of ELMo."
      ]
    }
  },
  {
    "paper_id": "1712.02121",
    "question": "Did the authors try stacking multiple convolutional layers?",
    "answer": {
      "unanswerable": false,
      "extractive_spans": [],
      "yes_no": false,
      "free_form_answer": "",
      "evidence": [
        "Recently, convolutional neural networks (CNNs), originally designed for computer vision BIBREF27 , have significantly received research attention in natural language processing BIBREF28 , BIBREF29 . CNN learns non-linear features to capture complex relationships with a remarkably less number of parameters compared to fully connected neural networks. Inspired from the success in computer vision, BIBREF30 proposed ConvE—the first model applying CNN for the KB completion task. In ConvE, only $v_h$ and $v_r$ are reshaped and then concatenated into an input matrix which is fed to the convolution layer. Different filters of the same $3\\times 3$ shape are operated over the input matrix to output feature map tensors. These feature map tensors are then vectorized and mapped into a vector via a linear transformation. Then this vector is computed with $v_t$ via a dot product to return a score for (h, r, t). See a formal definition of the ConvE score function in Table 1 . It is worth noting that ConvE focuses on the local relationships among different dimensional entries in each of $v_h$ or $v_r$ , i.e., ConvE does not observe the global relationships among same dimensional entries of an embedding triple ( $v_h$ , $v_r$ , $v_t$ ), so that ConvE ignores the transitional characteristic in transition-based models, which is one of the most useful intuitions for the task."
      ],
      "highlighted_evidence": [
        "In ConvE, only $v_h$ and $v_r$ are reshaped and then concatenated into an input matrix which is fed to the convolution layer"
      ]
    }
  }
]