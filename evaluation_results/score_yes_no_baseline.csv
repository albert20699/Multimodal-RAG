user_input,retrieved_contexts,response,reference,context_precision,context_recall,faithfulness,answer_relevancy
Do they report results only on English datasets?,"['large, annotated datasets. UD’s v2.1 release ( et al. large resource, constructed by independent parties, evinces problems in the goal of a universal inven- tory of annotations. Annotators may choose to omit certain values (like the coerced gender of cante a linguistic concept is encoded. (See, e.g., math ditionally, many of the treebanks “were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD” ( Spanish word “ G in a character sequence which\n10 language resource was to Finnish on 3 August 2017. 6.1 Intrinsic evaluation We transform all UD data to the UniMorph. We compare the simple lookup-based transformation to the one with linguistically informed post-edits on all languages with both UD and UniMorph data. We then evaluate the recall of MSDs without partial credit.\nreview polarity, Yelp review full, Amazon review polarity, and Amazon review full. We exclude the Sougu News dataset, which is a transliterated Chinese text, as we only utilize English language models and word embeddings for the purposes of this demonstration. The results gathered by ( tional models and the fastText approach, are used as baselines. External resources required to ex- tract namely a language model ( of skip-gram based word embeddings with mensions ( a subset of the approx. 200 kens (\nunanticipated user needs are en- countered. To build classiﬁers on datasets of this size, we use spaCy ( and fastText ( ization, with transformation for improved perfor- mance ( with L2 regularization ( The dataset for shared tained through the data management tool by staff. One such classiﬁer is shared by all compa- nies that use a particular language; projects span English, German, Chinese, and French. About general intents are trained with a total of about to include intents that control the']",no,True,0.9999999999,0.0,1.0,0.0
Do they test their approach on a dataset without incomplete data?,"['Total 100 106 206 The incomplete dataset used for training is composed of lower-cased in- complete data obtained by manipulating the original corpora. The incomplete\nand neural networks. These works focus on investigating the impact in the performance of an existing model given incomplete data. More recently, with the\ndata into a clean or standard form before classiﬁcation for improved perfor- mance, instead of considering the data incompleteness as it is. Vateekul and\ndatasets, to varying degrees (from 0.45 on MPE to 31.11 on SICK). We also see improvements on datasets with biases (high performance of train- ing on each dataset compared to the correspond- ing majority baseline), most noticeably SPR. The only exception seems to be SCITAIL, where we do not improve despite it having different biases than SNLI. However, when we strengthen (below), Method 1 outperforms the baseline.']",no,False,0.0,0.0,0.0,0.0
Do they test their approach on a dataset without incomplete data?,"['Total 100 106 206 The incomplete dataset used for training is composed of lower-cased in- complete data obtained by manipulating the original corpora. The incomplete\nand neural networks. These works focus on investigating the impact in the performance of an existing model given incomplete data. More recently, with the\ndata into a clean or standard form before classiﬁcation for improved perfor- mance, instead of considering the data incompleteness as it is. Vateekul and\ndatasets, to varying degrees (from 0.45 on MPE to 31.11 on SICK). We also see improvements on datasets with biases (high performance of train- ing on each dataset compared to the correspond- ing majority baseline), most noticeably SPR. The only exception seems to be SCITAIL, where we do not improve despite it having different biases than SNLI. However, when we strengthen (below), Method 1 outperforms the baseline.']",no,False,0.0,0.0,0.0,0.0
Should their approach be applied only when dealing with incomplete data?,"['data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate\ndata into a clean or standard form before classiﬁcation for improved perfor- mance, instead of considering the data incompleteness as it is. Vateekul and\nAbstract In this paper, we propose B improves robustness in incomplete data, when compared to existing systems, by\nand neural networks. These works focus on investigating the impact in the performance of an existing model given incomplete data. More recently, with the']",yes,False,0.0,0.0,0.0,0.0
Should their approach be applied only when dealing with incomplete data?,"['data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate\ndata into a clean or standard form before classiﬁcation for improved perfor- mance, instead of considering the data incompleteness as it is. Vateekul and\nAbstract In this paper, we propose B improves robustness in incomplete data, when compared to existing systems, by\nand neural networks. These works focus on investigating the impact in the performance of an existing model given incomplete data. More recently, with the']",yes,False,0.0,0.0,0.0,0.0
Do they look for inconsistencies between different languages' annotations in UniMorph?,"['The Universal Dependencies (UD) and Uni- versal Morphology (UniMorph) projects each present schemata for annotating the mor- phosyntactic details of language. Each project also provides corpora of annotated text in many languages—UD at the token level and UniMorph at the type level. As each cor- pus is built by different annotators, language- speciﬁc decisions hinder the goal of universal schemata. With compatibility of tags, each project’s annotations could be used to validate the other’s. Additionally,\nThis tool enables a synergistic use of UniMorph and Universal Dependencies, as well as teasing out the annotation discrepancies within and across projects. When one dataset disobeys its schema or disagrees with a related language, the ﬂaws may not be noticed except by such a methodological dive into the resources. When the maintainers of the resources ameliorate these ﬂaws, the resources move closer to the goal of a universal, cross-lingual inventory of features for morphological annotation. The\n9 Conclusion and Future Work We created a tool for annotating Universal Depen- dencies CoNLL-U ﬁles with UniMorph annota- tions. Our tool is ready to use off-the-shelf today, requires no training, and is deterministic. While under-speciﬁcation necessitates a lossy and imper- fect conversion, ours is interpretable. Patterns of mistakes can be identiﬁed and ameliorated. The tool allows a bridge between resources an- notated in the Universal Dependencies and Uni- versal Morphology (UniMorph) schemata. As the\n• tributes or annotation inconsistencies in both UD and UniMorph, a guidepost for strength- ening and harmonizing each resource.']",yes,True,0.9999999999,0.0,0.0,0.0
Do they look for inconsistencies between different UD treebanks?,"['is an annotated treebank, making it a resource of token-level these treebanks, with feature names chosen for rele- vance to native speakers. (In this with UniMorph’s treatment of morphosyntac- tic categories.) The UD datasets have been used in the CoNLL shared tasks ( to appear).\nlarge, annotated datasets. UD’s v2.1 release ( et al. large resource, constructed by independent parties, evinces problems in the goal of a universal inven- tory of annotations. Annotators may choose to omit certain values (like the coerced gender of cante a linguistic concept is encoded. (See, e.g., math ditionally, many of the treebanks “were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD” ( Spanish word “ G in a character sequence which\n4 UD treebanks and UniMorph tables Prima facie, the alignment task may seem trivial. But we’ve yet to explore the humans in the loop. This conversion is a hard problem because we’re operating on idealized schemata. We’re actually annotating human decisions—and human mistakes. If both schemata were perfectly applied, their over- lapping attributes could be mapped to each other simply, in a cross-lingual and totally general way. Unfortunately, the resources are imperfect realiza- tions of their schemata. The\n• ping and post-editing, which replaces the UD features in a CoNLL-U ﬁle with UniMorph features. • mance tagging accuracy on UD treebanks is similar, whichever annotation schema is used (']",yes,True,0.0,0.0,1.0,0.0
Was each text augmentation technique experimented individually?,"['that can also be represented using word to vector representation. 6. the labelled messages for three different classes, we employed a data augmentation technique to boost the learning of the deep network. Following techniques from the paper by Jason et al. was utilized in this setting that really helped during the training phase.Thsi techniques wasnt used in previous studies. The techniques were: • not stop words. Replace each of these words with one of its synonyms chosen at random. • that is not a stop\n• messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an architecture learnt on about 14,500 messages with an accuracy of 83.90. We addressed this concern using data augmentation techniques applied on text data.\nas text generation, sentence correction, image captioning and text classiﬁca- tion, have been possible via models such as Convolutional Neural Networks\nand VRS are applied to generate the target text by ﬁrst sampling a selection mask, then run beam search decoding with beam size 10. We are inter- ested in seeing (1) if multiple generations from the same selection mask are paraphrases to each other (intra-consistent) and (2) if generations from dif- ferent selection masks do differ in the content they described (inter-diverse). The results in Table show that VRS signiﬁcantly outperforms the other two in both intra-consistency and inter-diversity. RS has the']",no,False,0.0,0.0,0.0,0.0
Does the dataset contain content from various social media platforms?,"['Introduction Recent years have seen a huge boom in the number of dif- ferent social media platforms available to users. People are increasingly using these platforms to voice their opinions or let others know about their whereabouts and activities. Each of these platforms has its own characteristics and is used for different purposes. The availability of a huge amount of data from many social media platforms has inspired researchers to study the relation between the data generated through the use of these\ncommunity (Quercia et al. 2012). Social media data has been used in many domains to ﬁnd\nattributes can reach an average Pearson correlation coefﬁcient of ing Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted from both datasets and their relative demographic attributes. Furthermore, the correlations highlight the differ- ent natures of the information contained in Yahoo! Answers and Twitter. While the former seems to offer a more encyclo- pedic content, the latter provides information related to cur- rent\n3 Dataset and Features We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the']",yes,False,0.0,0.0,0.0,0.0
Do they provide decision sequences as supervision while training models?,"['trajectories. How- ever, there often exist multiple trajectories to solve each question, many of which may not be observed in the supervised data since it is difﬁcult to exhaust all valid trajectories. Generalization can be limited when an agent is trained on such data.\n4. phase (A2C Regarding the four aspects, we report the base- line agent’s training performance followed by its generalization performance on test data. We use DQN and A2C to refer to our baseline agent trained with DQN and A2C, respectively. We set the maximum number of episodes (data points) to be 1 million, this is approximately 10 epochs in supervised learning tasks given the size of datasets. The agent may further improve af- ter 1 million episodes, however we believe some meaningful and interesting\n❄ ❄ ❄ ❄ Figure 1: Learning settings that we consider. Model components with frozen parameters are shown in gray and decorated with snowﬂakes. pretrain a BiLSTM on a task (left), and learn a target task model on top of the representations it produces (right). Middle train a BiLSTM on top of ELMo for an intermediate task (left). We then train a target task model on top of the intermediate task BiLSTM and ELMo (right). tom on an intermediate task (left), and then ﬁne-tune the re- sulting model again on a\nRecently, there has been various work on and around interactive environments. For instance, Nogueira and Cho that automatically transforms a website into a goal- driven web navigation task. They train a neural agent to follow traces using supervised learning. et al. tive retrieve-and-read system that answers complex open-domain questions, which is also trained with supervised learning. Although an effective training strategy, supervised learning requires either human labeled or heuristically generated']",no,False,0.0,0.0,0.0,0.0
